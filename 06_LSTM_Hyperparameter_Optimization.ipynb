{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1afb2ec3-c35b-4f2f-b689-372194f6dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_lstm_model(best_params):\n",
    "    model = Sequential()\n",
    "    n_layers = best_params['lstm_n_layers']\n",
    "    units = best_params['lstm_units']\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=(i < n_layers - 1),\n",
    "            input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]) if i == 0 else None\n",
    "        ))\n",
    "        model.add(Dropout(rate=best_params['lstm_dropout']))\n",
    "    model.add(Dense(1))\n",
    "    learning_rate = best_params['lstm_learning_rate']\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(trial):\n",
    "    model = Sequential()\n",
    "    n_layers = trial.suggest_int('lstm_n_layers', 2, 4)  # Optimize the number of LSTM layers\n",
    "    units = trial.suggest_categorical('lstm_units', [50, 100, 150, 200])\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=(i < n_layers - 1),\n",
    "            input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]) if i == 0 else None\n",
    "        ))\n",
    "        dropout_rate = trial.suggest_categorical('lstm_dropout', [0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "        model.add(Dropout(rate=dropout_rate))  # Use specific Dropout rate\n",
    "    model.add(Dense(1))\n",
    "    learning_rate = trial.suggest_categorical('lstm_learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001])  # Optimize learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define optimization objective function\n",
    "def objective_LSTM(trial):\n",
    "    num_samples = X_train_lstm.shape[0]*0.8\n",
    "    batch_size_candidates = [16, 32, 64, 128, 256, 512,1024,2048]#\n",
    "    batch_size_candidates = [bs for bs in batch_size_candidates if bs <= num_samples]\n",
    "    param = {\n",
    "        'epochs': trial.suggest_categorical('lstm_epochs', [10, 20, 30, 40, 50, 75, 100, 150, 200]),#\n",
    "        'batch_size': trial.suggest_categorical('lstm_batch_size', batch_size_candidates)\n",
    "    }\n",
    "    param['batch_size'] = min(param['batch_size'], num_samples) # Ensure batch_size does not exceed the number of training samples\n",
    "    model = KerasRegressor(build_fn=lambda: create_lstm_model(trial), epochs=param['epochs'], batch_size=param['batch_size'], verbose=0) #\n",
    "    \n",
    "    \n",
    "    # Use 5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_lstm):\n",
    "        X_train, X_val = X_train_lstm[train_index], X_train_lstm[val_index]\n",
    "        y_train_fold, y_val = y_train[train_index], y_train[val_index]\n",
    "        num_samples = X_train.shape[0]\n",
    "        # The 'patience' parameter defines how many epochs the model continues training without improvement before stopping\n",
    "        # If 'restore_best_weights' is True, the model's weights will be restored to the point with the lowest validation loss\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train_fold, validation_data=(X_val, y_val), callbacks=[early_stopping, KerasPruningCallback(trial, 'val_loss')])\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "    \n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plt_scatter(data,filename):\n",
    "    dataframes = data[['predicts','records']]\n",
    "    dataframes1 = dataframes\n",
    "    r2 = r2_score(dataframes['records'], dataframes['predicts'])  \n",
    "    # Calculate NRMSE  \n",
    "    mse = mean_squared_error(dataframes['records'], dataframes['predicts'])  \n",
    "    nrmse = np.sqrt(mse) / np.mean(dataframes['records'])  \n",
    "    rrmse = calculate_rrmse1(dataframes['records'], dataframes['predicts'])\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(dataframes['records'], dataframes['predicts'])\n",
    "    acc = calculate_acc(dataframes['records'], dataframes['predicts'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hexbin(dataframes['records'], dataframes['predicts'], gridsize=50, cmap='viridis', mincnt=1)\n",
    "    cb = plt.colorbar(label='Density')\n",
    "    fit_params = np.polyfit(dataframes['records'], dataframes['predicts'], 1)\n",
    "    fit_line = np.polyval(fit_params, dataframes['records'])\n",
    "    plt.plot(dataframes['records'], fit_line, color='red', label='Fit line')# Add 1:1 line\n",
    "    max_val = max(max(dataframes['records']), max(dataframes['predicts']))\n",
    "    plt.plot([0, max_val], [0, max_val], linestyle='--', color='green', label='1:1 line')\n",
    "    plt.title('')\n",
    "    plt.xlabel('records')\n",
    "    plt.ylabel('predicts')\n",
    "    plt.grid(True)\n",
    "    plt.text(0.02, 0.95, f'R² = {r2:.2f}', transform=plt.gca().transAxes, fontsize=12, va='top')  \n",
    "    plt.text(0.02, 0.90, f'ACC = {acc:.2f}', transform=plt.gca().transAxes, fontsize=12, va='top')  \n",
    "    plt.text(0.02, 0.85, f'RRMSE = {rrmse:.2f}%', transform=plt.gca().transAxes, fontsize=12, va='top')  \n",
    "    plt.text(0.02, 0.80, f'MAPE = {mape*100:.2f}%', transform=plt.gca().transAxes, fontsize=12, va='top')  \n",
    "    \n",
    "    # Display the figure  \n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def extract_selected_variables(inputpath_base):\n",
    "    inpath_dates = os.path.join(inputpath_base, '01_data','05_buildmodel', '04_selectFeatures','selectFeatures.txt')\n",
    "    # Construct file path\n",
    "    gs_infornamtion = pd.read_csv(inpath_dates, sep='\\t', header=None)\n",
    "    gs_infornamtion.columns = ['slected_dynamic_features', 'slected_static', 'regionID']\n",
    "    gs_infornamtion['slected_dynamic_features'] = gs_infornamtion['slected_dynamic_features'].apply(ast.literal_eval)\n",
    "    gs_infornamtion['slected_static'] = gs_infornamtion['slected_static'].apply(ast.literal_eval)\n",
    "    return gs_infornamtion\n",
    "\n",
    "def calculate_rrmse1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate RRMSE (Relative Root Mean Square Error) using the mean of actual y values as reference\n",
    "    \n",
    "    Parameters:\n",
    "    y_true -- Array or list of true values\n",
    "    y_pred -- Array or list of predicted values\n",
    "    \n",
    "    Returns:\n",
    "    rrmse -- Relative Root Mean Square Error (in percentage)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    \n",
    "    # Calculate mean of true values\n",
    "    mean_y_true = np.mean(y_true)\n",
    "    \n",
    "    # Calculate RRMSE\n",
    "    rrmse = (rmse / mean_y_true) * 100\n",
    "    \n",
    "    return rrmse\n",
    "    \n",
    "def calculate_rrmse2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate rRMSE (Relative Root Mean Square Error) using each actual y value as reference\n",
    "    \n",
    "    Parameters:\n",
    "    y_true -- Array or list of true values\n",
    "    y_pred -- Array or list of predicted values\n",
    "    \n",
    "    Returns:\n",
    "    rrmse -- Relative Root Mean Square Error (in percentage)\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # Calculate rRMSE\n",
    "    rrmse = np.sqrt(np.mean(((y_true - y_pred) / y_true) ** 2)) * 100\n",
    "    \n",
    "    return rrmse\n",
    "# Define custom nRMSE evaluation function\n",
    "def calculate_nrmse(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    nrmse = rmse / (y_true.max() - y_true.min())\n",
    "    return nrmse* 100\n",
    "\n",
    "def calculate_acc(y_true, y_pred):\n",
    "    # Calculate means of observed and predicted values\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    \n",
    "    # Calculate anomalies\n",
    "    anomaly_true = y_true - mean_true\n",
    "    anomaly_pred = y_pred - mean_pred\n",
    "    \n",
    "    # Calculate ACC\n",
    "    numerator = np.sum(anomaly_true * anomaly_pred)\n",
    "    denominator = np.sqrt(np.sum(anomaly_true**2) * np.sum(anomaly_pred**2))\n",
    "    \n",
    "    acc = numerator / denominator\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca5fb3c-1540-4fea-9931-9911e8efbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from optuna_integration.keras import KerasPruningCallback\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tqdm.notebook import tqdm\n",
    "import tensorflow.keras.backend as K\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a307d-52fc-42c5-8f4d-f9489bb9d16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a744a2fd-5ba2-47cc-8fe5-3bb2b698e94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: F:\\SCI\\SCI9_1\\01_code\\02_Wheat\\06_India-test\n",
      "当前文件夹名字: 06_India\n",
      "上一级文件夹名字: 02_Wheat\n"
     ]
    }
   ],
   "source": [
    "root_directory = os.getcwd()[0:3]\n",
    "'''\n",
    "# Required packages to install\n",
    "Pre-installed packages:\n",
    "os, sys, warnings, optuna \n",
    "\n",
    "Packages to install:\n",
    "matplotlib, sklearn, optuna_integration, tensorflow, scikeras, ast\n",
    "\n",
    "Installation commands:\n",
    "conda install optuna matplotlib sklearn optuna_integration tensorflow scikeras ast -c pytorch\n",
    "conda install matplotlib tensorflow scikeras -c pytorch\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Get current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)\n",
    " \n",
    "# Get the name of the current folder\n",
    "current_folder_name = os.path.basename(current_directory)[0:8]\n",
    "print(\"Current folder name:\", current_folder_name)\n",
    " \n",
    "# Get the name of the parent folder\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "parent_folder_name = os.path.basename(parent_directory)\n",
    "print(\"Parent folder name:\", parent_folder_name)\n",
    "\n",
    "# Variables that need to be modified\n",
    "crop = parent_folder_name;countryID =current_folder_name;variable = 'mx2t6';\n",
    "country = countryID.split('_')[1]\n",
    "\n",
    "############## Region settings #############################################\n",
    "inpath_dates_other = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID+'\\\\'+'01_data'+'\\\\'+'07_Information'\n",
    "other_infornamtion = pd.read_csv(os.path.join(inpath_dates_other,'information.txt'), sep=' ', header=None)\n",
    "startyear,endyear,shp_name = other_infornamtion.iloc[0,0],other_infornamtion.iloc[0,1],other_infornamtion.iloc[0,2]\n",
    "\n",
    "years = range(startyear,endyear+1)\n",
    "years_str = [str(year) for year in years]\n",
    "modelname = 'LSTM'\n",
    "yield_type = 'actual_yield';\n",
    "root_directory = os.getcwd()[0:3]\n",
    "inputpath_base = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID\n",
    "SelFeature_infornamtion = extract_selected_variables(inputpath_base)\n",
    "region='I'\n",
    "institution = 'ECMWF';country =countryID[3:];yield_type= 'actual_yield';\n",
    "regions = ['I'];Forecastyear = endyear;# Define the best forecast year for each region\n",
    "Forecastyears = {\n",
    "    'I': Forecastyear}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6f17a-3ef8-46f8-8267-33e480981830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea206d12-55a3-46ee-a526-bbcde34111cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Hyperparameter Tuning ###########################################\n",
    "\n",
    "################### Load Selected Feature Variables #########################################\n",
    "Forecastyear = Forecastyears[region]\n",
    "data= pd.read_csv(os.path.join(inputpath_base, '01_data','05_buildmodel','03_modeldata',region+'_data_ori.csv'))\n",
    "data = data.drop_duplicates(subset=['year', 'idJoin'], keep='first')\n",
    "data = data[data['year']!=Forecastyear] # Exclude data from the forecast year\n",
    "\n",
    "TimeFeatures_sel, Static_sel, regionID = SelFeature_infornamtion[SelFeature_infornamtion['regionID'] == region].iloc[0]\n",
    "feature_all = TimeFeatures_sel+Static_sel\n",
    "filtered_columns = [col for col in data.columns if any(feature in col for feature in feature_all)]\n",
    "filtered_columns = [col for col in filtered_columns if 'year.1' not in col] # Try excluding yield-related features\n",
    "#### Exclude yield data from previous years #########################\n",
    "filtered_columns = [col for col in filtered_columns if 'Yield' not in col] # Try excluding yield-related features\n",
    "Static_sel= [col for col in Static_sel if 'Yield' not in col] # Try excluding yield-related features\n",
    "\n",
    "\n",
    "\n",
    "MLdata_reduced = data[filtered_columns+[yield_type]]\n",
    "MLdata_reduced['year'] = data['year']\n",
    "# Load selected weeks\n",
    "inpath_dates = os.path.join(inputpath_base, '01_data','05_buildmodel', '02_extractdates','gs_three_periods.txt')\n",
    "gs_infornamtion = pd.read_csv(inpath_dates, delim_whitespace=True, header=None)\n",
    "gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point', 'VI_select2','regionID']\n",
    "start_point, peak, harvest_point, VI_select2, region = gs_infornamtion[gs_infornamtion['regionID'] == region].iloc[0]\n",
    "\n",
    "############################ Data Preparation for Parameter Tuning - Data Standardization ###########################################################\n",
    "data_all = MLdata_reduced;X_all = data_all.drop([yield_type], axis=1);\n",
    "\n",
    "y_all = data_all[yield_type];\n",
    "# Data standardization\n",
    "scaler_X = StandardScaler().fit(X_all)\n",
    "X = scaler_X.transform(X_all)\n",
    "scaler_y = StandardScaler().fit(y_all.values.reshape(-1, 1))\n",
    "y = scaler_y.transform(y_all.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "X = pd.DataFrame(data=X,columns=X_all.columns.tolist())\n",
    "if start_point>harvest_point:\n",
    "    weeks_select_list= list(range(start_point, 47))+list(range(1, harvest_point+1))\n",
    "else:\n",
    "    weeks_select_list= list(range(start_point,harvest_point+1))\n",
    "    \n",
    "data_list = []\n",
    "for i in weeks_select_list:\n",
    "    data_i = X[[f'Week{i}_{feature}' for feature in TimeFeatures_sel] + Static_sel]\n",
    "    data_list.append(data_i.values)\n",
    "# Stack all arrays in data_list into a 3D array\n",
    "data_list = np.array(data_list)\n",
    "X_train_lstm = np.transpose(data_list, (1, 0, 2)) # Reshape array to (samples, time steps, features)\n",
    "y_train = y\n",
    "\n",
    "################## Create LSTM Model and Perform Hyperparameter Tuning; Skip if Tuning is Completed (Most Time-Consuming); Resume Tuning from Previous Progress if Paused ####################################\n",
    "storage_name = f\"sqlite:///{country}_{modelname}_region{region}.db\" # Tuning records will be saved in the folder where your code is located\n",
    "study_name =f\"{country}_{modelname}_region{region}\"\n",
    "\n",
    "# Perform hyperparameter optimization with Optuna and set early stopping mechanism\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(), study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "\n",
    "# n_jobs indicates parallel computing\n",
    "study.optimize(objective_LSTM, n_trials=200, n_jobs=4) \n",
    "\n",
    "# Output best hyperparameters\n",
    "print('Best parameters: ', study.best_params)\n",
    "print('Best score: ', study.best_value)\n",
    "\n",
    "# Save best hyperparameters as JSON file\n",
    "with open(f\"{study_name}_best_params.json\", \"w\") as f:\n",
    "    json.dump(study.best_params, f, indent=4)\n",
    "print(\"Best parameters have been saved to JSON file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0d7cd-a84f-4ce7-8525-0cbe5b929168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4d432-6d9b-4875-a47f-6176032ba160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798c657-b302-4bb4-a4d3-a0770b994c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc2bc8-ea10-44cf-9312-88fe647f87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50b479-356a-4322-b2d6-1e8e45abfaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31269c-352b-4753-8306-757950eacbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4a32c-b922-44d4-ade1-ead8f30a4b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d84cb-14ed-42c7-8ae1-19da40105798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c83dd3-f6ba-4b1b-bc9b-a7e85fb13530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e583332-37a4-4610-b767-be77c7a7a9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236a3c3-7265-4b8d-ad24-04921995f556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8545f4-9969-4340-a653-bd15138de58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d5529-6799-485f-a848-20b67a39d203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703b43f-4da6-4f4a-af70-f9d414ac35f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3567c-e146-4e4d-9977-1ea6598f286b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53172a18-dcb5-4785-8d6e-d012ea2a10f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802dda5-1761-4d05-b67e-a68d833400b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d002c-e2a0-459f-a687-88374490cc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a3f15-bc58-4ce4-ad21-49ff1a1f2a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a89ad8-28bd-4091-9d51-82ea676ac2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184a029-2528-43c9-af2c-3d8b0bbab583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa5d71-aec9-41e0-b461-be518c9741cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c18c92-078b-484c-ae68-b6ff79d7d7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abad2c0-8a59-4261-b0d3-6a2a3309b856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29f181-89c4-427c-9da7-744cb5ec9fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0f25b-cd92-482e-8164-db64e01a17d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1ac82-841a-4d77-89c4-056e3f22e5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0a117-0431-4d73-8015-d61122649ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f21fb-fee1-4fb8-92ec-cdef40e8fe69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153cebd-6b35-4efb-970d-370a42327e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05078a10-2173-4f83-a9b0-40c330db7233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a531a1c-9b44-4aba-b2af-bd39b024bdff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053c05a-5981-4768-933a-3b2db57d3473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a73b85-fdfb-4795-b711-fe7dc2d8d4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91ac12-9968-4928-a9c4-9f54fce07ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f7353-cfcb-497b-8cf9-599b76c87119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d39cb7-cf19-446e-8c9b-e14520ae7654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242565c3-7f71-46d9-aad1-a3f406c6a3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a0173-42f6-4b1b-ae84-39aa8f074f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec3dfb-0b9d-40fe-9f69-78199076b212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf59e4-f57d-4a47-bb68-c55d11d501ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75b1c1-5f23-47b0-89bf-86f2ac53e2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f2abb-5943-4047-9fc2-abf006c74e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb1052-c3f9-48ef-9fc0-58a3c26b4f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c2cab-20e1-45dd-8c37-d0ac56d72b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c86bd-66c2-4602-b6d4-d757af8fc502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323ac9c-c2ff-42c4-a6b4-7976cb226708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8847819-d127-4c5d-9874-93c73d108a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a42d5-2fcb-4343-adea-889fbb33fe5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa5154-d0e6-42f5-a095-c4e5053e228f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4425836-86a2-4a73-95e1-1b744d430e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf03a6-4985-4f16-85ea-f5b9e2cc31ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d5783-426c-4e80-a858-0779d809a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf713424-19bc-4e3d-9b21-047a2f6933b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4f84f-a4c1-4eaf-8171-9bf57d6f048c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf54ae-0b17-4992-93fb-a3c0a7909b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efc902-d00c-49be-9388-a9c28d0f2de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
