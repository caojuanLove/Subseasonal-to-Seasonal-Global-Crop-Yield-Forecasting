{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c18d7ad-e0a9-43b6-96ad-1b198e1418a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_lstm_model(best_params):\n",
    "    model = Sequential()\n",
    "    n_layers = best_params['lstm_n_layers']\n",
    "    units = best_params['lstm_units']\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=(i < n_layers - 1),\n",
    "            input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]) if i == 0 else None\n",
    "        ))\n",
    "        model.add(Dropout(rate=best_params['lstm_dropout']))\n",
    "    model.add(Dense(1))\n",
    "    learning_rate = best_params['lstm_learning_rate']\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_lstm_model(trial):\n",
    "    model = Sequential()\n",
    "    n_layers = trial.suggest_int('lstm_n_layers', 2, 4)  # Optimize the number of LSTM layers\n",
    "    units = trial.suggest_categorical('lstm_units', [50, 100, 150, 200])\n",
    "    for i in range(n_layers):\n",
    "        model.add(LSTM(\n",
    "            units=units,\n",
    "            return_sequences=(i < n_layers - 1),\n",
    "            input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]) if i == 0 else None\n",
    "        ))\n",
    "        dropout_rate = trial.suggest_categorical('lstm_dropout', [0,0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "        model.add(Dropout(rate=dropout_rate))  # Use specific Dropout rate\n",
    "    model.add(Dense(1))\n",
    "    learning_rate = trial.suggest_categorical('lstm_learning_rate', [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001])  # Optimize learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define optimization objective function\n",
    "def objective_LSTM(trial):\n",
    "    num_samples = X_train_lstm.shape[0]*0.8\n",
    "    batch_size_candidates = [16, 32, 64, 128, 256, 512,1024,2048]#\n",
    "    batch_size_candidates = [bs for bs in batch_size_candidates if bs <= num_samples]\n",
    "    param = {\n",
    "        'epochs': trial.suggest_categorical('lstm_epochs', [10, 20, 30, 40, 50, 75, 100, 150, 200]),#\n",
    "        'batch_size': trial.suggest_categorical('lstm_batch_size', batch_size_candidates)\n",
    "    }\n",
    "    param['batch_size'] = min(param['batch_size'], num_samples) # Ensure batch_size does not exceed the number of training samples\n",
    "    model = KerasRegressor(build_fn=lambda: create_lstm_model(trial), epochs=param['epochs'], batch_size=param['batch_size'], verbose=0) #\n",
    "    \n",
    "    \n",
    "    # Use 5-fold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mse_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_lstm):\n",
    "        X_train, X_val = X_train_lstm[train_index], X_train_lstm[val_index]\n",
    "        y_train_fold, y_val = y_train[train_index], y_train[val_index]\n",
    "        num_samples = X_train.shape[0]\n",
    "        # The 'patience' parameter defines how many epochs the model continues training without improvement before stopping\n",
    "        # If 'restore_best_weights' is True, the model's weights will be restored to the point with the lowest validation loss\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        model.fit(X_train, y_train_fold, validation_data=(X_val, y_val), callbacks=[early_stopping, KerasPruningCallback(trial, 'val_loss')])\n",
    "        \n",
    "        y_pred = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "        mse_scores.append(mse)\n",
    "    \n",
    "    return np.mean(mse_scores)\n",
    "\n",
    "\n",
    "def plot_importance(df_importances, output_path, model_name, region,filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot bar chart and concentric pie chart of feature importance.\n",
    "\n",
    "    Parameters:\n",
    "        df_importances (DataFrame): Feature importance data, required to contain 'feature' column.\n",
    "        output_path (str): Base path for output files.\n",
    "        model_name (str): Model name, which will be used for folder structure and file naming.\n",
    "        region (str): Region name, which will be used for output file naming.\n",
    "        category_map (dict): Dictionary mapping features to categories.\n",
    "    \"\"\"\n",
    "    # Classify features by category\n",
    "    # Categories corresponding to features\n",
    "    category_map = {\n",
    "        'KNDVI': 'VI',\n",
    "        'EVI': 'VI',\n",
    "        'NDVI': 'VI',\n",
    "        'modis_Gpp': 'VI',\n",
    "        'Fpar': 'VI',\n",
    "        'modis_LAI': 'VI',\n",
    "        'PML_Gpp': 'VI',\n",
    "        'Mean_Yield': 'TI',\n",
    "        'Previous_Yield': 'TI',\n",
    "        'year': 'TI',\n",
    "        'Pre': 'MC',\n",
    "        'Tmin': 'MC',\n",
    "        'Solar': 'MC',\n",
    "        'Tmean': 'MC',\n",
    "        'Tmax': 'MC',\n",
    "        'CDD': 'EC',\n",
    "        'HDD': 'EC',\n",
    "        'GDD': 'EC',\n",
    "        'VPD': 'EC',\n",
    "        'SPEI': 'EC',\n",
    "        'wind_speed': 'EC',\n",
    "        'SAND': 'CEC',\n",
    "        'AWC': 'CEC',\n",
    "        'SILT': 'CEC',\n",
    "        'ORG_CARBON': 'CEC',\n",
    "        'TOTAL_N': 'CEC',\n",
    "        'PH_WATER': 'CEC',\n",
    "        'CEC_SOIL': 'CEC',\n",
    "        'CLAY': 'CEC',\n",
    "        'elevation': 'CEC',\n",
    "        'lat': 'CEC',\n",
    "         'lon': 'CEC'\n",
    "    }\n",
    "    # Set file save path\n",
    "    folder_path = os.path.join(output_path, '06_figure', 'results', model_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Calculate mean of feature contributions\n",
    "    df_importances_mean = df_importances.mean(axis=1).reset_index()\n",
    "    df_importances_mean['Category'] = df_importances_mean['feature'].map(category_map)\n",
    "    df_importances_mean = df_importances_mean[['feature', 0, 'Category']]\n",
    "    df_importances_mean.columns = ['Feature', 'Contribution', 'Category']\n",
    "\n",
    "    # Exclude baseline feature and sort by category and contribution\n",
    "    contribution_df_sorted = df_importances_mean[df_importances_mean['Feature'] != 'BASELINE']\n",
    "    contribution_df_sorted = contribution_df_sorted.sort_values(by=['Category', 'Contribution'], ascending=[True, False])\n",
    "\n",
    "    # Define category colors and gradient function\n",
    "    category_colors = {\n",
    "        'VI': (0.9, 0.7, 0.2, 1),  # Yellow\n",
    "        'TI': (0.6, 0.3, 0.9, 1),  # Purple\n",
    "        'MC': (0.7, 0.3, 0.3, 1),  # Dark red\n",
    "        'EC': (0.2, 0.9, 0.9, 1),  # Cyan\n",
    "        'CEC': (0.3, 0.6, 0.9, 1)  # Light blue\n",
    "    }\n",
    "    default_color = (0.8, 0.8, 0.8, 1)  # Gray\n",
    "\n",
    "    def get_color_gradient(base_color, num_shades):\n",
    "        gradient = np.linspace(0.4, 1, num_shades)\n",
    "        return [(base_color[0], base_color[1], base_color[2], shade) for shade in gradient]\n",
    "\n",
    "    # Inner and outer circle data\n",
    "    inner_contribution = contribution_df_sorted.groupby('Category')['Contribution'].sum()\n",
    "    outer_contribution = contribution_df_sorted.set_index('Feature')['Contribution']\n",
    "\n",
    "    # Create color gradient for outer circle\n",
    "    outer_colors = []\n",
    "    for category in inner_contribution.index:\n",
    "        category_df = contribution_df_sorted[contribution_df_sorted['Category'] == category]\n",
    "        base_color = category_colors.get(category, default_color)\n",
    "        gradient_colors = get_color_gradient(base_color, len(category_df))\n",
    "        outer_colors.extend(gradient_colors)\n",
    "\n",
    "    # Plot figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8), dpi=1200)\n",
    "    ax.set_facecolor('#f0f0f0')\n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.7, color='gray', alpha=0.7)\n",
    "\n",
    "    # Plot bar chart\n",
    "    contribution_df_sorted = contribution_df_sorted.sort_values(by='Contribution', ascending=False)\n",
    "    bar_colors = [category_colors.get(cat, default_color) for cat in contribution_df_sorted['Category']]\n",
    "    ax.barh(contribution_df_sorted['Feature'], contribution_df_sorted['Contribution'], color=bar_colors)\n",
    "    ax.set_xlabel('Contribution')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_title('Feature Contributions by Category')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Add legend\n",
    "    handles = [plt.Rectangle((0, 0), 1, 1, color=category_colors[cat]) for cat in category_colors]\n",
    "    ax.legend(handles, category_colors.keys(), loc='lower right')\n",
    "\n",
    "    # Plot embedded concentric pie chart\n",
    "    inset_ax = inset_axes(ax, width=2, height=2, loc='upper right', bbox_to_anchor=(0.8, 0.35, 0.2, 0.2), bbox_transform=ax.transAxes)\n",
    "    inset_ax.pie(inner_contribution, labels=[''] * len(inner_contribution), autopct='%1.1f%%', radius=1,\n",
    "                 colors=[category_colors.get(cat, default_color) for cat in inner_contribution.index], wedgeprops=dict(width=0.3, edgecolor='w'))\n",
    "    inset_ax.pie(outer_contribution, labels=[''] * len(outer_contribution), radius=0.7, colors=outer_colors, wedgeprops=dict(width=0.3, edgecolor='w'))\n",
    "    inset_ax.add_artist(plt.Circle((0, 0), 0.4, color='white'))\n",
    "\n",
    "    # Save chart\n",
    "    plt.savefig(filename, format='tiff', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plt_scatter(data,filename):\n",
    "    dataframes = data[['predicts','records']]\n",
    "    dataframes1 = dataframes\n",
    "    r2 = r2_score(dataframes['records'], dataframes['predicts'])  \n",
    "    # Calculate NRMSE  \n",
    "    mse = mean_squared_error(dataframes['records'], dataframes['predicts'])  \n",
    "    nrmse = np.sqrt(mse) / np.mean(dataframes['records'])  \n",
    "    rrmse = calculate_rrmse1(dataframes['records'], dataframes['predicts'])\n",
    "    \n",
    "    mape = mean_absolute_percentage_error(dataframes['records'], dataframes['predicts'])\n",
    "    acc = calculate_acc(dataframes['records'], dataframes['predicts'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hexbin(dataframes['records'], dataframes['predicts'], gridsize=50, cmap='viridis', mincnt=1)\n",
    "    cb = plt.colorbar(label='Density')\n",
    "    fit_params = np.polyfit(dataframes['records'], dataframes['predicts'], 1)\n",
    "    fit_line = np.polyval(fit_params, dataframes['records'])\n",
    "    plt.plot(dataframes['records'], fit_line, color='red', label='Fit line')# Add 1:1 line\n",
    "    max_val = max(max(dataframes['records']), max(dataframes['predicts']))\n",
    "    plt.plot([0, max_val], [0, max_val], linestyle='--', color='green', label='1:1 line')\n",
    "    plt.title('')\n",
    "    plt.xlabel('records')\n",
    "    plt.ylabel('predicts')\n",
    "    plt.grid(True)\n",
    "    plt.text(0.02, 0.95, f'R² = {r2:.2f}', transform=plt.gca().transAxes, fontsize=12, va='top')  \n",
    "    plt.text(0.02, 0.90, f'ACC = {acc:.2f}', transform=plt.gca().transAxes, fontsize=12, va='top')  \n",
    "    plt.text(0.02, 0.85, f'RRMSE = {rrmse:.2f}%', transform=plt.gca().transAxes, fontsize=12, va='top')  \n",
    "    plt.text(0.02, 0.80, f'MAPE = {mape*100:.2f}%', transform=plt.gca().transAxes, fontsize=12, va='top')  \n",
    "    \n",
    "    # Display the figure  \n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa0041e-cc94-47cf-8a00-66ffedb60f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ast\n",
    "root_directory = os.getcwd()[0:3] \n",
    "sys.path.append(root_directory+'SCI\\\\SCI9\\\\2_DataAnalysis\\\\04_code\\\\1_04_Soybean')\n",
    "sys.path.append(r'C:\\ProgramData\\anaconda3\\Lib\\site-packages') \n",
    "sys.path.append(r'C:\\Users\\DELL\\.conda\\envs\\myenv\\Lib\\site-packages') \n",
    "sys.path.append(r'C:\\Users\\DELL\\.conda\\envs\\rasterio_env\\Lib\\site-packages') \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from functions import prepare_and_model_data,extract_selected_variables\n",
    "from functions import calculate_rrmse1,calculate_rrmse2,calculate_acc\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna_integration.keras import KerasPruningCallback\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from functions import calculate_rrmse1,calculate_rrmse2,calculate_acc,calculate_nrmse\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow.keras.backend as K\n",
    "import json\n",
    "\n",
    "# 需要修改下\n",
    "countryID = '06_India';\n",
    "country =countryID[3:]\n",
    "crop = '02_wheat';yield_type= 'actual_yield';\n",
    "inputpath_base = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID\n",
    "root_directory = os.getcwd()[0:3]\n",
    "inpath_dates_other = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID+'\\\\'+'01_data'+'\\\\'+'07_Information'\n",
    "other_infornamtion = pd.read_csv(os.path.join(inpath_dates_other,'information.txt'), sep=' ', header=None)\n",
    "startyear,endyear,shp_name = other_infornamtion.iloc[0,0],other_infornamtion.iloc[0,1],other_infornamtion.iloc[0,2]\n",
    "regions = ['I']\n",
    "Forecastyear = endyear\n",
    "# 定义每个区的最佳预报年\n",
    "Forecastyears = {\n",
    "    'I': Forecastyear}\n",
    "\n",
    "\n",
    "years = range(startyear,endyear+1)\n",
    "years_str = [str(year) for year in years]\n",
    "modelname = 'LSTM'\n",
    "yield_type = 'actual_yield';\n",
    "SelFeature_infornamtion = extract_selected_variables(inputpath_base)\n",
    "institution = 'ECMWF';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5529cc1-ad2a-4543-9df9-08890975dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions:\n",
    "    ################### Load Selected Feature Variables #########################################\n",
    "    Forecastyear = Forecastyears[region]\n",
    "    data= pd.read_csv(os.path.join(inputpath_base, '01_data','05_buildmodel','03_modeldata',region+'_data_ori.csv'))\n",
    "    # data= pd.read_csv(os.path.join(inputpath_base, '01_data','05_buildmodel','03_modeldata',region+'_data_ori.csv'))\n",
    "    data = data.drop_duplicates(subset=['year', 'idJoin'], keep='first')\n",
    "    TimeFeatures_sel, Static_sel, regionID = SelFeature_infornamtion[SelFeature_infornamtion['regionID'] == region].iloc[0]\n",
    "    feature_all = TimeFeatures_sel+Static_sel\n",
    "    filtered_columns = [col for col in data.columns if any(feature in col for feature in feature_all)]\n",
    "    filtered_columns = [col for col in filtered_columns if 'year.1' not in col] # Try excluding yield-related features\n",
    "\n",
    "    \n",
    "    #### Exclude yield data from previous years (check if accuracy changes) #########################\n",
    "    filtered_columns = [col for col in filtered_columns if 'Yield' not in col] # Try excluding yield-related features\n",
    "    Static_sel= [col for col in Static_sel if 'Yield' not in col] # Try excluding yield-related features\n",
    "    feature_all = [col for col in feature_all if 'Yield' not in col] # Try excluding yield-related features\n",
    "    #############################################################\n",
    "    \n",
    "    MLdata_reduced = data[filtered_columns+[yield_type]]\n",
    "    MLdata_reduced['year'] = data['year']\n",
    "    \n",
    "    # Load selected weeks\n",
    "    inpath_dates = os.path.join(inputpath_base, '01_data','05_buildmodel', '02_extractdates','gs_three_periods.txt')\n",
    "    gs_infornamtion = pd.read_csv(inpath_dates, delim_whitespace=True, header=None)\n",
    "    gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point', 'VI_select2','regionID']\n",
    "    start_point, peak, harvest_point, VI_select2, region = gs_infornamtion[gs_infornamtion['regionID'] == region].iloc[0]\n",
    "    \n",
    "    ############################ Data Standardization ###########################################################\n",
    "    data_all = MLdata_reduced;X_all = data_all.drop([yield_type], axis=1);\n",
    "    \n",
    "    y_all = data_all[yield_type];\n",
    "    # Data standardization\n",
    "    scaler_X = StandardScaler().fit(X_all)\n",
    "    scaler_y = StandardScaler().fit(y_all.values.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "    dataframes = pd.DataFrame()\n",
    "    df_importances = pd.DataFrame()\n",
    "    os.makedirs(os.path.join(inputpath_base,  '03_results',modelname,yield_type,region),exist_ok=True)\n",
    "    data_all = MLdata_reduced[(MLdata_reduced['year'] < Forecastyear)]\n",
    "    X_all = data_all.drop([yield_type], axis=1)\n",
    "    y_all = data_all[yield_type]\n",
    "    X = scaler_X.transform(X_all)\n",
    "    y = scaler_y.transform(y_all.values.reshape(-1, 1)).flatten()\n",
    "    X = pd.DataFrame(data=X,columns=X_all.columns.tolist())\n",
    "    if start_point>harvest_point:\n",
    "        weeks_select_list= list(range(start_point, 47))+list(range(1, harvest_point+1))\n",
    "    else:\n",
    "        weeks_select_list= list(range(start_point,harvest_point+1))\n",
    "        \n",
    "    data_list = []\n",
    "    for i in weeks_select_list:\n",
    "        data_i = X[[f'Week{i}_{feature}' for feature in TimeFeatures_sel] + Static_sel]\n",
    "        data_list.append(data_i.values)\n",
    "    \n",
    "    # Stack all arrays in data_list into a 3D array\n",
    "    data_list = np.array(data_list)\n",
    "    X_train_lstm = np.transpose(data_list, (1, 0, 2)) # Reshape array to (samples, time steps, features)\n",
    "    y_train = y\n",
    "    \n",
    "    data_test =  MLdata_reduced[(MLdata_reduced['year'] == Forecastyear)]\n",
    "    X_test = data_test.drop([yield_type], axis=1);y_test = data_test[yield_type]\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "    y_test = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "    data_list = []\n",
    "    \n",
    "    X_test = pd.DataFrame(data=X_test,columns=X_all.columns.tolist())\n",
    "    for i in weeks_select_list:\n",
    "        data_i = X_test[[f'Week{i}_{feature}' for feature in TimeFeatures_sel] + Static_sel]\n",
    "        data_list.append(data_i.values)\n",
    "    data_list = np.array(data_list)\n",
    "    X_test_lstm = np.transpose(data_list, (1, 0, 2)) # Reshape array to (samples, time steps, features)\n",
    "    # Load saved optimal hyperparameters from JSON file and build model\n",
    "    with open(f\"{country}_{modelname}_region{region}_best_params.json\", \"r\") as f:\n",
    "        best_params = json.load(f)\n",
    "        \n",
    "    model = create_best_lstm_model(best_params)#study.best_params\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "    ## Train and save the model\n",
    "\n",
    "    # Randomly split into 8:2 for training and validation data\n",
    "    x_train_lstm,x_val_lstm,y_train,y_val = train_test_split(X_train_lstm, y_train, test_size=0.2, random_state=42)\n",
    "    model.fit(x_train_lstm, y_train,validation_data=(x_val_lstm,y_val),epochs=best_params['lstm_epochs'], batch_size=best_params['lstm_batch_size'], callbacks=[early_stopping])\n",
    "\n",
    "    # Save trained model\n",
    "    outpathmodel = os.path.join(inputpath_base,'04_model',region, 'LSTM')\n",
    "    os.makedirs(outpathmodel,exist_ok=True)\n",
    "    model_path = os.path.join(outpathmodel,region+'my_lstm_model.keras')\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Or load trained model\n",
    "    outpathmodel = os.path.join(inputpath_base,'04_model',region, 'LSTM')\n",
    "    model_path = os.path.join(outpathmodel,region+'my_lstm_model.keras')\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Validate accuracy on test (forecast year) data \n",
    "    y_pred = model.predict(X_test_lstm)\n",
    "    y_pred_inverse = scaler_y.inverse_transform(y_pred)\n",
    "    y_test_inverse = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "    data_pre = pd.DataFrame({'predicts':y_pred_inverse.ravel(),'records':y_test_inverse.ravel()})\n",
    "    data_pre['year'] = Forecastyear\n",
    "    \n",
    "    # Save and output results\n",
    "    os.makedirs(os.path.join(inputpath_base, '03_results',modelname,region), exist_ok=True)\n",
    "    data_pre.to_csv(os.path.join(inputpath_base,  '03_results',modelname,region,str(Forecastyear)+'.csv'))\n",
    "    df_importances.to_csv(os.path.join(inputpath_base,  '03_results',modelname,region,f'lstm_importance_all in {region}.csv'))\n",
    "    \n",
    "     ############# Compare if the prediction model accuracy is better than ZeroR algorithm #######################################\n",
    "    Test_year = Forecastyear\n",
    "    data1 = data[['year','actual_yield','idJoin']].pivot(index='idJoin', columns='year', values='actual_yield')\n",
    "    train_df = data1.iloc[:,0:Forecastyear-startyear]\n",
    "    test_df = data1.loc[:,Forecastyear]\n",
    "    \n",
    "    train_means = train_df.mean(axis=1)\n",
    "    combined = pd.concat([train_means.reset_index(drop=True), test_df.reset_index(drop=True)], axis=1)\n",
    "    combined_cleaned = combined.dropna()  # Remove any rows containing NaN\n",
    "    combined_cleaned.columns = ['predicts','records']\n",
    "    \n",
    "    folder_path =  os.path.join(inputpath_base,  '06_figure2','results', modelname,yield_type)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    filename = os.path.join(folder_path,f'null model scatter in {region} in {Forecastyear}.tiff')\n",
    "    plt_scatter(combined_cleaned,filename) # Plot prediction scatter plot\n",
    "    # Plot feature importance using permutation method\n",
    "    \n",
    "    filename = os.path.join(folder_path,f'prediction scatter in {region} in {Forecastyear}.tiff')\n",
    "    plt_scatter(data_pre,filename) # Plot prediction scatter plot\n",
    "\n",
    "\n",
    "    # Permutation Feature Importance（PFI）计算因子重要性\n",
    "    oof_preds = model.predict(x_val_lstm, verbose=0).squeeze() \n",
    "    baseline_mae = np.mean(np.abs(oof_preds-y_val))\n",
    "    results.append({'feature':'BASELINE','mae':baseline_mae}) \n",
    "    results=[]# Store feature importance ranking\n",
    "    for k in tqdm(range(len(feature_all))):\n",
    "        # SHUFFLE FEATURE K\n",
    "        save_col = x_val_lstm[:,:,k].copy()\n",
    "        np.random.shuffle(x_val_lstm[:,:,k])      \n",
    "        # COMPUTE OOF MAE WITH FEATURE K SHUFFLED\n",
    "        oof_preds = model.predict(x_val_lstm, verbose=0).squeeze() \n",
    "        mae = np.mean(np.abs(oof_preds-y_val))\n",
    "        results.append({'feature':feature_all[k],'mae':mae})\n",
    "        x_val_lstm[:,:,k] = save_col\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values('mae')\n",
    "    df.to_csv(os.path.join(inputpath_base,  '03_results',modelname,yield_type,region,f'lstm_importance{Forecastyear}.csv'), index=False)\n",
    "    df.rename(columns={'mae':Forecastyear}, inplace=True)\n",
    "    df.set_index('feature', inplace=True)\n",
    "    df_importances = pd.concat([df, df_importances], axis=1)\n",
    "    filename = os.path.join(folder_path,f'importance in {region} in {Forecastyear}.tiff')\n",
    "    plot_importance(df_importances, inputpath_base, modelname, region,filename) \n",
    "    # Plot feature importance using permutation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2270755-69e7-4bca-abc1-0774cee20966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2623ffbf-3fc6-48d3-a700-d4d49d6089a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "超参数具体设置:\n",
      "==================================================\n",
      "• LSTM 层数: 3\n",
      "• 每层神经元数: 100 (所有层相同)\n",
      "• Dropout 丢弃率: 0.2 (所有层相同)\n",
      "• 优化器类型: Adam\n",
      "• 学习率: 0.001000\n",
      "\n",
      "• 输入维度: (19, 17)\n",
      "• 输出维度: (1,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# 2. Extract specific hyperparameter values\n",
    "\n",
    "model_path = os.path.join(outpathmodel,region+'my_lstm_model.keras')\n",
    "model = load_model(model_path)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Hyperparameter settings:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get LSTM layers and Dropout layers\n",
    "lstm_layers = [l for l in model.layers if 'lstm' in l.name]\n",
    "dropout_layers = [l for l in model.layers if 'dropout' in l.name]\n",
    "\n",
    "# Print key parameters\n",
    "print(f\"• Number of LSTM layers: {len(lstm_layers)}\")\n",
    "print(f\"• Number of neurons per layer: {lstm_layers[0].units} (same for all layers)\")\n",
    "\n",
    "if dropout_layers:\n",
    "    print(f\"• Dropout rate: {dropout_layers[0].rate:.1f} (same for all layers)\")\n",
    "else:\n",
    "    print(\"• Dropout rate: not used\")\n",
    "\n",
    "# 3. Get optimizer parameters\n",
    "optimizer = model.optimizer\n",
    "if optimizer:\n",
    "    print(f\"• Optimizer type: {optimizer.__class__.__name__}\")\n",
    "    print(f\"• Learning rate: {optimizer.learning_rate.numpy():.6f}\")\n",
    "else:\n",
    "    print(\"• Optimizer information: not saved\")\n",
    "\n",
    "# 4. Get input/output dimensions\n",
    "input_shape = model.input_shape[1:]\n",
    "output_shape = model.output_shape[1:]\n",
    "print(f\"\\n• Input dimensions: {input_shape}\")\n",
    "print(f\"• Output dimensions: {output_shape}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2b1d2-2a07-4580-9e7a-b3ea034ac35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572aa5f-b48f-4aed-b22c-e378ea96ca1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8545f4-9969-4340-a653-bd15138de58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9f37f-f295-4b70-ae82-206e93483696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3567c-e146-4e4d-9977-1ea6598f286b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53172a18-dcb5-4785-8d6e-d012ea2a10f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5802dda5-1761-4d05-b67e-a68d833400b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d002c-e2a0-459f-a687-88374490cc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a3f15-bc58-4ce4-ad21-49ff1a1f2a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a89ad8-28bd-4091-9d51-82ea676ac2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184a029-2528-43c9-af2c-3d8b0bbab583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa5d71-aec9-41e0-b461-be518c9741cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c18c92-078b-484c-ae68-b6ff79d7d7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abad2c0-8a59-4261-b0d3-6a2a3309b856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29f181-89c4-427c-9da7-744cb5ec9fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0f25b-cd92-482e-8164-db64e01a17d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1ac82-841a-4d77-89c4-056e3f22e5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c0a117-0431-4d73-8015-d61122649ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 16:32:56,442] A new study created in RDB with name: China_LSTM_regionII_1007\n"
     ]
    }
   ],
   "source": [
    "# Parameter Optimization\n",
    "'''\n",
    "storage_name =  f\"sqlite:///{country}_{country}_{modelname}_region{region}_1007.db\"\n",
    "study_name = f\"{country}_{modelname}_region{region}_1007\"\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(), study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "study.optimize(objective_LSTM, n_trials=100, n_jobs=4)  # Use 16-core CPU for parallel computing\n",
    "# Output best hyperparameters\n",
    "print('Best parameters: ', study.best_params)\n",
    "print('Best MSE: ', study.best_value)\n",
    "\n",
    "# Load existing study\n",
    "import matplotlib.pyplot as plt\n",
    "optuna.visualization.plot_param_importances(study)\n",
    "plt.show()\n",
    "# Visualize convergence\n",
    "df = study.trials_dataframe()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['number'], df['value'])\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Convergence Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f21fb-fee1-4fb8-92ec-cdef40e8fe69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e153cebd-6b35-4efb-970d-370a42327e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05078a10-2173-4f83-a9b0-40c330db7233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a531a1c-9b44-4aba-b2af-bd39b024bdff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053c05a-5981-4768-933a-3b2db57d3473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a73b85-fdfb-4795-b711-fe7dc2d8d4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91ac12-9968-4928-a9c4-9f54fce07ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a23f7353-cfcb-497b-8cf9-599b76c87119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20d39cb7-cf19-446e-8c9b-e14520ae7654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242565c3-7f71-46d9-aad1-a3f406c6a3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68a0173-42f6-4b1b-ae84-39aa8f074f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec3dfb-0b9d-40fe-9f69-78199076b212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfbf59e4-f57d-4a47-bb68-c55d11d501ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75b1c1-5f23-47b0-89bf-86f2ac53e2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1f2abb-5943-4047-9fc2-abf006c74e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb1052-c3f9-48ef-9fc0-58a3c26b4f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c2cab-20e1-45dd-8c37-d0ac56d72b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c86bd-66c2-4602-b6d4-d757af8fc502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323ac9c-c2ff-42c4-a6b4-7976cb226708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8847819-d127-4c5d-9874-93c73d108a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a42d5-2fcb-4343-adea-889fbb33fe5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa5154-d0e6-42f5-a095-c4e5053e228f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4425836-86a2-4a73-95e1-1b744d430e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf03a6-4985-4f16-85ea-f5b9e2cc31ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d5783-426c-4e80-a858-0779d809a9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf713424-19bc-4e3d-9b21-047a2f6933b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4f84f-a4c1-4eaf-8171-9bf57d6f048c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf54ae-0b17-4992-93fb-a3c0a7909b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efc902-d00c-49be-9388-a9c28d0f2de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
