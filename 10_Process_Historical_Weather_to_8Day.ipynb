{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf754451-2b53-478a-9d67-fc3c5623fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure both forecast data and S2S data into annual data required by the LSTM model\n",
    "# Output to 04_mergeData, including two folders:\n",
    "# 01_ECMWF; 02_histdata [mean and similar]\n",
    "\n",
    "def process_climate_data(data_new, year, T_upper, T_lower, dynamic_features):\n",
    "    # Select columns\n",
    "    Tmin_columns = [col for col in data_new.columns if '_Tmin' in col]\n",
    "    Tmin = data_new[Tmin_columns].values\n",
    "    Tmean_columns = [col for col in data_new.columns if '_Tmean' in col]\n",
    "    Tmean = data_new[Tmean_columns].values\n",
    "    Tmax_columns = [col for col in data_new.columns if '_Tmax' in col]\n",
    "    Tmax = data_new[Tmax_columns].values\n",
    "    Pre_columns = [col for col in data_new.columns if '_Pre' in col]\n",
    "    Pre = data_new[Pre_columns].values\n",
    "    \n",
    "    # Calculate date range\n",
    "    days = Pre.shape[1]\n",
    "    dates = pd.date_range(start=str(year) + '-01-01', periods=days, freq='D')\n",
    "    \n",
    "    # Add year information\n",
    "    data_new['year'] = year\n",
    "    \n",
    "    # Calculate extreme meteorological indicators\n",
    "    spei_df = spei(dates, Pre, Tmean)\n",
    "    CDD_df, HDD_df, GDD_df = extreme_temperature(dates, Tmax, Tmin, T_upper, T_lower)\n",
    "    \n",
    "    # Aggregate 8-day data\n",
    "    data_new1 = aggre_8days(dynamic_features, dates, data_new)\n",
    "    \n",
    "    # Merge all data\n",
    "    data_new1 = pd.concat([CDD_df, HDD_df, GDD_df, spei_df, data_new1], axis=1)\n",
    "    \n",
    "    return data_new1\n",
    "    \n",
    "def find_weeks(forecastDataList, week_dates):\n",
    "    result = []\n",
    "    # Iterate through each date in forecastDataList\n",
    "    for date in forecastDataList:\n",
    "        # Iterate through week_dates to find the week of the date\n",
    "        for i in range(len(week_dates) - 1):\n",
    "            # Check if the date is within the current date range (inclusive of lower bound, exclusive of upper bound)\n",
    "            if week_dates[i] <= date < week_dates[i + 1]:\n",
    "                result.append((date, i + 1))  # week 1 corresponds to index 0, so week number is i + 1\n",
    "                break\n",
    "        # Handle dates beyond the last date range (i.e., week 46 range)\n",
    "        else:\n",
    "            if date >= week_dates[-1]:\n",
    "                result.append((date, len(week_dates)))  # Last week: week 46\n",
    "    result = {date: week for date, week in result}\n",
    "    return result\n",
    "\n",
    "def update_S2Sandhist_VI(data_S2S_new_all_new, VI_select2, result, years, start_point, harvest_point, outpath_S2S,ii,type):\n",
    "    # Set index\n",
    "  #  data_S2S_new_all_new.set_index(['year', 'idJoin'], inplace=True)\n",
    "\n",
    "    # Filter columns containing VI_select2\n",
    "    filtered_columns = [col for col in data_S2S_new_all_new.columns if VI_select2 in col]\n",
    "    data_S2S_VI = data_S2S_new_all_new[filtered_columns].reset_index()\n",
    "\n",
    "    # Initialize updated DataFrame\n",
    "    update_VI = pd.DataFrame()\n",
    "\n",
    "    # Update data year by year\n",
    "    for year in years:\n",
    "        week_forecast = result[ii]\n",
    "        forecast_weeklist = range(week_forecast, harvest_point + 1)\n",
    "        actual_weeklist = range(start_point, week_forecast)\n",
    "        \n",
    "        forecast_weeklist = [f'Week{week}{VI_select2}' for week in forecast_weeklist]\n",
    "        before_weeklist = [f'Week{week}{VI_select2}' for week in actual_weeklist]\n",
    "\n",
    "        # Calculate historical mean and forecast mean for the current year\n",
    "        data_S2S_VI_before = data_S2S_VI[before_weeklist + ['year']].groupby('year').mean()\n",
    "        data_S2S_VI_forecast = data_S2S_VI[forecast_weeklist + ['year']].groupby('year').mean()\n",
    "        \n",
    "        # Extract data for the current year\n",
    "        current_S2S_VI = data_S2S_VI[data_S2S_VI['year'] == year]\n",
    "\n",
    "        # Calculate DTW distances\n",
    "        dtw_distances = {}\n",
    "        for year1 in years:\n",
    "            current_S2S_VI_before = data_S2S_VI_before.loc[year]\n",
    "            if year1 < year: # Only use previous years for forecast\n",
    "                other_S2S_VI_before = data_S2S_VI_before.loc[year1]\n",
    "                distance, path = fastdtw(current_S2S_VI_before, other_S2S_VI_before)\n",
    "                dtw_distances[year1] = distance\n",
    "\n",
    "        # Find the most similar year\n",
    "        most_similar_by_dtw = min(dtw_distances, key=dtw_distances.get)\n",
    "        dataVI_similaryear = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw]\n",
    "\n",
    "        # Update forecast week data for the current year\n",
    "        current_S2S_VI[forecast_weeklist] = dataVI_similaryear[forecast_weeklist].values\n",
    "        current_S2S_VI['year'] = year\n",
    "        current_S2S_VI['idJoin'] = dataVI_similaryear['idJoin']\n",
    "\n",
    "        # Merge updated data\n",
    "        update_VI = pd.concat([update_VI, current_S2S_VI], axis=0)\n",
    "\n",
    "    # Reset index for the updated data\n",
    "    update_VI.set_index(['year', 'idJoin'], inplace=True)\n",
    "    data_S2S_new_all_new[forecast_weeklist] = update_VI[forecast_weeklist].values\n",
    "\n",
    "    # Save results\n",
    "    output_path = os.path.join(outpath_S2S, 'data_'+type+'.csv')\n",
    "    data_S2S_new_all_new.to_csv(output_path)\n",
    "    print(f\"Updated data saved to {output_path}\")\n",
    "\n",
    "    return data_S2S_new_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15d54a88-9c0e-46b1-995d-09f06242b0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: F:\\SCI\\SCI9_1\\01_code\\02_Wheat\\06_India\n",
      "Current folder name: 06_India\n",
      "Parent folder name: 02_Wheat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os\n",
    "root_directory = os.getcwd()[0:3]\n",
    "sys.path.append(root_directory+'\\\\SCI\\\\SCI9_1\\\\01_code')\n",
    "sys.path.append(r'C:\\ProgramData\\anaconda3\\Lib\\site-packages') \n",
    "sys.path.append(r'C:\\Users\\DELL\\.conda\\envs\\myenv\\Lib\\site-packages') \n",
    "sys.path.append(r'C:\\Users\\DELL\\.conda\\envs\\rasterio_env\\Lib\\site-packages') \n",
    "from functions import spei,extreme_temperature,aggre_8days,extract_dates\n",
    "from sklearn.metrics import mean_absolute_percentage_error, accuracy_score, roc_auc_score, roc_curve,r2_score,mean_squared_error\n",
    "from functions import calculate_rrmse1,calculate_rrmse2,calculate_acc,calculate_nrmse,calculate_mare,extract_selected_variables\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "\n",
    "VIs =  ['_KNDVI' ,'_EVI','_NDVI']\n",
    "Cilmate = ['_Pre' ,'_Tmin' ,'_Solar','_Tmean','_Tmax']\n",
    "Climate_Exogenous  = ['_CDD' ,'_HDD' ,'_GDD','_VPD','_wind_speed','_SPEI'] #'_VPD','_wind_speed',\n",
    "soil_feature = [ 'SAND','AWC', 'SILT','ORG_CARBON',  'TOTAL_N', 'PH_WATER',  'CEC_SOIL', 'CLAY']\n",
    "loc_feature = ['elevation', 'lat', 'lon']\n",
    "Year_feature = ['year'];union_feature = ['idJoin'];\n",
    "dynamic_features = [ '_KNDVI' ,'_EVI','_NDVI','_Pre' ,'_Tmin' ,'_Solar','_Tmean','_VPD', '_wind_speed' ,'_Tmax']\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ast\n",
    "from fastdtw import fastdtw\n",
    " \n",
    "# Get current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)\n",
    " \n",
    "# Get current folder name\n",
    "current_folder_name = os.path.basename(current_directory)\n",
    "print(\"Current folder name:\", current_folder_name)\n",
    " \n",
    "# Get parent folder name\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "parent_folder_name = os.path.basename(parent_directory)\n",
    "print(\"Parent folder name:\", parent_folder_name)\n",
    "\n",
    "crop = parent_folder_name;countryID =current_folder_name\n",
    "# Variables that need to be changed\n",
    "country = countryID.split('_')[1]\n",
    "##############Region Configuration#############################################\n",
    "inpath_dates_other = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID+'\\\\'+'01_data'+'\\\\'+'07_Information'\n",
    "other_infornamtion = pd.read_csv(os.path.join(inpath_dates_other,'information.txt'), sep=' ', header=None)\n",
    "startyear,endyear,shp_name = other_infornamtion.iloc[0,0],other_infornamtion.iloc[0,1],other_infornamtion.iloc[0,2]\n",
    "\n",
    "inputpath_base = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID\n",
    "\n",
    "\n",
    "Forecastyear = endyear\n",
    "\n",
    "\n",
    "years = range(startyear,endyear+1)\n",
    "regions = ['I']#\n",
    "Forecastyears = {\n",
    "    'I': endyear, \n",
    "}\n",
    "# Define temperature thresholds by crop type\n",
    "if crop == '02_Wheat':\n",
    "    T_upper = 34\n",
    "    T_lower = 0\n",
    "elif crop == '01_Maize':  # Fixed spelling error\n",
    "    T_upper = 30\n",
    "    T_lower = 8\n",
    "elif crop == '03_Rice':\n",
    "    T_upper = 35\n",
    "    T_lower = 8    \n",
    "else:\n",
    "    T_upper = 30\n",
    "    T_lower = 10\n",
    "\n",
    "\n",
    "\n",
    "inputpath_base = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID+'\\\\'\n",
    "institution = 'ECMWF';ECMWF_path = os.path.join(inputpath_base,'02_S2S')\n",
    "\n",
    "file_path = os.path.join(inputpath_base, '02_S2S', '01_dataori', 'ECMWF','CommonYear_Week.txt')\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = [line.strip() for line in file.readlines()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8892e811-2d2b-4bfa-8cd1-c50fa24f38c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70590a93-7268-4b6f-a8ea-8209a67de9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "[20250101]\n",
    "# 1. Historical data; modify its header\n",
    "[20250106]\n",
    "# 1. Fixed minor issues in the case of start_point < harvest_point, mainly incorrect replacement of before data\n",
    "# 2. Added handling for start_point > harvest_point (i.e., cross-year scenarios)\n",
    "'''\n",
    "\n",
    "\n",
    "for region in regions:\n",
    "    Forecastyear = Forecastyears[region]\n",
    "    hist_outputpath = os.path.join(inputpath_base,'02_S2S','03_outputData','02_histdata',region)\n",
    "    os.makedirs(hist_outputpath,exist_ok=True)\n",
    "    pre_name = 'Wheat_'+region+'_';\n",
    "    hist_inputpath = os.path.join(inputpath_base,'01_data','04_GEEdownloadData','02_histdata')\n",
    "    data = pd.read_csv(os.path.join(hist_inputpath,pre_name+str(1990)+'.csv'));\n",
    "    data.columns = data.columns.str.replace(rf'^{1990}', '', regex=True)\n",
    "    columns_sta = data.columns\n",
    "    hist_start_year = Forecastyear-31;hist_end_year = Forecastyear-1;\n",
    "    allhist = pd.DataFrame()\n",
    "    \n",
    "    for year_hist in range(hist_start_year,hist_end_year+1):\n",
    "        data = pd.read_csv( os.path.join(hist_inputpath,pre_name+str(year_hist)+'.csv'));\n",
    "        data.columns = data.columns.str.replace(rf'^{year_hist}', '', regex=True)\n",
    "        data = data[columns_sta]\n",
    "        data['idGroup'] = data['idJoin']\n",
    "        data.drop(['idGroup', 'iso3', '.geo','system:index'], axis=1, inplace=True)#'idGroup', \n",
    "        # data.drop(['idGroup', 'iso3', 'lat', 'lon', '.geo','system:index'], axis=1, inplace=True)#'idGroup', \n",
    "        data.columns = str(Forecastyear) + data.columns\n",
    "        data.rename(columns={f\"{Forecastyear}idJoin\": \"idJoin\"}, inplace=True)\n",
    "        data.to_csv(os.path.join(hist_outputpath,'hist_'+str(year_hist)+'.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937e9183-7997-4794-9ec1-3118954e41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47a403a-9b3c-4e0e-b991-369bd4729f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "[20250106]\n",
    "# 1. Fixed minor issues in the case of start_point < harvest_point, mainly incorrect replacement of before data\n",
    "# 2. Added handling for start_point > harvest_point (i.e., cross-year scenarios)\n",
    "\n",
    "[20250107]\n",
    "# 1. Fixed cross-year issues: For cross-year growth, the yield data from the start to week 16 should be from the previous year, not the current year\n",
    "'''\n",
    "\n",
    "'''\n",
    "[20250106]\n",
    "# 1. Fixed minor issues in the case of start_point < harvest_point, mainly incorrect replacement of before data\n",
    "# 2. Added handling for start_point > harvest_point (i.e., cross-year scenarios)\n",
    "\n",
    "[20250107]\n",
    "# 1. Fixed cross-year issues: For cross-year growth, the yield data from the start to week 16 should be from the previous year, not the current year\n",
    "\n",
    "\n",
    "[20250316]\n",
    "\n",
    "Fixed historical data errors. Based on the revised code for Pakistan, currently the US, Pakistan, Europe, Argentina, Australia, Canada, and India have been corrected.\n",
    "Dual-region countries: Russia and the US\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "for region in regions:\n",
    "        # Read selected variables for subsequent variable filtering\n",
    "        Forecastyear = Forecastyears[region]\n",
    "        SelFeature_infornamtion = extract_selected_variables(inputpath_base)\n",
    "        TimeFeatures_sel, Static_sel, regionID = SelFeature_infornamtion[SelFeature_infornamtion['regionID'] == region].iloc[0]\n",
    "        # Actual modeling weeks\n",
    "        inpath_dates = os.path.join(inputpath_base, '01_data','05_buildmodel', '02_extractdates','gs_three_periods.txt')\n",
    "        gs_infornamtion = pd.read_csv(inpath_dates, delim_whitespace=True, header=None)\n",
    "        gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point', 'VI_select2','regionID']\n",
    "        start_point, peak, harvest_point, VI_select2, region = gs_infornamtion[gs_infornamtion['regionID'] == region].iloc[0]\n",
    "        print(harvest_point)\n",
    "        # Data reading and index filtering\n",
    "        data_ori_all = pd.read_csv(os.path.join(inputpath_base, '01_data','05_buildmodel','01_weekdata',region+'_allweekYielddata_VIs.csv'))\n",
    "        data_ori_all = data_ori_all.drop_duplicates(subset=['year', 'idJoin'],keep='last')\n",
    "        Static_sel= [col for col in Static_sel if 'year.1' not in col] \n",
    "        TimeFeatures_sel_all= [col for col in data_ori_all.columns if any(feature in col for feature in TimeFeatures_sel)]\n",
    "        TimeFeatures_sel_all= [col for col in TimeFeatures_sel_all if 'Previous_Yield' not in col] # Note: Previous year's yield may be filtered due to 'Pre' in precipitation, need careful verification\n",
    "        filtered_columns_all = TimeFeatures_sel_all+Static_sel\n",
    "        data_ori_all = data_ori_all[filtered_columns_all+['idJoin','Yield']] # Filter selected variables for subsequent analysis\n",
    "    \n",
    "        \n",
    "        # Filter VI for subsequent identification\n",
    "        filtered_columns_VI = [col for col in data_ori_all.columns if VI_select2 in col]\n",
    "        data_S2S_VI = data_ori_all[filtered_columns_VI + ['year','idJoin']]\n",
    "        data_S2S_VI_mean = data_S2S_VI[filtered_columns_VI + ['year']].groupby('year').mean()\n",
    "        if start_point < harvest_point: # Same-year growth\n",
    "            hisWeekList = ['leadweek_'+str(week) for week in range(1,harvest_point-start_point+1)] # The set hisWeekList does not seem to include the start_point week\n",
    "        else:\n",
    "            hisWeekList = ['leadweek_'+str(week) for week in range(1,harvest_point-start_point+1+46)]+['leadweek_'+str(week) for week in range(1,harvest_point-start_point+1)]\n",
    "        hist_inputpath = os.path.join(inputpath_base,'02_S2S','03_outputData','02_histdata',region)\n",
    "        data_ori_current = data_ori_all[data_ori_all['year']==Forecastyear]\n",
    "        hist_start_year = Forecastyear-30;hist_end_year = Forecastyear-1;\n",
    "    \n",
    "        for year_hist in range(hist_start_year,hist_end_year+1):\n",
    "            # Process data for the current historical year\n",
    "            data_his_new_ori = pd.read_csv(os.path.join(hist_inputpath,'hist_'+str(year_hist)+'.csv'))\n",
    "            data_his_new_ori = data_his_new_ori.drop_duplicates(subset=['idJoin'],keep='last')\n",
    "            data_his_new_ori.set_index('idJoin', inplace=True)\n",
    "            data_his_new_ori['year'] = Forecastyear\n",
    "            data_ori_all = data_ori_all.drop_duplicates(subset=['year', 'idJoin'],keep='last')\n",
    "            data_his_new = data_his_new_ori.copy()\n",
    "            data_his_new = process_climate_data(data_his_new.reset_index(), Forecastyear, T_upper, T_lower, dynamic_features)\n",
    "            data_his_new = data_his_new.dropna(how='all',axis=1) # process_climate_data will introduce all vegetation indices\n",
    "            # The data calculated by data_his_new has errors (missing values not handled), replace with previously interpolated modeling data\n",
    "            hist_outputpath1 = os.path.join(inputpath_base,'02_S2S','05_WeekData','02_hist',region)\n",
    "            os.makedirs(hist_outputpath1,exist_ok=True)\n",
    "            data_his_new.index=data_his_new_ori.index\n",
    "            data_his_new.to_csv(os.path.join(hist_outputpath1,'hist_'+str(year_hist)+'.csv'))\n",
    "            \n",
    "            data_his_new_update = data_ori_current.copy()\n",
    "            #data_his_new = data_his_new.merge(data_ori_current[filtered_columns_VI+Static_sel+['idJoin','Yield']],on='idJoin',how='inner')# Update VI, static variables and Yield to maintain consistent data types\n",
    "            data_his_new['year'] = Forecastyear \n",
    "\n",
    "\n",
    "            for ii in hisWeekList:\n",
    "                # data_his_new = data_his_new[filtered_columns_all+['idJoin']]\n",
    "                ############################################## Fill with most similar vegetation indices, non-cross-year, start_point < harvest_point####\n",
    "                if int(ii[9:])>harvest_point: # Only need to distinguish whether the forecast week (week_forecast) is cross-year\n",
    "                    week_forecast = harvest_point+1-int(ii[9:])+46\n",
    "                else:\n",
    "                    week_forecast = harvest_point+1-int(ii[9:])\n",
    "\n",
    "                if start_point < harvest_point:  \n",
    "                    # Planting and harvesting in the same year\n",
    "                    forecast_weeklist1 = range(week_forecast, harvest_point + 1)\n",
    "                    V1= [f'Week{week}{VI_select2}' for week in range(1, week_forecast)]; # Vegetation indices before the forecast period\n",
    "                    V2= [f'Week{week}{VI_select2}' for week in forecast_weeklist1];# Indices to be forecasted # Forecast from current week to harvest week (week_forecast not included)\n",
    "            \n",
    "                    current_S2S_VI_before =data_S2S_VI_mean.loc[Forecastyear][V1]\n",
    "                    dtw_distances = {}\n",
    "                    for year1 in range(startyear,Forecastyear):# Will not include the start year to the year before Forecastyear\n",
    "                        other_S2S_VI_before = data_S2S_VI_mean.loc[year1][V1]\n",
    "                        distance, path = fastdtw(current_S2S_VI_before, other_S2S_VI_before)# Forecast from current week to harvest week\n",
    "                        dtw_distances[year1] = distance\n",
    "                    most_similar_by_dtw = min(dtw_distances, key=dtw_distances.get) # \n",
    "                    data_S2S_VI_forecast2 = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw][V2+['idJoin']]# Only data needed for modeling\n",
    "                    data_his_new_update = data_his_new_update.drop(V2,axis=1) # Delete original columns corresponding to forecast dates, keep non-forecasted ones\n",
    "                    data_his_new_update = data_his_new_update.merge(data_S2S_VI_forecast2,on='idJoin',how='inner')\n",
    "                else:\n",
    "                    ############################################## Fill with most similar vegetation indices, cross-year, start_point > harvest_point####\n",
    "                    week_forecast = harvest_point+1-int(ii[9:]) # \n",
    "                    if week_forecast<=0: # Forecast period is in the previous year (calculated as negative); add 46\n",
    "                        week_forecast = harvest_point+1-int(ii[9:])+46 # \n",
    "                    else:\n",
    "                        week_forecast = week_forecast\n",
    "\n",
    "                    if week_forecast<=harvest_point: # = indicates the first week of the current year\n",
    "                        # Use full-year data because cross-year growth may start in week 1, requiring similar years from the previous year;\n",
    "                        # Different from same-year reproductive period which usually has a sequence before the forecast\n",
    "                        forecast_weeklist1 = range(week_forecast, harvest_point+1)\n",
    "                        V1_1 = [f'Week{week}{VI_select2}' for week in range(1, 46+1)];\n",
    "                        V1_2 = [f'Week{week}{VI_select2}' for week in range(1, week_forecast)];\n",
    "                        V2 = [f'Week{week}{VI_select2}' for week in forecast_weeklist1];\n",
    "                        current_S2S_VI_before =pd.concat([data_S2S_VI_mean.loc[Forecastyear][V1_1], data_S2S_VI_mean.loc[Forecastyear-1][V1_2]])\n",
    "                        dtw_distances = {}\n",
    "\n",
    "                        for year1 in range(startyear+1,Forecastyear):# Requires two years of data, will not include Forecastyear\n",
    "                            other_S2S_VI_before = pd.concat([data_S2S_VI_mean.loc[year1][V1_1], data_S2S_VI_mean.loc[year1-1][V1_2]])\n",
    "                            distance, path = fastdtw(current_S2S_VI_before, other_S2S_VI_before)# Forecast from current week to harvest week\n",
    "                            dtw_distances[year1] = distance\n",
    "\n",
    "                        most_similar_by_dtw = min(dtw_distances, key=dtw_distances.get) \n",
    "\n",
    "                        # Only replace modeling data for the current year\n",
    "\n",
    "                        data_S2S_VI_forecast2 = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw][V2+['idJoin']]\n",
    "                        data_his_new_update = data_his_new_update.drop(V2,axis=1) # Delete original columns corresponding to forecast dates, keep non-forecasted ones\n",
    "                        data_his_new_update = data_his_new_update.merge(data_S2S_VI_forecast2,on='idJoin',how='inner')\n",
    "\n",
    "                    else:  \n",
    "                        # Cross-year, replacement covers list(range(week_forecast, 46)) + list(range(1, harvest_point + 1))\n",
    "\n",
    "                        forecast_weeklist1 = list(range(week_forecast, 46+1))+list(range(1,harvest_point + 1))\n",
    "                        \n",
    "                        V1_1= [f'Week{week}{VI_select2}' for week in range(1, week_forecast)]; # Previous year\n",
    "                        V2_1 =  [f'Week{week}{VI_select2}' for week in range(week_forecast, 46+1)]; # Previous year\n",
    "                        V2_2 =  [f'Week{week}{VI_select2}' for week in range(1,harvest_point + 1)]; # Current year\n",
    "\n",
    "                        current_S2S_VI_before =data_S2S_VI_mean.loc[Forecastyear-1][V1_1]\n",
    "                        dtw_distances = {}\n",
    "                        for year1 in range(startyear+1,Forecastyear-1):# Will not include Forecastyear\n",
    "                            other_S2S_VI_before = data_S2S_VI_mean.loc[year1-1][V1_1]\n",
    "                            distance, path = fastdtw(current_S2S_VI_before, other_S2S_VI_before)# Forecast from current week to harvest week\n",
    "                            dtw_distances[year1] = distance\n",
    "                        most_similar_by_dtw = min(dtw_distances, key=dtw_distances.get) # Find the most similar year (e.g., 2016)\n",
    "                            \n",
    "                        data_S2S_VI_forecast1 = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw][V2_1+['idJoin']]# Only data needed for modeling\n",
    "                        data_S2S_VI_forecast2 = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw+1][V2_2]# Only data needed for modeling\n",
    "                        data_S2S_VI_forecast2 = pd.concat([data_S2S_VI_forecast1.reset_index(drop=True), data_S2S_VI_forecast2.reset_index(drop=True)], axis=1)# Horizontal concatenation\n",
    "                        data_his_new_update = data_his_new_update.drop(V2_1+V2_2,axis=1) # Delete original columns corresponding to forecast dates, keep non-forecasted ones\n",
    "                        data_his_new_update = data_his_new_update.merge(data_S2S_VI_forecast2,on='idJoin',how='inner')               \n",
    "                data_his_new_update.set_index('idJoin', inplace=True)\n",
    "                ############################################## Replace forecast weeks in original data with historical data ###################################################################\n",
    "                if week_forecast<=harvest_point: # Indicates non-cross-year; only use current year data for replacement\n",
    "                    update_climate = []\n",
    "                    for feature in [feature for feature in TimeFeatures_sel if feature != VI_select2[1:]]: # Selected meteorological data excluding vegetation indices\n",
    "                        update_climate += [f'Week{week}_{feature}' for week in forecast_weeklist1] # Note: Cross-year scenarios may exist, requiring attention\n",
    "                        \n",
    "                    data_his_new_update[update_climate] = data_his_new[update_climate] # Replace original data with historical data\n",
    "                else: # Cross-year requires historical data from the previous year\n",
    "                    # Read and process data from the previous year for replacement (weeks from week_forecast to 46)\n",
    "                    data_his_new_ori_lastyear = pd.read_csv(os.path.join(hist_inputpath,'hist_'+str(year_hist-1)+'.csv'))\n",
    "                    data_his_new_ori_lastyear.set_index('idJoin', inplace=True)\n",
    "                    data_his_new_ori_lastyear['year'] = Forecastyear\n",
    "                    data_his_new_lastyear = data_his_new_ori_lastyear.copy()\n",
    "                    data_his_new_lastyear = process_climate_data(data_his_new_lastyear.reset_index(), Forecastyear, T_upper, T_lower, dynamic_features)\n",
    "                    data_his_new_lastyear = data_his_new_lastyear.dropna(how='all',axis=1) # process_climate_data will introduce all vegetation indices\n",
    "\n",
    "                    # Previous year: range(week_forecast, 46)\n",
    "                    update_climate1 = []\n",
    "                    for feature in [feature for feature in TimeFeatures_sel if feature != VI_select2[1:]]: # Selected meteorological data excluding vegetation indices\n",
    "                        update_climate1 += [f'Week{week}_{feature}' for week in list(range(week_forecast, 46))] # \n",
    "                    data_his_new_update[update_climate1] = data_his_new_lastyear[update_climate1] # Replace original data with historical data\n",
    "                    \n",
    "                    # Current year: 1 to harvest_point\n",
    "                    update_climate2 = []\n",
    "                    for feature in [feature for feature in TimeFeatures_sel if feature != VI_select2[1:]]: # Selected meteorological data excluding vegetation indices\n",
    "                        update_climate2 += [f'Week{week}_{feature}' for week in list(range(1, harvest_point + 1))] # \n",
    "                    data_his_new_update[update_climate2] = data_his_new[update_climate2] # Replace original data with historical data\n",
    "                ############################################## Filter variables for the growing season ############################################################################\n",
    "                data_his_new_update = data_his_new_update.reset_index()\n",
    "                weeks = []\n",
    "                # Determine if cross-year\n",
    "                if start_point < harvest_point:  # Non-cross-year\n",
    "                    for feature in TimeFeatures_sel:\n",
    "                        # Generate combinations of weeks and features using list comprehension\n",
    "                        weeks += [f'Week{week}_{feature}' for week in range(start_point, harvest_point + 1)]\n",
    "                    gs_features = weeks + Static_sel+['Yield']+['idJoin']\n",
    "                    data_his_new_update = data_his_new_update[gs_features]\n",
    "                else:  # Cross-year\n",
    "                    for feature in TimeFeatures_sel:\n",
    "                        # Merge two ranges and generate combinations of weeks and features\n",
    "                        weeks += [f'Week{week}_{feature}' for week in list(range(start_point, 47)) + list(range(1, harvest_point + 1))]\n",
    "                     # For cross-year growth, the previous part of the yield should be replaced with data from the previous year (correct in _data_ori; but newly generated data is still from the current year)\n",
    "                    data= pd.read_csv(os.path.join(inputpath_base, '01_data','05_buildmodel','03_modeldata',region+'_data_ori.csv'))\n",
    "                    data = data.drop_duplicates(subset=['year', 'idJoin'],keep='last')\n",
    "                    weeks = []\n",
    "                    for feature in TimeFeatures_sel:\n",
    "                        weeks += [f'Week{week}_{feature}' for week in list(range(start_point, 47))]     \n",
    "                    data = data[weeks+['idJoin','year']];\n",
    "                    data_his_new_update = data_his_new_update.drop(weeks,axis=1);\n",
    "                    data = data[data['year']==Forecastyear]\n",
    "                    data_his_new_update = data_his_new_update.merge(data,on=['idJoin','year'],how='inner')  \n",
    "                # Replace with previous year's data\n",
    "                ##############################################Output ############################################################################\n",
    "                hist_outputpath = os.path.join(inputpath_base,'02_S2S','06_buildmodel','02_hist','VI_Like',region,ii)\n",
    "                os.makedirs(hist_outputpath,exist_ok=True)\n",
    "                data_his_new_update.to_csv(os.path.join(hist_outputpath,'hist_'+str(year_hist)+'.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde96cce-e477-4430-99dd-3146a4420b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0064f3-1ce5-451c-acaa-50702b68975c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c9b778-e41a-4777-bd1c-7e4c6e0c83b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce458e-9806-4865-9ed3-c28345f92eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ad3c0-2c4a-4d82-94fa-da637c414597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203a06d-0917-440c-946c-e75986fda1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba673555-31ce-4cc2-9002-c5448eaaf852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c5169-c733-48c3-b673-4fc2161c6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94585720-19e0-45ae-bcf4-8539429cf358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62368e-22fc-4f81-9b2e-15179058b719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c933b04-9806-4d50-a05e-cd663d5d66fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146e081-2d77-428a-b40d-d97de589cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba74bdd-ca6e-449e-8ac6-95fb0e8b85f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854d544-77ab-4545-bec8-b9de8eab6971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e65841-9034-4161-ac6b-df6136b17d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c702591-98d9-421c-8c1b-92c6bb1cf43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ae1ce-e7a4-4ec2-a4e0-b20adfaa8a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591477e-c876-43c9-8622-17777cb087ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "\n",
    "# Find similar years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b2b1c-a66a-46af-b284-27bcef98f432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a44fb920-1fa1-431c-8dd6-2f661f878475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# data_S2S_new_all_new['year'] == current_S2S_VI['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4919cc18-a20f-4d18-9e77-8459362eb97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5fbdf-f966-421b-aa7e-bfa743dd031d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0896bb-69ca-4f8e-8585-086973b0f84f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9928f8-9c37-481a-9af9-56547be9a6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1102e-c46e-43aa-b8c6-9f7949cdc5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f0abb-1754-4c40-956b-b8bf2b1bab05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9160d-1c84-4e9f-93c9-d7889cc3b885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f3b2a5-4e6c-45c4-9117-a3f59b7ab74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d1b2f-1d18-43d7-8f19-7ac6a4629654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e94185-ebff-4c36-94d5-007a1847f77f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f27b67-9c7c-406f-b75f-a486f9a450df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a066a011-752e-4a6f-b208-5c193228ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935feaa-c37e-411e-a38f-665e010a9bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b30446-e6b4-4775-afb9-e51d6d47dbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce59ee6-74a2-4702-bf43-679cf8490f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d2371-ddde-43b0-9562-8ea98ce2dd82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b000c49-389f-4a4c-bbac-291af0deecc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}