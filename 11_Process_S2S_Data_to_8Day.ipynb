{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "525cedda-ca1c-4440-a0bd-27acb64e8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# spei, extreme_temperature, aggre_8days 等\n",
    "\n",
    "def calculate_wind_speed_and_vpd(ds_10u, ds_10v, ds_2d, ds_2t):\n",
    "    # Rename wind speed component variables\n",
    "    ds_10u = ds_10u.rename({'u10': 'wind_speed_u'})\n",
    "    ds_10v = ds_10v.rename({'v10': 'wind_speed_v'})\n",
    "    # Calculate wind speed\n",
    "    wind_speed = np.sqrt(ds_10u['wind_speed_u']**2 + ds_10v['wind_speed_v']**2)\n",
    "    # Add wind speed to a new Dataset\n",
    "    ds_wind_speed = xr.Dataset({'wind_speed': wind_speed})\n",
    "    \n",
    "    # Rename temperature and dewpoint temperature variables\n",
    "    ds_2d = ds_2d.rename({'d2m': 'dewpoint_temperature'})\n",
    "    ds_2t = ds_2t.rename({'t2m': 'temperature'})\n",
    "    \n",
    "    # Convert temperature from Kelvin to Celsius (commented out in original code)\n",
    "    T_celsius = ds_2t['temperature']# - 273.15\n",
    "    Td_celsius = ds_2d['dewpoint_temperature']# - 273.15\n",
    "    \n",
    "    # Calculate relative humidity\n",
    "    # Calculate relative humidity and convert to percentage\n",
    "    RH = np.exp((17.625 * Td_celsius) / (Td_celsius + 243.04)) / np.exp((17.625 * T_celsius) / (T_celsius + 243.04))\n",
    "\n",
    "    # Calculate saturated vapor pressure (e_s)\n",
    "    e_s = 6.107 * 10 ** (7.5 * T_celsius / (T_celsius + 237.3))\n",
    "    \n",
    "    # Calculate actual vapor pressure (e_a)\n",
    "    e_a = e_s * RH\n",
    "    \n",
    "    # Calculate VPD (Vapor Pressure Deficit)\n",
    "    vpd = e_s - e_a\n",
    "    ds_vpd = xr.Dataset({'vpd': vpd})\n",
    "    \n",
    "    return ds_wind_speed, ds_vpd\n",
    "\n",
    "\n",
    "def calculate_days_between(start_date_str, end_date_str, date_format=\"%Y%m%d\"):\n",
    "    \"\"\"\n",
    "    Calculate the number of days between two date strings (including the day before the end date).\n",
    "    For example, from 20220901 to 20220922, the result is 21 days (excluding the last day).\n",
    "    \n",
    "    :param start_date_str: Start date string\n",
    "    :param end_date_str: End date string\n",
    "    :param date_format: Format of the date string, default is \"%Y%m%d\"\n",
    "    :return: Number of days between the two dates\n",
    "    \"\"\"\n",
    "    # Convert date strings to date objects\n",
    "    start_date = datetime.strptime(start_date_str, date_format)\n",
    "    end_date = datetime.strptime(end_date_str, date_format)\n",
    "    \n",
    "    # Calculate date difference\n",
    "    days_between = (end_date - start_date).days\n",
    "    \n",
    "    return days_between\n",
    "    \n",
    "def create_directory(path):\n",
    "    \"\"\"\n",
    "    Create a directory (and all its parent directories if they do not exist).\n",
    "    \n",
    "    :param path: Directory path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "       # print(f\"Directory '{path}' created successfully.\")\n",
    "    except OSError as error:\n",
    "        pass\n",
    "    \n",
    "def process_temperature_data(ds_pf_mn2t6, ds_pf_mx2t6):  \n",
    "    # Initialize lists to store minimum and maximum temperature values  \n",
    "    min_temps = []  \n",
    "    max_temps = []  \n",
    "      \n",
    "    # Iterate through years (empty loop body in original code)  \n",
    "\n",
    "        # Select data for the specific year  \n",
    "    ds_pf_mn2t61 = ds_pf_mn2t6#.isel(time=time1)  \n",
    "    ds_pf_mx2t61 = ds_pf_mx2t6#.isel(time=time1)  \n",
    "      \n",
    "    # Convert valid_time to pandas datetime, subtract 6 hours, then format as date string  \n",
    "    ds_pf_mn2t61['step'] = (pd.to_datetime(ds_pf_mn2t61['valid_time']) - pd.to_timedelta(6, unit='h')).strftime(\"%Y-%m-%d\")  \n",
    "    ds_pf_mx2t61['step'] = (pd.to_datetime(ds_pf_mx2t61['valid_time']) - pd.to_timedelta(6, unit='h')).strftime(\"%Y-%m-%d\")  \n",
    "      \n",
    "    # Group by step and calculate minimum and maximum values  \n",
    "    ds_pf_mn2t61_grouped = ds_pf_mn2t61.groupby('step').min()  # Note: Should be 'time' instead of 'step' (assumes 'time' is the time dimension)  \n",
    "    ds_pf_mx2t61_grouped = ds_pf_mx2t61.groupby('step').max()  \n",
    "      \n",
    "    # Add results to lists  \n",
    "    min_temps.append(ds_pf_mn2t61_grouped['mn2t6'].values)  \n",
    "    max_temps.append(ds_pf_mx2t61_grouped['mx2t6'].values)  \n",
    "      \n",
    "    # Use numpy.stack to merge data along the last axis  \n",
    "    min_temps_4d = min_temps  \n",
    "    max_temps_4d = max_temps \n",
    "      \n",
    "    # If necessary, reorder dimensions to match the expected order (e.g., time dimension first)  \n",
    "      \n",
    "    return max_temps_4d, min_temps_4d  \n",
    "\n",
    "\n",
    "def read_pf_data(pathto, origen, region, years):\n",
    "    # Read ensemble forecast minimum temperature data\n",
    "    ds_pf_mn2t6 = xr.open_dataset(f'{pathto}\\\\{origen}_mn2t6_{region}_pf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_pf_mn2t6 = ds_pf_mn2t6 - 273.15  # Convert to Celsius\n",
    "   # ds_pf_mn2t6 = ds_pf_mn2t6.mean(dim='number')  # Ensemble mean (commented out)\n",
    "    \n",
    "    # Read ensemble forecast maximum temperature data\n",
    "    ds_pf_mx2t6 = xr.open_dataset(f'{pathto}\\\\{origen}_mx2t6_{region}_pf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_pf_mx2t6 = ds_pf_mx2t6 - 273.15  # Convert to Celsius\n",
    "    #ds_pf_mx2t6 = ds_pf_mx2t6.mean(dim='number')  # Ensemble mean (commented out)\n",
    "    \n",
    "    # Read ensemble forecast dewpoint temperature data\n",
    "    ds_pf_2d = xr.open_dataset(f'{pathto}\\\\{origen}_2d_{region}_pf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_pf_2d = ds_pf_2d - 273.15  # Convert to Celsius\n",
    "    #ds_pf_2d = ds_pf_2d.mean(dim='number')  # Ensemble mean (commented out)\n",
    "    \n",
    "    # Read ensemble forecast 2m temperature data\n",
    "    ds_pf_2t = xr.open_dataset(f'{pathto}\\\\{origen}_2t_{region}_pf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_pf_2t = ds_pf_2t - 273.15  # Convert to Celsius\n",
    "    #ds_pf_2t = ds_pf_2t.mean(dim='number')  # Ensemble mean (commented out)\n",
    "    \n",
    "    # Read ensemble forecast total precipitation data\n",
    "    ds_pf_tp = xr.open_dataset(f'{pathto}\\\\{origen}_tp_{region}_pf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_pf_tp = ds_pf_tp.diff(dim='step')  # Calculate precipitation difference\n",
    "    ds_pf_tp = ds_pf_tp.where(ds_pf_tp > 0, 0)  # Set negative values to 0\n",
    "    #ds_pf_tp = ds_pf_tp.mean(dim='number')  # Ensemble mean (commented out)\n",
    "    \n",
    "    # Read ensemble forecast surface solar radiation data\n",
    "    ds_pf_ssr = xr.open_dataset(f'{pathto}\\\\{origen}_ssr_{region}_pf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_pf_ssr = ds_pf_ssr / 86400  # Convert to W/m² (seconds to days)\n",
    "    #ds_pf_ssr = ds_pf_ssr.mean(dim='number')  # Ensemble mean (commented out)\n",
    "    \n",
    "    # Read ensemble forecast 10m u-component wind speed data\n",
    "    ds_pf_10u = xr.open_dataset(f'{pathto}\\\\{origen}_10u_{region}_pf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    # Read ensemble forecast 10m v-component wind speed data\n",
    "    ds_pf_10v = xr.open_dataset(f'{pathto}\\\\{origen}_10v_{region}_pf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    #ds_pf_10u = ds_pf_10u.mean(dim='number')  # Ensemble mean (commented out)\n",
    "    #ds_pf_10v = ds_pf_10v.mean(dim='number')  # Ensemble mean (commented out)\n",
    "    \n",
    "    # Calculate wind speed and VPD\n",
    "    ds_wind_speed, ds_vpd = calculate_wind_speed_and_vpd(ds_pf_10u, ds_pf_10v, ds_pf_2d, ds_pf_2t)\n",
    "    \n",
    "    return ds_pf_mx2t6, ds_pf_mn2t6, ds_pf_2t, ds_pf_tp, ds_wind_speed, ds_vpd, ds_pf_ssr\n",
    "\n",
    "\n",
    "def read_cf_data(pathto, origen, region, years):\n",
    "    # Read control forecast minimum temperature data\n",
    "    ds_cf_mn2t6 = xr.open_dataset(f'{pathto}\\\\{origen}_mn2t6_{region}_cf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_cf_mn2t6 = ds_cf_mn2t6 - 273.15  # Convert to Celsius\n",
    "   # ds_cf_mn2t6 = ds_cf_mn2t6.mean(dim='number')  # Control mean (commented out)\n",
    "    \n",
    "    # Read control forecast maximum temperature data\n",
    "    ds_cf_mx2t6 = xr.open_dataset(f'{pathto}\\\\{origen}_mx2t6_{region}_cf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_cf_mx2t6 = ds_cf_mx2t6 - 273.15  # Convert to Celsius\n",
    "   # ds_cf_mx2t6 = ds_cf_mx2t6.mean(dim='number')  # Control mean (commented out)\n",
    "    \n",
    "    # Read control forecast dewpoint temperature data\n",
    "    ds_cf_2d = xr.open_dataset(f'{pathto}\\\\{origen}_2d_{region}_cf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_cf_2d = ds_cf_2d - 273.15  # Convert to Celsius\n",
    "    #ds_cf_2d = ds_cf_2d.mean(dim='number')  # Control mean (commented out)\n",
    "    \n",
    "    # Read control forecast 2m temperature data\n",
    "    ds_cf_2t = xr.open_dataset(f'{pathto}\\\\{origen}_2t_{region}_cf_forecasttimefcst_{years}.grib', engine='cfgrib')\n",
    "    ds_cf_2t = ds_cf_2t - 273.15  # Convert to Celsius\n",
    "   # ds_cf_2t = ds_cf_2t.mean(dim='number')  # Control mean (commented out)\n",
    "    \n",
    "    # Read control forecast total precipitation data\n",
    "    ds_cf_tp = xr.open_dataset(f'{pathto}\\\\{origen}_tp_{region}_cf_forecasttimefcstt_{years}.grib', engine='cfgrib')\n",
    "    ds_cf_tp = ds_cf_tp.diff(dim='step')  # Calculate precipitation difference\n",
    "    ds_cf_tp = ds_cf_tp.where(ds_cf_tp > 0, 0)  # Set negative values to 0\n",
    "  #  ds_cf_tp = ds_cf_tp.mean(dim='number')  # Control mean (commented out)\n",
    "    \n",
    "    # Read control forecast surface solar radiation data\n",
    "    ds_cf_ssr = xr.open_dataset(f'{pathto}\\\\{origen}_ssr_{region}_cf_forecasttimefcstt_{years}.grib', engine='cfgrib')\n",
    "    ds_cf_ssr = ds_cf_ssr / 86400  # Convert to W/m² (seconds to days)\n",
    "  #  ds_cf_ssr = ds_cf_ssr.mean(dim='number')  # Control mean (commented out)\n",
    "    \n",
    "    # Read control forecast 10m u-component wind speed data\n",
    "    ds_cf_10u = xr.open_dataset(f'{pathto}\\\\{origen}_10u_{region}_cf_forecasttimefcstt_{years}.grib', engine='cfgrib')\n",
    "    # Read control forecast 10m v-component wind speed data\n",
    "    ds_cf_10v = xr.open_dataset(f'{pathto}\\\\{origen}_10v_{region}_cf_forecasttimefcstt_{years}.grib', engine='cfgrib')\n",
    "   # ds_cf_10u = ds_cf_10u.mean(dim='number')  # Control mean (commented out)\n",
    "  #  ds_cf_10v = ds_cf_10v.mean(dim='number')  # Control mean (commented out)\n",
    "    \n",
    "    # Calculate wind speed and VPD\n",
    "    ds_wind_speed, ds_vpd = calculate_wind_speed_and_vpd(ds_cf_10u, ds_cf_10v, ds_cf_2d, ds_cf_2t)\n",
    "    \n",
    "    return ds_cf_mx2t6, ds_cf_mn2t6, ds_cf_2t, ds_cf_tp, ds_wind_speed, ds_vpd, ds_cf_ssr\n",
    "\n",
    "def process_df_arrays(ds_pf_2t, ds_pf_tp, ds_pf_ssr, ds_wind_speed, ds_vpd):\n",
    "    # Extract variable values\n",
    "    t2m_values = ds_pf_2t['t2m'].values\n",
    "    tp_values = ds_pf_tp['tp'].values\n",
    "    ssr_values = ds_pf_ssr['ssr'].values\n",
    "    \n",
    "    # Calculate hourly difference of solar radiation (on 2D step dimension)\n",
    "    new_array = np.zeros_like(ssr_values)\n",
    "    new_array[ :, 0, :] = ssr_values[:, 0, :]\n",
    "    new_array[:, 1:, :] = np.diff(ssr_values, axis=1)\n",
    "    ssr_processed = new_array\n",
    "    \n",
    "    # Extract wind speed and VPD values\n",
    "    wind_speed_values = ds_wind_speed['wind_speed'].values\n",
    "    vpd_values = ds_vpd['vpd'].values\n",
    "    \n",
    "    return t2m_values, tp_values, ssr_processed, wind_speed_values, vpd_values\n",
    "\n",
    "\n",
    "\n",
    "def process_cf_arrays(ds_cf_2t, ds_cf_tp, ds_cf_ssr, ds_wind_speed, ds_vpd):\n",
    "    # Extract variable values\n",
    "    t2m_values = ds_cf_2t['t2m'].values\n",
    "    tp_values = ds_cf_tp['tp'].values\n",
    "    ssr_values = ds_cf_ssr['ssr'].values\n",
    "    \n",
    "    # Calculate hourly difference of solar radiation\n",
    "    new_array = np.zeros_like(ssr_values)\n",
    "    new_array[0, :, :] = ssr_values[ 0, :, :]\n",
    "    new_array[1:, :, :] = np.diff(ssr_values, axis=0)\n",
    "    ssr_processed = new_array\n",
    "    \n",
    "    # Extract wind speed and VPD values\n",
    "    wind_speed_values = ds_wind_speed['wind_speed'].values\n",
    "    vpd_values = ds_vpd['vpd'].values\n",
    "    \n",
    "    return t2m_values, tp_values, ssr_processed, wind_speed_values, vpd_values\n",
    "\n",
    "def extract_dates(inputpath_base,institution,region):\n",
    "    # Return week numbers, corresponding days of start/end dates, and selected VI\n",
    "    file_path = os.path.join(inputpath_base, '02_S2S', '01_dataori',institution,'CommonYear_Week.txt')\n",
    "    inpath_dates = os.path.join(inputpath_base, '01_data', '05_buildmodel', '02_extractdates', 'gs_three_periods.txt')\n",
    "    \n",
    "    # Read lines from the file and strip whitespace from both ends\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "    # Read start point and harvest point from another file (duplicate code in original, kept as-is)\n",
    "    file_path = os.path.join(inputpath_base, '02_S2S', '01_dataori',institution,'CommonYear_Week.txt')\n",
    "    inpath_dates = os.path.join(inputpath_base, '01_data', '05_buildmodel', '02_extractdates', 'gs_three_periods.txt')\n",
    "    \n",
    "    # Read lines from the file and strip whitespace from both ends\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "    # Read growth stage information\n",
    "    gs_infornamtion = pd.read_csv(inpath_dates, sep='\\t', header=None)\n",
    "    gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point','VI_select2','regions']\n",
    "    harvest_point = gs_infornamtion[gs_infornamtion['regions']==region]['harvest_point'].values[0]\n",
    "    start_point = gs_infornamtion[gs_infornamtion['regions']==region]['start_point'].values[0]\n",
    "    VI_select2 = gs_infornamtion[gs_infornamtion['regions']==region]['VI_select2'].values[0]\n",
    "    \n",
    "    # Convert start point and harvest point from strings to integers\n",
    "    harvest_point = int(harvest_point)  # 8-day aggregation is backward-looking; e.g., '01-01' refers to aggregated indicator from 01-01 to 01-08\n",
    "    start_point = int(start_point)\n",
    "    \n",
    "    # Get corresponding dates from lines based on indices of start/harvest points\n",
    "    if harvest_point ==46:\n",
    "        harvest_date_doy = '-'+'12-31'\n",
    "    else:\n",
    "        harvest_date_doy = '-' + lines[harvest_point]\n",
    "    start_date_doy = '-' + lines[start_point]\n",
    "    \n",
    "    # Return start date and harvest date\n",
    "    return start_point,harvest_point,start_date_doy, harvest_date_doy,VI_select2\n",
    "\n",
    "\n",
    "def process_climate_data(data_new, year, T_upper, T_lower, dynamic_features, soil_feature, loc_feature, Year_feature, union_feature):\n",
    "    # Select columns\n",
    "    Tmin_columns = [col for col in data_new.columns if '_Tmin' in col]\n",
    "    Tmin = data_new[Tmin_columns].values\n",
    "    Tmean_columns = [col for col in data_new.columns if '_Tmean' in col]\n",
    "    Tmean = data_new[Tmean_columns].values\n",
    "    Tmax_columns = [col for col in data_new.columns if '_Tmax' in col]\n",
    "    Tmax = data_new[Tmax_columns].values\n",
    "    Pre_columns = [col for col in data_new.columns if '_Pre' in col]\n",
    "    Pre = data_new[Pre_columns].values\n",
    "    \n",
    "    # Calculate date range\n",
    "    days = Pre.shape[1]\n",
    "    dates = pd.date_range(start=str(year) + '-01-01', periods=days, freq='D')\n",
    "    \n",
    "    # Add year information\n",
    "    data_new['year'] = year\n",
    "    \n",
    "    # Calculate extreme meteorological indicators\n",
    "    spei_df = spei(dates, Pre, Tmean)\n",
    "    CDD_df, HDD_df, GDD_df = extreme_temperature(dates, Tmax, Tmin, T_upper, T_lower)\n",
    "    \n",
    "    # Aggregate 8-day data\n",
    "    data_new1 = aggre_8days(dynamic_features, dates, data_new)\n",
    "    \n",
    "    # Merge all data\n",
    "    data_new1 = pd.concat([CDD_df, HDD_df, GDD_df, spei_df, data_new1, data_new[soil_feature + loc_feature + Year_feature + union_feature]], axis=1)\n",
    "    \n",
    "    return data_new1\n",
    "    \n",
    "def spei(dates,Pre, Tmean):   \n",
    "    # Calculate Standardized Precipitation Evapotranspiration Index (SPEI)\n",
    "    precipitation = pd.DataFrame(Pre.T, index=dates) \n",
    "    Tmean = pd.DataFrame(Tmean.T, index=dates)  \n",
    "    \n",
    "    # Transpose the DataFrame so that dates are rows and features are columns\n",
    "    precipitation = precipitation.T\n",
    "    Tmean = Tmean.T\n",
    "    \n",
    "    # Calculate Potential Evapotranspiration (PET)\n",
    "    PET = thornthwaite(Tmean)\n",
    "    \n",
    "    # Calculate the difference between precipitation and PET\n",
    "    D = precipitation - PET\n",
    "    \n",
    "    # Calculate cumulative values every 8 days\n",
    "    D_resampled = D.resample('8D', axis=1).sum()\n",
    "    #D_resampled = D.T.resample('8D').sum().T (alternative resampling method, commented out)\n",
    "    \n",
    "    # Calculate SPEI\n",
    "    def compute_spei(series, scale):\n",
    "        \"\"\"Calculate SPEI (Standardized Precipitation Evapotranspiration Index)\"\"\"\n",
    "        # Cumulative sum\n",
    "        cum_sum = series.cumsum()\n",
    "        # Calculate mean and standard deviation\n",
    "        mean = cum_sum.mean()\n",
    "        std = cum_sum.std()\n",
    "        # Standardization\n",
    "        spei = (cum_sum - mean) / std\n",
    "        return spei\n",
    "    \n",
    "    spei_values = D_resampled.apply(lambda x: compute_spei(x, scale=1), axis=1)\n",
    "    spei_df = pd.DataFrame(data = spei_values.values,columns= [f'Week{week}_SPEI' for week in range(1, 47)])\n",
    "    return spei_df\n",
    "\n",
    "def thornthwaite(T, lat=45):\n",
    "    \"\"\"Thornthwaite method for Potential Evapotranspiration (PET) calculation\"\"\"\n",
    "    I = (T / 5.0) ** 1.514\n",
    "    a = (6.75e-7) * I**3 - (7.71e-5) * I**2 + (1.79e-2) * I + 0.49239\n",
    "    PET = 16 * ((10 * T / I) ** a)\n",
    "    return PET\n",
    "    \n",
    "def extreme_temperature(dates,Tmax,Tmin,T_upper,T_lower):\n",
    "    # Calculate extreme temperature indicators (GDD, HDD, CDD)\n",
    "    # GDD: Growing Degree Days; HDD: Heating Degree Days; CDD: Cooling Degree Days\n",
    "    GDD = np.where(Tmax < T_lower, 0, (np.minimum(Tmax, T_upper) + np.maximum(Tmin, T_lower)) / 2 - T_lower)\n",
    "    HDD = np.maximum(Tmax, T_upper) - T_upper\n",
    "    CDD = np.minimum(Tmin, T_lower) - T_lower\n",
    "    \n",
    "    # Resample to 8-day periods and sum\n",
    "    GDD = pd.DataFrame(GDD.T, index=dates).T.resample('8D', axis=1).sum()\n",
    "    HDD = pd.DataFrame(HDD.T, index=dates).T.resample('8D', axis=1).sum()\n",
    "    CDD = pd.DataFrame(CDD.T, index=dates).T.resample('8D', axis=1).sum()\n",
    "    \n",
    "    # Convert to DataFrames with standard column names\n",
    "    CDD_df = pd.DataFrame(data = CDD.values,columns= [f'Week{week}_CDD' for week in range(1, 47)])\n",
    "    HDD_df = pd.DataFrame(data = HDD.values,columns= [f'Week{week}_HDD' for week in range(1, 47)])\n",
    "    GDD_df = pd.DataFrame(data = GDD.values,columns= [f'Week{week}_GDD' for week in range(1, 47)])\n",
    "    return CDD_df,HDD_df,GDD_df  \n",
    "\n",
    "\n",
    "def aggre_8days(dynamic_features,dates,data_new):\n",
    "    # Aggregate dynamic feature data by 8-day periods\n",
    "    data_new1 = pd.DataFrame()\n",
    "    for feature in dynamic_features:\n",
    "        # Select columns corresponding to the current dynamic feature\n",
    "        columns = [col for col in data_new.columns if feature in col];\n",
    "        data = data_new[columns];\n",
    "        \n",
    "        # Reindex to fill date gaps if the number of columns is less than the number of dates\n",
    "        if data.shape[1]<len(dates):\n",
    "            date_columns = pd.to_datetime(data.columns.str.replace(feature, ''), format='%Y_%m_%d')\n",
    "            data = data.T\n",
    "            data.index = date_columns\n",
    "            full_index = pd.date_range(start=dates[0], end=dates[-1])\n",
    "            data = data.reindex(full_index).sort_index()\n",
    "        else:\n",
    "            data = pd.DataFrame(data.T.values, index=dates)\n",
    "        \n",
    "        # Aggregate by 8 days: sum for precipitation, average for other features\n",
    "        if feature =='_Pre':\n",
    "            data = data.T.resample('8D', axis=1).sum()\n",
    "            data.columns = [f'Week{week}_{feature[1:]}' for week in range(1, 47)]\n",
    "        else:\n",
    "            data = data.T.resample('8D', axis=1).mean()\n",
    "            data.columns = [f'Week{week}_{feature[1:]}' for week in range(1, 47)]\n",
    "        \n",
    "        # Merge aggregated data to the result\n",
    "        data_new1 = pd.concat([data_new1, data], axis=1)\n",
    "    return data_new1\n",
    "\n",
    "\n",
    "def extract_selected_variables(inputpath_base):\n",
    "    # Read selected feature variables\n",
    "    inpath_dates = os.path.join(inputpath_base, '01_data','05_buildmodel', '04_selectFeatures','selectFeatures.txt')\n",
    "    # Construct file path\n",
    "    gs_infornamtion = pd.read_csv(inpath_dates, sep='\\t', header=None)\n",
    "    gs_infornamtion.columns = ['slected_dynamic_features', 'slected_static', 'regionID']\n",
    "    \n",
    "    # Parse string-formatted lists to actual lists\n",
    "    gs_infornamtion['slected_dynamic_features'] = gs_infornamtion['slected_dynamic_features'].apply(ast.literal_eval)\n",
    "    gs_infornamtion['slected_static'] = gs_infornamtion['slected_static'].apply(ast.literal_eval)\n",
    "    return gs_infornamtion\n",
    "\n",
    "# Direct reading may reduce significant data loading time (original comment)\n",
    "def find_weeks(inputpath_base,forecastDataList):\n",
    "    # Find corresponding week numbers based on forecast dates\n",
    "    \n",
    "    # Read week-date mapping file\n",
    "    file_path = os.path.join(inputpath_base, '02_S2S', '01_dataori', 'ECMWF','CommonYear_Week.txt')\n",
    "    with open(file_path, 'r') as file:\n",
    "        week_dates = [line.strip() for line in file.readlines()]\n",
    "    result = []\n",
    "    \n",
    "    # Iterate through each date in forecastDataList\n",
    "    for date in forecastDataList:\n",
    "        # Iterate through week_dates to find the week of the date\n",
    "        for i in range(len(week_dates) - 1):\n",
    "            # Check if the date is within the current date range (inclusive of lower bound, exclusive of upper bound)\n",
    "            if week_dates[i] <= date < week_dates[i + 1]:\n",
    "                result.append((date, i + 1))  # Week 1 corresponds to index 0, so week number is i + 1\n",
    "                break\n",
    "        # Handle dates beyond the last date range (i.e., week 46 range)\n",
    "        else:\n",
    "            if date >= week_dates[-1]:\n",
    "                result.append((date, len(week_dates)))  # Last week: week 46\n",
    "    result = {date: week for date, week in result}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6758cd69-d282-4094-befd-86062c6d55f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: F:\\SCI\\SCI9_1\\01_code\\02_Wheat\\06_India\n",
      "当前文件夹名字: 06_India\n",
      "上一级文件夹名字: 02_Wheat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta, date\n",
    "root_directory = os.getcwd()[0:3]\n",
    "sys.path.append(root_directory+'SCI\\\\SCI9_1\\\\01_code')\n",
    "sys.path.append(r'C:\\ProgramData\\anaconda3\\Lib\\site-packages') \n",
    "sys.path.append(r'C:\\Users\\DELL\\.conda\\envs\\myenv\\Lib\\site-packages') \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ast\n",
    "from fastdtw import fastdtw\n",
    " \n",
    "# Get current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)\n",
    " \n",
    "# Get the name of the current folder\n",
    "current_folder_name = os.path.basename(current_directory)\n",
    "print(\"Current folder name:\", current_folder_name)\n",
    " \n",
    "# Get the name of the parent folder\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "parent_folder_name = os.path.basename(parent_directory)\n",
    "print(\"Parent folder name:\", parent_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bc4ff92-9c24-483e-90d0-2ef82da1719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables that need to be modified\n",
    "crop = parent_folder_name; countryID = current_folder_name; variable = 'mx2t6';\n",
    "region = 'I';\n",
    "country = countryID.split('_')[1]\n",
    "############## Regional Configuration #############################################\n",
    "inpath_dates_other = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID+'\\\\'+'01_data'+'\\\\'+'07_Information'\n",
    "other_infornamtion = pd.read_csv(os.path.join(inpath_dates_other,'information.txt'), sep=' ', header=None)\n",
    "startyear, endyear, shp_name = other_infornamtion.iloc[0,0], other_infornamtion.iloc[0,1], other_infornamtion.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0368723a-8b53-4325-9be4-02138b440b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(inputpath_base, '02_S2S', '01_dataori',institution,'CommonYear_Week.txt')\n",
    "inpath_dates = os.path.join(inputpath_base, '01_data', '05_buildmodel', '02_extractdates', 'gs_three_periods.txt')\n",
    "# Read lines from the file and strip whitespace from both ends\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Read start point and harvest point from another file\n",
    "file_path = os.path.join(inputpath_base, '02_S2S', '01_dataori',institution,'CommonYear_Week.txt')\n",
    "inpath_dates = os.path.join(inputpath_base, '01_data', '05_buildmodel', '02_extractdates', 'gs_three_periods.txt')\n",
    "# Read lines from the file and strip whitespace from both ends\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Read start point and harvest point from another file\n",
    "gs_infornamtion = pd.read_csv(inpath_dates, sep='\\t', header=None)\n",
    "gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point','VI_select2','regions']\n",
    "harvest_point = gs_infornamtion[gs_infornamtion['regions']==region]['harvest_point'].values[0]\n",
    "start_point = gs_infornamtion[gs_infornamtion['regions']==region]['start_point'].values[0]\n",
    "VI_select2 = gs_infornamtion[gs_infornamtion['regions']==region]['VI_select2'].values[0]\n",
    "\n",
    "# Convert the string-type start point and harvest point to integers\n",
    "harvest_point = int(harvest_point) # 8-day aggregation is backward-looking; for example, '01-01' refers to the aggregated indicator from 01-01 to 01-08\n",
    "start_point = int(start_point)\n",
    "\n",
    "# Get the corresponding dates from the 'lines' list based on the indices of the start point and harvest point\n",
    "if harvest_point ==46:\n",
    "    harvest_date_doy = '-'+'12-31'\n",
    "else:\n",
    "    harvest_date_doy = '-' + lines[harvest_point]\n",
    "start_date_doy = '-' + lines[start_point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01bbf35b-3792-4f07-a6b4-27d49c03cb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harvest_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4cca4a9-3a44-41d3-9c78-0a558218b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['I']\n",
    "Forecastyears = {'I': endyear}\n",
    "# Define temperature thresholds according to crop type\n",
    "if crop == '02_Wheat':\n",
    "    T_upper = 34\n",
    "    T_lower = 0\n",
    "elif crop == '01_Maize':  # Fixed spelling error\n",
    "    T_upper = 30\n",
    "    T_lower = 8\n",
    "elif crop == '03_Rice':\n",
    "    T_upper = 35\n",
    "    T_lower = 8    \n",
    "else:\n",
    "    T_upper = 30\n",
    "    T_lower = 10\n",
    "    \n",
    "\n",
    "\n",
    "VIs =  ['_KNDVI' ,'_EVI','_NDVI']  # Vegetation Indices (KNDVI: Kernel NDVI, EVI: Enhanced Vegetation Index, NDVI: Normalized Difference Vegetation Index)\n",
    "Cilmate = ['_Pre' ,'_Tmin' ,'_Solar','_Tmean','_Tmax']  # Climate variables (Pre: Precipitation, Tmin: Minimum Temperature, Solar: Solar Radiation, Tmean: Mean Temperature, Tmax: Maximum Temperature)\n",
    "Climate_Exogenous  = ['_CDD' ,'_HDD' ,'_GDD','_VPD','_wind_speed','_SPEI'] # Exogenous climate variables (CDD: Cooling Degree Days, HDD: Heating Degree Days, GDD: Growing Degree Days, VPD: Vapor Pressure Deficit, SPEI: Standardized Precipitation Evapotranspiration Index)\n",
    "soil_feature = [ 'SAND','AWC', 'SILT','ORG_CARBON',  'TOTAL_N', 'PH_WATER',  'CEC_SOIL', 'CLAY']  # Soil characteristic variables (SAND: Sand content, AWC: Available Water Capacity, SILT: Silt content, ORG_CARBON: Organic Carbon, TOTAL_N: Total Nitrogen, PH_WATER: Soil pH (in water), CEC_SOIL: Cation Exchange Capacity, CLAY: Clay content)\n",
    "loc_feature = ['elevation', 'lat', 'lon']  # Location feature variables (elevation: Elevation, lat: Latitude, lon: Longitude)\n",
    "Year_feature = ['year']; union_feature = ['idJoin'];  # Year feature and union key variable\n",
    "dynamic_features = [ '_KNDVI' ,'_EVI','_NDVI','_Pre' ,'_Tmin' ,'_Solar','_Tmean','_VPD', '_wind_speed' ,'_Tmax']  # Dynamic feature variables (combination of VIs and core climate variables)\n",
    "inputpath_base = root_directory + '\\\\SCI\\\\SCI9_1\\\\02_data\\\\'+crop+'\\\\'+countryID+'\\\\'  # Base input path\n",
    "\n",
    "forecast_days = 46; country = countryID.split('_')[1]  # Forecast duration (46 weeks equivalent) and extract country code from countryID\n",
    "yield_type = 'actual_yield'; origen ='ECMWF'; institution = 'ECMWF';  # Yield type (actual yield), data source (ECMWF: European Centre for Medium-Range Weather Forecasts)\n",
    "ECMWF_path = os.path.join(inputpath_base,'02_S2S')  # S2S (Subseasonal to Seasonal) data path\n",
    "S2S_data_path = os.path.join(ECMWF_path,'02_Reforecast', origen)  # Reforecast data path under S2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fe55c71-7956-42dc-a2f1-2a739e2f5441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Indiagee_up_load_winter.shp'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22b6f085-c21d-4afc-95c7-b04f00ffee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_all = os.path.join(inputpath_base,'01_data','02_shp',shp_name)\n",
    "gdf_all = gpd.read_file(shp_all)\n",
    "\n",
    "\n",
    "'''\n",
    "#shp_name = 'Chinagee_up_load.shp'\n",
    "#shp_all = os.path.join(inputpath_base,'01_data','02_shp',shp_name)\n",
    "# Output sample variable TIF file for subsequent pointid reading preparation\n",
    "gdf_all = gpd.read_file(shp_all)\n",
    "if 'region' not in gdf_all.columns:\n",
    "    gdf_all['region'] ='I'\n",
    "else:\n",
    "    pass\n",
    "'''\n",
    "\n",
    "for region in regions:\n",
    "    if 'region' not in gdf_all.columns:\n",
    "        gdf_all['region'] ='I'\n",
    "        filtered_data_ori_upload = gdf_all\n",
    "    else:\n",
    "        filtered_data_ori_upload = gdf_all# [gdf_all['region']==region] # [ToDo] Verify if there are sub-regions\n",
    "    Forecastyear =  Forecastyears[region]; years = str(Forecastyear)+'_'+str(Forecastyear);\n",
    "    forecastDataList = os.listdir(os.path.join(inputpath_base,'02_S2S','02_Reforecast','ECMWF',region))\n",
    "    filename = institution+\"_\"+variable+\"_\"+country+\"_cf_forecasttimefcst_\"+years+\".grib\"\n",
    "    file = os.path.join(inputpath_base, '02_S2S','02_Reforecast','ECMWF',region,forecastDataList[0],filename)\n",
    "    ds = xr.open_dataset(file, engine='cfgrib')\n",
    "    d2m_data = ds.isel(step=0)['mx2t6']\n",
    "    d2m_data.rio.write_crs(\"epsg:4326\", inplace=True)  # Assume the data uses WGS84 coordinate system\n",
    "    os.makedirs(os.path.join(inputpath_base,'02_S2S','08_Tif',region), exist_ok=True)\n",
    "    output_path = os.path.join(inputpath_base,'02_S2S','08_Tif',region,region+'_'+variable+'_data.tif')\n",
    "    d2m_data.rio.to_raster(output_path)\n",
    "    # filtered_data_ori_upload = gdf_all[gdf_all['region']==region]\n",
    "    filtered_data_ori_upload.to_file(os.path.join(inputpath_base, '01_data','02_shp',country+'_'+region+'.shp'), driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f986d87-5c8f-435f-aea9-23e926069f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;mx2t6&#x27; (latitude: 18, longitude: 23)&gt;\n",
       "[414 values with dtype=float32]\n",
       "Coordinates:\n",
       "    number             int32 ...\n",
       "    time               datetime64[ns] ...\n",
       "    step               timedelta64[ns] 06:00:00\n",
       "    heightAboveGround  float64 ...\n",
       "  * latitude           (latitude) float64 36.4 34.9 33.4 31.9 ... 13.9 12.4 10.9\n",
       "  * longitude          (longitude) float64 66.1 67.6 69.1 ... 96.1 97.6 99.1\n",
       "    valid_time         datetime64[ns] ...\n",
       "    spatial_ref        int32 0\n",
       "Attributes: (12/30)\n",
       "    GRIB_paramId:                             121\n",
       "    GRIB_dataType:                            cf\n",
       "    GRIB_numberOfPoints:                      414\n",
       "    GRIB_typeOfLevel:                         heightAboveGround\n",
       "    GRIB_stepUnits:                           1\n",
       "    GRIB_stepType:                            max\n",
       "    ...                                       ...\n",
       "    GRIB_shortName:                           mx2t6\n",
       "    GRIB_totalNumber:                         11\n",
       "    GRIB_units:                               K\n",
       "    long_name:                                Maximum temperature at 2 metres...\n",
       "    units:                                    K\n",
       "    standard_name:                            air_temperature</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'mx2t6'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>latitude</span>: 18</li><li><span class='xr-has-index'>longitude</span>: 23</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-750d63ba-25b2-4ded-a380-d4776bff9226' class='xr-array-in' type='checkbox' checked><label for='section-750d63ba-25b2-4ded-a380-d4776bff9226' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>...</span></div><div class='xr-array-data'><pre>[414 values with dtype=float32]</pre></div></div></li><li class='xr-section-item'><input id='section-74769b6e-adf5-4663-80c2-8c0f3ad7117d' class='xr-section-summary-in' type='checkbox'  checked><label for='section-74769b6e-adf5-4663-80c2-8c0f3ad7117d' class='xr-section-summary' >Coordinates: <span>(8)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>number</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-2f054d89-5268-412e-9646-d28a680d430a' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-2f054d89-5268-412e-9646-d28a680d430a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-f0dece3c-537f-42ad-bd2f-7c6f9e8d8845' class='xr-var-data-in' type='checkbox'><label for='data-f0dece3c-537f-42ad-bd2f-7c6f9e8d8845' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>ensemble member numerical id</dd><dt><span>units :</span></dt><dd>1</dd><dt><span>standard_name :</span></dt><dd>realization</dd></dl></div><div class='xr-var-data'><pre>[1 values with dtype=int32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>time</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-b65023e8-2ee2-4b14-835f-e1906058fcac' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b65023e8-2ee2-4b14-835f-e1906058fcac' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e37ef396-9e6c-4bc7-b99a-492a1ad2f7ff' class='xr-var-data-in' type='checkbox'><label for='data-e37ef396-9e6c-4bc7-b99a-492a1ad2f7ff' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>initial time of forecast</dd><dt><span>standard_name :</span></dt><dd>forecast_reference_time</dd></dl></div><div class='xr-var-data'><pre>[1 values with dtype=datetime64[ns]]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>step</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>timedelta64[ns]</div><div class='xr-var-preview xr-preview'>06:00:00</div><input id='attrs-001a7efa-29d4-4e32-b3cd-7435b380b244' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-001a7efa-29d4-4e32-b3cd-7435b380b244' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6cef04fc-9179-41bc-9606-465f02513db1' class='xr-var-data-in' type='checkbox'><label for='data-6cef04fc-9179-41bc-9606-465f02513db1' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>time since forecast_reference_time</dd><dt><span>standard_name :</span></dt><dd>forecast_period</dd></dl></div><div class='xr-var-data'><pre>array(21600000000000, dtype=&#x27;timedelta64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>heightAboveGround</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-b9c63693-f28c-4119-93ca-a59f1511852e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b9c63693-f28c-4119-93ca-a59f1511852e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4533994b-cd21-42fc-9f2e-1a2711b03be7' class='xr-var-data-in' type='checkbox'><label for='data-4533994b-cd21-42fc-9f2e-1a2711b03be7' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>height above the surface</dd><dt><span>units :</span></dt><dd>m</dd><dt><span>positive :</span></dt><dd>up</dd><dt><span>standard_name :</span></dt><dd>height</dd></dl></div><div class='xr-var-data'><pre>[1 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>latitude</span></div><div class='xr-var-dims'>(latitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>36.4 34.9 33.4 ... 13.9 12.4 10.9</div><input id='attrs-198856f7-8865-4e5e-ad82-08c10d827737' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-198856f7-8865-4e5e-ad82-08c10d827737' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3c01eaf0-ba41-4fc8-822d-5ac58313a2b4' class='xr-var-data-in' type='checkbox'><label for='data-3c01eaf0-ba41-4fc8-822d-5ac58313a2b4' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>stored_direction :</span></dt><dd>decreasing</dd></dl></div><div class='xr-var-data'><pre>array([36.4, 34.9, 33.4, 31.9, 30.4, 28.9, 27.4, 25.9, 24.4, 22.9, 21.4, 19.9,\n",
       "       18.4, 16.9, 15.4, 13.9, 12.4, 10.9])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>longitude</span></div><div class='xr-var-dims'>(longitude)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>66.1 67.6 69.1 ... 96.1 97.6 99.1</div><input id='attrs-1032db25-9998-4cb8-aed1-461ce8b5286d' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-1032db25-9998-4cb8-aed1-461ce8b5286d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-14a30586-8a76-4278-90a7-8ce578ac72ca' class='xr-var-data-in' type='checkbox'><label for='data-14a30586-8a76-4278-90a7-8ce578ac72ca' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>long_name :</span></dt><dd>longitude</dd></dl></div><div class='xr-var-data'><pre>array([66.1, 67.6, 69.1, 70.6, 72.1, 73.6, 75.1, 76.6, 78.1, 79.6, 81.1, 82.6,\n",
       "       84.1, 85.6, 87.1, 88.6, 90.1, 91.6, 93.1, 94.6, 96.1, 97.6, 99.1])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>valid_time</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-58baac7b-f1b5-4406-8829-17f2debcca15' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-58baac7b-f1b5-4406-8829-17f2debcca15' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-724d040e-5dc9-4e99-8428-bd936f38d453' class='xr-var-data-in' type='checkbox'><label for='data-724d040e-5dc9-4e99-8428-bd936f38d453' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>long_name :</span></dt><dd>time</dd></dl></div><div class='xr-var-data'><pre>[1 values with dtype=datetime64[ns]]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int32</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-e4cb0340-bd57-4d67-9abe-b1a83d533c96' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-e4cb0340-bd57-4d67-9abe-b1a83d533c96' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b3dcc5c3-273d-4e45-af73-d74c9c80e6b5' class='xr-var-data-in' type='checkbox'><label for='data-b3dcc5c3-273d-4e45-af73-d74c9c80e6b5' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>crs_wkt :</span></dt><dd>GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AXIS[&quot;Latitude&quot;,NORTH],AXIS[&quot;Longitude&quot;,EAST],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]]</dd><dt><span>semi_major_axis :</span></dt><dd>6378137.0</dd><dt><span>semi_minor_axis :</span></dt><dd>6356752.314245179</dd><dt><span>inverse_flattening :</span></dt><dd>298.257223563</dd><dt><span>reference_ellipsoid_name :</span></dt><dd>WGS 84</dd><dt><span>longitude_of_prime_meridian :</span></dt><dd>0.0</dd><dt><span>prime_meridian_name :</span></dt><dd>Greenwich</dd><dt><span>geographic_crs_name :</span></dt><dd>WGS 84</dd><dt><span>horizontal_datum_name :</span></dt><dd>World Geodetic System 1984</dd><dt><span>grid_mapping_name :</span></dt><dd>latitude_longitude</dd><dt><span>spatial_ref :</span></dt><dd>GEOGCS[&quot;WGS 84&quot;,DATUM[&quot;WGS_1984&quot;,SPHEROID[&quot;WGS 84&quot;,6378137,298.257223563,AUTHORITY[&quot;EPSG&quot;,&quot;7030&quot;]],AUTHORITY[&quot;EPSG&quot;,&quot;6326&quot;]],PRIMEM[&quot;Greenwich&quot;,0,AUTHORITY[&quot;EPSG&quot;,&quot;8901&quot;]],UNIT[&quot;degree&quot;,0.0174532925199433,AUTHORITY[&quot;EPSG&quot;,&quot;9122&quot;]],AXIS[&quot;Latitude&quot;,NORTH],AXIS[&quot;Longitude&quot;,EAST],AUTHORITY[&quot;EPSG&quot;,&quot;4326&quot;]]</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-131ca5f5-04af-4832-90a5-fd8da084d991' class='xr-section-summary-in' type='checkbox'  ><label for='section-131ca5f5-04af-4832-90a5-fd8da084d991' class='xr-section-summary' >Indexes: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>latitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-4fdb9d04-222d-4856-b240-c57f63affc37' class='xr-index-data-in' type='checkbox'/><label for='index-4fdb9d04-222d-4856-b240-c57f63affc37' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([              36.4,               34.9,               33.4,\n",
       "                     31.9,               30.4,               28.9,\n",
       "                     27.4,               25.9,               24.4,\n",
       "                     22.9,               21.4,               19.9,\n",
       "                     18.4,               16.9, 15.399999999999999,\n",
       "       13.899999999999999, 12.399999999999999,               10.9],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;latitude&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>longitude</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-74316121-07d6-419f-ad86-072f4ecfd7c2' class='xr-index-data-in' type='checkbox'/><label for='index-74316121-07d6-419f-ad86-072f4ecfd7c2' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([66.1, 67.6, 69.1, 70.6, 72.1, 73.6, 75.1, 76.6, 78.1, 79.6, 81.1, 82.6,\n",
       "       84.1, 85.6, 87.1, 88.6, 90.1, 91.6, 93.1, 94.6, 96.1, 97.6, 99.1],\n",
       "      dtype=&#x27;float64&#x27;, name=&#x27;longitude&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-7f1a8111-85c3-4d47-be2f-4a1d72d95e42' class='xr-section-summary-in' type='checkbox'  ><label for='section-7f1a8111-85c3-4d47-be2f-4a1d72d95e42' class='xr-section-summary' >Attributes: <span>(30)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>GRIB_paramId :</span></dt><dd>121</dd><dt><span>GRIB_dataType :</span></dt><dd>cf</dd><dt><span>GRIB_numberOfPoints :</span></dt><dd>414</dd><dt><span>GRIB_typeOfLevel :</span></dt><dd>heightAboveGround</dd><dt><span>GRIB_stepUnits :</span></dt><dd>1</dd><dt><span>GRIB_stepType :</span></dt><dd>max</dd><dt><span>GRIB_gridType :</span></dt><dd>regular_ll</dd><dt><span>GRIB_NV :</span></dt><dd>0</dd><dt><span>GRIB_Nx :</span></dt><dd>23</dd><dt><span>GRIB_Ny :</span></dt><dd>18</dd><dt><span>GRIB_cfName :</span></dt><dd>air_temperature</dd><dt><span>GRIB_cfVarName :</span></dt><dd>mx2t6</dd><dt><span>GRIB_gridDefinitionDescription :</span></dt><dd>Latitude/longitude</dd><dt><span>GRIB_iDirectionIncrementInDegrees :</span></dt><dd>1.5</dd><dt><span>GRIB_iScansNegatively :</span></dt><dd>0</dd><dt><span>GRIB_jDirectionIncrementInDegrees :</span></dt><dd>1.5</dd><dt><span>GRIB_jPointsAreConsecutive :</span></dt><dd>0</dd><dt><span>GRIB_jScansPositively :</span></dt><dd>0</dd><dt><span>GRIB_latitudeOfFirstGridPointInDegrees :</span></dt><dd>36.4</dd><dt><span>GRIB_latitudeOfLastGridPointInDegrees :</span></dt><dd>10.9</dd><dt><span>GRIB_longitudeOfFirstGridPointInDegrees :</span></dt><dd>66.1</dd><dt><span>GRIB_longitudeOfLastGridPointInDegrees :</span></dt><dd>99.1</dd><dt><span>GRIB_missingValue :</span></dt><dd>3.4028234663852886e+38</dd><dt><span>GRIB_name :</span></dt><dd>Maximum temperature at 2 metres in the last 6 hours</dd><dt><span>GRIB_shortName :</span></dt><dd>mx2t6</dd><dt><span>GRIB_totalNumber :</span></dt><dd>11</dd><dt><span>GRIB_units :</span></dt><dd>K</dd><dt><span>long_name :</span></dt><dd>Maximum temperature at 2 metres in the last 6 hours</dd><dt><span>units :</span></dt><dd>K</dd><dt><span>standard_name :</span></dt><dd>air_temperature</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'mx2t6' (latitude: 18, longitude: 23)>\n",
       "[414 values with dtype=float32]\n",
       "Coordinates:\n",
       "    number             int32 ...\n",
       "    time               datetime64[ns] ...\n",
       "    step               timedelta64[ns] 06:00:00\n",
       "    heightAboveGround  float64 ...\n",
       "  * latitude           (latitude) float64 36.4 34.9 33.4 31.9 ... 13.9 12.4 10.9\n",
       "  * longitude          (longitude) float64 66.1 67.6 69.1 ... 96.1 97.6 99.1\n",
       "    valid_time         datetime64[ns] ...\n",
       "    spatial_ref        int32 0\n",
       "Attributes: (12/30)\n",
       "    GRIB_paramId:                             121\n",
       "    GRIB_dataType:                            cf\n",
       "    GRIB_numberOfPoints:                      414\n",
       "    GRIB_typeOfLevel:                         heightAboveGround\n",
       "    GRIB_stepUnits:                           1\n",
       "    GRIB_stepType:                            max\n",
       "    ...                                       ...\n",
       "    GRIB_shortName:                           mx2t6\n",
       "    GRIB_totalNumber:                         11\n",
       "    GRIB_units:                               K\n",
       "    long_name:                                Maximum temperature at 2 metres...\n",
       "    units:                                    K\n",
       "    standard_name:                            air_temperature"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [TODO] 确认是否已经准备好所有区的dataJoin#########确认好后继续往下处理\n",
    "d2m_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "546f19e0-ff2c-4a14-bae8-6fcafb337a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO########################## Prepare dataJoin_I before running this code ##############################################\n",
    "# Output to 03_outputData\n",
    "\n",
    "# Forecast data processing\n",
    "for region in regions:\n",
    "    Forecastyear = Forecastyears[region]; years = str(Forecastyear)+'_'+str(Forecastyear);\n",
    "    _, _, _, harvest_date_doy, _ = extract_dates(inputpath_base,institution,region)\n",
    "    dataJoin1 =  pd.read_csv(os.path.join(inputpath_base, '02_S2S', '01_dataori',institution, f'dataJoin_{region}.csv'))\n",
    "    forecastDataList = os.listdir(os.path.join(inputpath_base,'02_S2S','02_Reforecast','ECMWF',region))\n",
    "    # Method 2: Suitable for large countries (20241104) - One grid point per administrative region\n",
    "    # A total of 11 ensemble models, output by individual model\n",
    "    for ii in forecastDataList:# Note: Modify here to batch processing algorithm #[0:1]\n",
    "        ########################### Read harvest date ###################################\n",
    "        pathto = os.path.join(ECMWF_path,'02_Reforecast', origen, region,ii)\n",
    "        outpath = os.path.join(ECMWF_path,'03_outputData', origen, region,ii)\n",
    "        create_directory(outpath)\n",
    "        ds_pf_mx2t6, ds_pf_mn2t6, ds_pf_2t, ds_pf_tp, ds_df_wind_speed, ds_pf_vpd, ds_pf_ssr = read_pf_data(pathto, origen, country, years)\n",
    "        ds_cf_mx2t6, ds_cf_mn2t6, ds_cf_2t, ds_cf_tp, ds_cf_wind_speed, ds_cf_vpd, ds_cf_ssr = read_cf_data(pathto, origen, country, years)\n",
    "        \n",
    "        ds_cf_mn2t6, ds_cf_mx2t6 = process_temperature_data(ds_cf_mn2t6, ds_cf_mx2t6)\n",
    "        ds_pf_mn2t6, ds_pf_mx2t6 = process_temperature_data(ds_pf_mn2t6, ds_pf_mx2t6)\n",
    "        ds_pf_mx2t6 = ds_pf_mx2t6[0]; ds_pf_mn2t6 = ds_pf_mn2t6[0];\n",
    "        ds_cf_mx2t6 = ds_cf_mx2t6[0]; ds_cf_mn2t6 = ds_cf_mn2t6[0];\n",
    "        ds_pf_mn2t6 = np.transpose(ds_pf_mn2t6, (1, 0, 2, 3))\n",
    "        ds_pf_mx2t6 = np.transpose(ds_pf_mx2t6, (1, 0, 2, 3))\n",
    "        ds_pf_2t, ds_pf_tp, ds_pf_ssr, ds_df_wind_speed, ds_pf_vpd = process_df_arrays(ds_pf_2t, ds_pf_tp, ds_pf_ssr, ds_df_wind_speed, ds_pf_vpd)\n",
    "        ds_cf_2t, ds_cf_tp, ds_cf_ssr, ds_cf_wind_speed, ds_cf_vpd = process_cf_arrays(ds_cf_2t, ds_cf_tp, ds_cf_ssr, ds_cf_wind_speed, ds_cf_vpd)\n",
    "        \n",
    "        # Output of 10 PF (Probabilistic Forecast) models\n",
    "        number = 10\n",
    "        for ID in range(0, number):\n",
    "            year = Forecastyear\n",
    "            date_harvest = str(year) + harvest_date_doy\n",
    "            date_harvest = pd.Timestamp(date_harvest).strftime('%Y%m%d')\n",
    "            time_value = str(year) + '-' + ii  # Date of the first forecast day\n",
    "            time_value1 = pd.Timestamp(time_value).strftime('%Y%m%d') # Date of the first forecast day\n",
    "            days_between = calculate_days_between(time_value1, date_harvest)# Only extract data used for modeling\n",
    "            steps_total = min(days_between + 1, forecast_days)\n",
    "            features = ['Tmax', 'Tmin', 'Tmean', 'Pre', 'wind_speed', 'VPD', 'Solar']\n",
    "            data_every_number = dataJoin1[['idJoin','pointid']]\n",
    "            for feature, selected in zip(features, [ds_pf_mn2t6, ds_pf_mx2t6, ds_pf_2t, ds_pf_tp, ds_df_wind_speed, ds_pf_vpd, ds_pf_ssr]):\n",
    "                for step in range(0, steps_total):  # Only extract data used for modeling; one extra day is extracted but has no impact (will not be used)\n",
    "                    time = (pd.Timestamp(time_value1) + pd.Timedelta(days=step)).strftime('%Y%m%d') # Current forecast date (1st day, 2nd day, and so on)\n",
    "                    data_sel = selected[ID, step, :].flatten()\n",
    "                    data_every_number[time + '_' + feature] = data_every_number['pointid'].apply(lambda x: data_sel[x-1])\n",
    "                    # Link data according to the position indicated by pointId in dataJoin, generate a new set of feature columns, and add them to the original pointId to form data_every_number\n",
    "            data_every_number.to_csv(os.path.join(outpath, 'number' + str(ID) + '.csv'), index=False)\n",
    "            \n",
    "        # Output of the CF (Control Forecast) model (11th model)\n",
    "        number = 10\n",
    "        data_every_number = dataJoin1[['idJoin','pointid']]\n",
    "        for feature, selected in zip(features, [ds_cf_mn2t6, ds_cf_mx2t6, ds_cf_2t, ds_cf_tp, ds_cf_wind_speed, ds_cf_vpd, ds_cf_ssr]):\n",
    "            for step in range(0, steps_total):  # Only extract data used for modeling; one extra day is extracted but has no impact (will not be used)\n",
    "                time = (pd.Timestamp(time_value1) + pd.Timedelta(days=step)).strftime('%Y%m%d') # Current forecast date (1st day, 2nd day, and so on)\n",
    "                data_sel = selected[step, :].flatten()\n",
    "                data_every_number[time + '_' + feature] = data_every_number['pointid'].apply(lambda x: data_sel[x-1])\n",
    "        data_every_number.to_csv(os.path.join(outpath, 'number' + str(number) + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a82ce4-4702-47f7-9924-8c0553bd803f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac281c7e-496d-4918-8044-7d58d2d6a229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fa432d6-21b3-4b14-8844-0f36b2e01ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Verify if the format of pre_name is correct\n",
    "# For the forecast year, output the ensemble of forecast data after the forecast date and the weekly-scale ensemble of historical 30 years, aggregated into 0-46 weeks\n",
    "# Read the original forecast year and the advance forecast date\n",
    "\n",
    "# Using direct reading may significantly reduce data loading time\n",
    "\n",
    "#[20250317] The definition of simple_week is missing\n",
    "\n",
    "numbers=11\n",
    "for region in regions:\n",
    "    pre_name ='WinterWheat'+'_'+countryID[3:]+'I_' # Need to verify if it conforms to this identifier format\n",
    "    '''\n",
    "    if region == 'I':\n",
    "        pre_name = 'EarlyRice_daily'+country+region+'_'\n",
    "    elif region == 'II':\n",
    "        pre_name = 'LateRice_daily'+country+region+'_'\n",
    "    else:\n",
    "        pre_name = 'SingleRice_daily'+country+region+'_'\n",
    "    '''\n",
    "    Forecastyear = Forecastyears[region]\n",
    "    years = range(startyear,Forecastyear) # Can be used to find similar KNDVI in historical years\n",
    "    data_ori = pd.read_csv(os.path.join(inputpath_base, '01_data', '04_GEEdownloadData','01_DailyData',pre_name+str(Forecastyear))+'.csv')\n",
    "    data_ori.set_index('idJoin', inplace=True)\n",
    "    forecastDataList = os.listdir(os.path.join(inputpath_base,'02_S2S','02_Reforecast','ECMWF',region))\n",
    "    \n",
    "    # Read selected variables for subsequent variable filtering\n",
    "    Forecastyear = Forecastyears[region]\n",
    "    SelFeature_infornamtion = extract_selected_variables(inputpath_base)\n",
    "    TimeFeatures_sel, Static_sel, regionID = SelFeature_infornamtion[SelFeature_infornamtion['regionID'] == region].iloc[0]\n",
    "    \n",
    "    # Actual number of weeks used for modeling\n",
    "    inpath_dates = os.path.join(inputpath_base, '01_data','05_buildmodel', '02_extractdates','gs_three_periods.txt')\n",
    "    gs_infornamtion = pd.read_csv(inpath_dates, delim_whitespace=True, header=None)\n",
    "    gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point', 'VI_select2','regionID']\n",
    "    start_point, peak, harvest_point, VI_select2, region = gs_infornamtion[gs_infornamtion['regionID'] == region].iloc[0]\n",
    "    \n",
    "    # Data reading and index filtering\n",
    "    data_ori_all = pd.read_csv(os.path.join(inputpath_base, '01_data','05_buildmodel','01_weekdata',region+'_allweekYielddata_VIs.csv'))\n",
    "    Static_sel= [col for col in Static_sel if 'year.1' not in col] \n",
    "    TimeFeatures_sel_all= [col for col in data_ori_all.columns if any(feature in col for feature in TimeFeatures_sel)]\n",
    "    TimeFeatures_sel_all= [col for col in TimeFeatures_sel_all if 'Previous_Yield' not in col] # Note: Yield from the previous year may be filtered due to 'pre' (precipitation) related columns; carefully verify\n",
    "    filtered_columns_all = TimeFeatures_sel_all+Static_sel\n",
    "    data_ori_all = data_ori_all[filtered_columns_all+['idJoin','Yield']] # Filter selected variables for subsequent analysis\n",
    "\n",
    "\n",
    "        # Filter VIs for subsequent identification\n",
    "    filtered_columns_VI = [col for col in data_ori_all.columns if VI_select2 in col]\n",
    "    data_S2S_VI = data_ori_all[filtered_columns_VI + ['year','idJoin']]\n",
    "    data_S2S_VI_mean = data_S2S_VI[filtered_columns_VI + ['year']].groupby('year').mean()\n",
    "    \n",
    "    S2SWeekList = ['leadweek_'+str(week) for week in range(1,6+1)] # 1 year in advance (i.e., 46 weeks)\n",
    "    data_ori_current = data_ori_all[data_ori_all['year']==Forecastyear]\n",
    "\n",
    "\n",
    "\n",
    "    dataweeks = find_weeks(inputpath_base,forecastDataList)\n",
    "    \n",
    "    unique_values = {}\n",
    "    for key, value in dataweeks.items():\n",
    "        if value not in unique_values.values():\n",
    "            unique_values[key] = value\n",
    "    \n",
    "    simple_week = {key: f\"leadweek_{len(dataweeks) - idx}\" for idx, key in enumerate(unique_values.keys())}    \n",
    "    for ii, leadweek in simple_week.items():# Note: Modify here to batch processing algorithm #[0:1]\n",
    "        # Loop through model numbers\n",
    "        for number in range(0,numbers):\n",
    "            data_S2S_new = data_ori.copy()\n",
    "            inputpath = os.path.join(ECMWF_path,'03_outputData', institution, region,ii)\n",
    "            data_S2S = pd.read_csv(os.path.join(inputpath,'number'+ str(number) + '.csv'))\n",
    "            data_S2S.set_index('idJoin', inplace=True)\n",
    "            common_columns = data_S2S.columns.intersection(data_S2S_new.columns)\n",
    "            data_S2S_new[common_columns] = data_S2S[common_columns] \n",
    "           # data_S2S_new.to_csv(os.path.join(outpath, str(year) + '.csv'))\n",
    "            data_S2S_new = process_climate_data(data_S2S_new.reset_index(), Forecastyear, T_upper, T_lower, dynamic_features, soil_feature, loc_feature, Year_feature, union_feature)\n",
    "            #data_S2S_new.to_csv(os.path.join(outpath, 'number'+str(number) + '.csv'),index=False)\n",
    "\n",
    "            data_S2S_new_update = data_ori_current.copy()\n",
    "            #data_his_new = data_his_new.merge(data_ori_current[filtered_columns_VI+Static_sel+['idJoin','Yield']],on='idJoin',how='inner')# Update VIs, static variables and Yield to ensure consistent data types\n",
    "            # Perform variable category filtering to ensure only selected variables are included\n",
    "            data_S2S_new['year'] = Forecastyear \n",
    "            # data_his_new = data_his_new[filtered_columns_all+['idJoin']]      \n",
    "\n",
    "            ############################################## Find the most similar vegetation index for filling ############################################################################\n",
    "            week_forecast = harvest_point+1-int(leadweek[9:])\n",
    "            # From the previous year to the next year to ensure a full year of actual data (cannot only use the current year as actual data may be unavailable); from the current year to the current date\n",
    "            forecast_weeklist1 = range(week_forecast, harvest_point + 1)\n",
    "            forecast_weeklist = [f'Week{week}{VI_select2}' for week in forecast_weeklist1]# No forecast data for week_forecast (from the current forecast week to the harvest week)\n",
    "            V1= [f'Week{week}{VI_select2}' for week in range(1, week_forecast)];\n",
    "            V2= [f'Week{week}{VI_select2}' for week in range(week_forecast, 46+1)];\n",
    "    \n",
    "            current_S2S_VI_before =pd.concat([data_S2S_VI_mean.loc[Forecastyear][V1], data_S2S_VI_mean.loc[Forecastyear-1][V2]])\n",
    "            dtw_distances = {}\n",
    "            for year1 in range(startyear+1,Forecastyear):# Will not include Forecastyear\n",
    "                other_S2S_VI_before = pd.concat([data_S2S_VI_mean.loc[year1][V1], data_S2S_VI_mean.loc[year1-1][V2]])\n",
    "                distance, path = fastdtw(current_S2S_VI_before, other_S2S_VI_before)# Forecast from current week to harvest week\n",
    "                dtw_distances[year1] = distance\n",
    "            most_similar_by_dtw = min(dtw_distances, key=dtw_distances.get) # Find the most similar year (e.g., 2016 in the example)\n",
    "            model_VI_replace = forecast_weeklist\n",
    "                \n",
    "        \n",
    "                \n",
    "            data_S2S_VI_forecast2 = data_S2S_VI[data_S2S_VI['year'] == most_similar_by_dtw][model_VI_replace+['idJoin']]# Only retain data needed for modeling\n",
    "            data_S2S_new_update = data_S2S_new_update.drop(model_VI_replace,axis=1) # Delete columns corresponding to the forecast period; retain non-forecast columns\n",
    "            data_S2S_new_update = data_S2S_new_update.merge(data_S2S_VI_forecast2,on='idJoin',how='inner')\n",
    "\n",
    "            ############################################## Replace the forecast-period weeks in the original data with S2S forecast data ###################################################################\n",
    "            # Loop to find variables that need to be replaced (replace original data with historical/S2S data)\n",
    "            update_climate = []\n",
    "            for feature in [feature for feature in TimeFeatures_sel if feature != VI_select2[1:]]: # Selected meteorological data excluding vegetation indices\n",
    "                update_climate += [f'Week{week}_{feature}' for week in forecast_weeklist1]\n",
    "            data_S2S_new_update.set_index('idJoin', inplace=True)\n",
    "            data_S2S_new.set_index('idJoin', inplace=True)\n",
    "            data_S2S_new_update[update_climate] = data_S2S_new[update_climate] # Replace original data with S2S forecast data\n",
    "\n",
    "            ############################################## Filter variables for the growing season ############################################################################\n",
    "            weeks = []\n",
    "            # Determine if the growing season spans across years\n",
    "            if start_point < harvest_point:  # Does not span years\n",
    "                for feature in TimeFeatures_sel:\n",
    "                    # Generate combinations of weeks and features using list comprehension\n",
    "                    weeks += [f'Week{week}_{feature}' for week in range(start_point, harvest_point + 1)]\n",
    "            else:  # Spans across years\n",
    "                for feature in TimeFeatures_sel:\n",
    "                    # Merge two date ranges and generate combinations of weeks and features\n",
    "                    weeks += [f'Week{week}_{feature}' for week in list(range(start_point, 47)) + list(range(1, harvest_point + 1))]\n",
    "            gs_features = weeks + Static_sel+['Yield']+['idJoin']\n",
    "            data_S2S_new_update.reset_index('idJoin', inplace=True)            \n",
    "            data_S2S_new_update = data_S2S_new_update[gs_features]\n",
    "            \n",
    "            ############################################## Output ############################################################################\n",
    "            S2S_outputpath = os.path.join(inputpath_base,'02_S2S','05_WeekData','01_S2S','VI_Like',region,leadweek)\n",
    "            os.makedirs(S2S_outputpath,exist_ok=True)\n",
    "            data_S2S_new_update.to_csv(os.path.join(S2S_outputpath,'number'+ str(number) + '.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd0ae0-6dde-467a-ae02-d221e2c1377c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5907670c-77fc-4964-a811-f489b74bc8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Calculate similarity based on columns: ['number','Leadweek','hist_year','climate','distance'] #######################\n",
    "############################ Supplement subsequent missing forecast data (excluding KNDVI) based on similar years #######################################\n",
    "######################### Similar years are determined using all meteorological factors ##############################################\n",
    "gs_infornamtion = pd.read_csv(inpath_dates, delim_whitespace=True, header=None)\n",
    "gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point', 'VI_select2','regionID']\n",
    "start_point, peak, harvest_point, VI_select2, region = gs_infornamtion[gs_infornamtion['regionID'] == region].iloc[0]\n",
    "if VI_select2[1:] in TimeFeatures_sel:\n",
    "    TimeFeatures_sel.remove(VI_select2[1:])\n",
    "hist_start_year = Forecastyear-30; hist_end_year = Forecastyear-1;\n",
    "data = []\n",
    "for year_hist in range(hist_start_year, hist_end_year+1):\n",
    "    hist_outputpath1 = os.path.join(inputpath_base,'02_S2S','05_WeekData','02_hist',region)\n",
    "    data_his_new_update = pd.read_csv(os.path.join(hist_outputpath1,'hist_'+str(year_hist)+'.csv'))\n",
    "    dataweeks = find_weeks(inputpath_base,forecastDataList)\n",
    "    \n",
    "    unique_values = {}\n",
    "    for key, value in dataweeks.items():\n",
    "        if value not in unique_values.values():\n",
    "            unique_values[key] = value\n",
    "    \n",
    "    simple_week = {key: f\"leadweek_{len(dataweeks) - idx}\" for idx, key in enumerate(unique_values.keys())}  \n",
    "    \n",
    "    for _, S2S_leadweek in simple_week.items():# Note: Modify here to batch processing algorithm #[0:1]\n",
    "        if int(S2S_leadweek[9:])-6 <= 0:\n",
    "            pass # No need to supplement with historical similar years\n",
    "        else:\n",
    "            # Loop through model numbers\n",
    "            for number in range(0, numbers):\n",
    "                if start_point < harvest_point: # Growing season in the same year\n",
    "                    WeekList_len = len(list(range(1, harvest_point - start_point + 1)))# Note: The configured hisWeekList does not seem to include the start_point week\n",
    "                    forecast_point = start_point + WeekList_len - int(S2S_leadweek[9:]) + 1\n",
    "                    end_point = forecast_point + 6  # Excludes the last week\n",
    "                    weeklist_keep = list(range(start_point, end_point))\n",
    "                    weeklist_hiscom = list(range(end_point, harvest_point + 1))\n",
    "                else:\n",
    "                    WeekList_len = len(range(1, harvest_point - start_point + 1 + 46)) + len(range(1, harvest_point - start_point + 1))\n",
    "                    forecast_point = start_point + WeekList_len - int(S2S_leadweek[9:]) + 1\n",
    "                    end_point = forecast_point + 6 - 46 if forecast_point + 6 > 46 else forecast_point + 6\n",
    "                    weeklist_keep = list(range(start_point, 47)) + list(range(1, end_point)) if start_point > end_point else list(range(start_point, end_point))\n",
    "                    weeklist_hiscom = list(range(end_point, 47)) + list(range(1, harvest_point + 1)) if end_point > harvest_point else list(range(end_point, harvest_point + 1))\n",
    "                    \n",
    "    \n",
    "                S2S_outputpath = os.path.join(inputpath_base,'02_S2S','05_WeekData','01_S2S','VI_Like',region,leadweek)\n",
    "                data_S2S_new_update = pd.read_csv(os.path.join(S2S_outputpath,'number'+ str(number) + '.csv'))\n",
    "                for climate1 in TimeFeatures_sel:\n",
    "                    weeklist_keep_feature = [f'Week{week}_{climate1}' for week in weeklist_keep]\n",
    "                    weeklist_hiscom_feature = [f'Week{week}_{climate1}' for week in weeklist_hiscom]\n",
    "                    weeklist_keep_feature_mean_hist = data_his_new_update[weeklist_keep_feature].mean()\n",
    "                    weeklist_keep_feature_mean_S2S = data_S2S_new_update[weeklist_keep_feature].mean()\n",
    "                    distance, path = fastdtw(weeklist_keep_feature_mean_S2S, weeklist_keep_feature_mean_hist)# Forecast from current week to harvest week\n",
    "                    data.append([number, S2S_leadweek, year_hist, climate1, distance]) \n",
    "                    \n",
    "dtw_distances = pd.DataFrame(data, columns=['number','Leadweek','hist_year','climate','distance'])            \n",
    "dtw_distances.to_csv(os.path.join(inputpath_base,'02_S2S','similaryears_finds.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68c0e9-6efd-4d74-830b-953dfe55ffe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "878618b3-642c-494e-92aa-477bd92190b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 按照所以选择的指标计算一个最相似的年份，补充预报后面的数据#############################\n",
    "hist_start_year = Forecastyear-30;hist_end_year = Forecastyear-1;\n",
    "min_distance_idx = dtw_distances.groupby(['number', 'Leadweek'])['distance'].idxmin()\n",
    "# 通过索引获取对应的 hist_year 和其他列（如果需要的话）\n",
    "min_distance_years = dtw_distances.loc[min_distance_idx, ['number', 'Leadweek', 'hist_year', 'distance']]\n",
    "\n",
    "\n",
    "dtw_distances = pd.read_csv(os.path.join(inputpath_base,'02_S2S','similaryears_finds.csv'))\n",
    "dataweeks = find_weeks(inputpath_base,forecastDataList)\n",
    "\n",
    "unique_values = {}\n",
    "for key, value in dataweeks.items():\n",
    "    if value not in unique_values.values():\n",
    "        unique_values[key] = value\n",
    "\n",
    "simple_week = {key: f\"leadweek_{len(dataweeks) - idx}\" for idx, key in enumerate(unique_values.keys())}   \n",
    "for _, S2S_leadweek in simple_week.items():# 注意修改这里改成批量算法 #[0:1]\n",
    "    S2S_outputpath1 = os.path.join(inputpath_base,'02_S2S','06_buildmodel','01_S2S','VI_Like',region,S2S_leadweek)\n",
    "    os.makedirs(S2S_outputpath1,exist_ok=True)\n",
    "    if int(S2S_leadweek[9:])-6<=0:\n",
    "        for number in range(0,numbers):\n",
    "            S2S_outputpath = os.path.join(inputpath_base,'02_S2S','05_WeekData','01_S2S','VI_Like',region,S2S_leadweek)\n",
    "            data_S2S_new_update= pd.read_csv(os.path.join(S2S_outputpath,'number'+ str(number) + '.csv'))\n",
    "            S2S_outputpath1 = os.path.join(inputpath_base,'02_S2S','06_buildmodel','01_S2S','VI_Like',region,S2S_leadweek)\n",
    "            data_S2S_new_update.to_csv(os.path.join(S2S_outputpath1,'number'+ str(number) + '.csv'))  \n",
    "    else:\n",
    "        # for number循环\n",
    "        for number in range(0,numbers):\n",
    "            if start_point < harvest_point: # 同年生长\n",
    "                WeekList_len = len(list(range(1,harvest_point-start_point+1)))# 设定的hisWeekList好像不包括了start_point周\n",
    "                forecast_point = start_point+WeekList_len-int(S2S_leadweek[9:])+1\n",
    "                end_point = forecast_point+6  # 不包括最后一周\n",
    "                weeklist_keep =  list(range(start_point,end_point))\n",
    "                weeklist_hiscom =  list(range(end_point,harvest_point+1))\n",
    "            else:\n",
    "                WeekList_len =len(range(1,harvest_point-start_point+1+46))+len(range(1,harvest_point-start_point+1))\n",
    "                forecast_point = start_point+WeekList_len-int(S2S_leadweek[9:])+1\n",
    "                end_point = forecast_point + 6 - 46 if forecast_point + 6 > 46 else forecast_point + 6\n",
    "                weeklist_keep =  list(range(start_point,47))+list(range(1,end_point))  if start_point > end_point else list(range(start_point,end_point))\n",
    "                weeklist_hiscom = list(range(end_point,47))+list(range(1,harvest_point+1))  if end_point > harvest_point else list(range(end_point,harvest_point+1))\n",
    "                \n",
    "    \n",
    "            S2S_outputpath = os.path.join(inputpath_base,'02_S2S','05_WeekData','01_S2S','VI_Like',region,S2S_leadweek)\n",
    "            data_S2S_new_update= pd.read_csv(os.path.join(S2S_outputpath,'number'+ str(number) + '.csv'))\n",
    "            similar_years = int(min_distance_years[(min_distance_years['number']==number)&(min_distance_years['Leadweek']==S2S_leadweek)]['hist_year'].values)\n",
    "            hist_outputpath1 = os.path.join(inputpath_base,'02_S2S','05_WeekData','02_hist',region)\n",
    "            data_his_new_update = pd.read_csv(os.path.join(hist_outputpath1,'hist_'+str(similar_years)+'.csv'))\n",
    "            data_S2S_new_update_copy = data_S2S_new_update.copy()\n",
    "            data_S2S_new_update_copy = data_S2S_new_update.set_index(['idJoin']);\n",
    "            data_his_new_update = data_his_new_update.set_index(['idJoin']);\n",
    "            weeklist_hiscom = ['Week'+str(week)+'_' for week in weeklist_hiscom]\n",
    "            weeklist_hiscom_all= [col for col in data_S2S_new_update.columns if any(feature in col for feature in weeklist_hiscom)]\n",
    "            weeklist_hiscom_all = [item for item in weeklist_hiscom_all if VI_select2 not in item]\n",
    "            \n",
    "            data_S2S_new_update_copy[weeklist_hiscom_all] = data_his_new_update[weeklist_hiscom_all];\n",
    "           # S2S_outputpath = os.path.join(inputpath_base,'02_S2S','06_buildmodel','01_S2S','VI_Like',region,leadweek)\n",
    "           # os.makedirs(S2S_outputpath,exist_ok=True)\n",
    "\n",
    "            data_S2S_new_update_copy.to_csv(os.path.join(S2S_outputpath1,'number'+ str(number) + '.csv'))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b69e5f-efff-4638-9635-119d59499542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93b28a-8856-46ed-bb65-f2b42dc14a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df869286-1470-4756-a20a-f456e6fc3522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc081f2-a54b-4fa7-a569-8547936db8ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################寻找最相似的前五年的#############################\n",
    "min_distance_years_top_5 = dtw_distances[['number','Leadweek','hist_year','distance']].groupby(['number', 'Leadweek']).apply(\n",
    "    lambda group: group.nsmallest(5, 'distance')\n",
    ").reset_index(drop=True)\n",
    "min_distance_years_top_5[(min_distance_years_top_5['number']==0)&(min_distance_years_top_5['Leadweek']=='leadweek_10')]\n",
    "\n",
    "dtw_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e707c673-34da-411a-b6a1-057f67abb05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\\\\\SCI\\\\SCI9_1\\\\02_data\\\\02_Wheat\\\\09_European\\\\02_S2S\\\\05_WeekData\\\\02_hist\\\\I\\\\hist_1992.csv'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(hist_outputpath1,'hist_'+str(similar_years)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "342bc131-209d-4351-9c27-fb23c1ad9fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "dc43171f-fee0-494d-a5f6-8b32300a71fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lead_8  45, 46, 1, 2, 3, 4\n",
    "# lead_9  44, 45, 46, 1, 2, 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003465aa-126b-4252-b6fe-13b001bdafee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ec523220-b0c2-41aa-b301-6881299824dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week19_SPEI</th>\n",
       "      <th>Week20_SPEI</th>\n",
       "      <th>Week21_SPEI</th>\n",
       "      <th>Week22_SPEI</th>\n",
       "      <th>Week23_SPEI</th>\n",
       "      <th>Week24_SPEI</th>\n",
       "      <th>Week25_SPEI</th>\n",
       "      <th>Week26_SPEI</th>\n",
       "      <th>Week27_SPEI</th>\n",
       "      <th>Week28_SPEI</th>\n",
       "      <th>...</th>\n",
       "      <th>Week19_Pre</th>\n",
       "      <th>Week20_Pre</th>\n",
       "      <th>Week21_Pre</th>\n",
       "      <th>Week22_Pre</th>\n",
       "      <th>Week23_Pre</th>\n",
       "      <th>Week24_Pre</th>\n",
       "      <th>Week25_Pre</th>\n",
       "      <th>Week26_Pre</th>\n",
       "      <th>Week27_Pre</th>\n",
       "      <th>Week28_Pre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idJoin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT32</th>\n",
       "      <td>0.408049</td>\n",
       "      <td>0.317654</td>\n",
       "      <td>0.230743</td>\n",
       "      <td>0.134998</td>\n",
       "      <td>0.044302</td>\n",
       "      <td>-0.046029</td>\n",
       "      <td>-0.137463</td>\n",
       "      <td>-0.226015</td>\n",
       "      <td>-0.316863</td>\n",
       "      <td>-0.404500</td>\n",
       "      <td>...</td>\n",
       "      <td>25.071520</td>\n",
       "      <td>41.087120</td>\n",
       "      <td>71.494306</td>\n",
       "      <td>50.573064</td>\n",
       "      <td>35.502615</td>\n",
       "      <td>49.802822</td>\n",
       "      <td>28.829998</td>\n",
       "      <td>64.776065</td>\n",
       "      <td>31.749152</td>\n",
       "      <td>69.990794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT11</th>\n",
       "      <td>0.351903</td>\n",
       "      <td>0.272603</td>\n",
       "      <td>0.197916</td>\n",
       "      <td>0.125513</td>\n",
       "      <td>0.044137</td>\n",
       "      <td>-0.030291</td>\n",
       "      <td>-0.111182</td>\n",
       "      <td>-0.189610</td>\n",
       "      <td>-0.270201</td>\n",
       "      <td>-0.353404</td>\n",
       "      <td>...</td>\n",
       "      <td>4.850283</td>\n",
       "      <td>17.734279</td>\n",
       "      <td>45.574093</td>\n",
       "      <td>46.512334</td>\n",
       "      <td>6.481053</td>\n",
       "      <td>107.623962</td>\n",
       "      <td>10.463139</td>\n",
       "      <td>36.278669</td>\n",
       "      <td>2.860229</td>\n",
       "      <td>13.403733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT33</th>\n",
       "      <td>0.466780</td>\n",
       "      <td>0.374567</td>\n",
       "      <td>0.284014</td>\n",
       "      <td>0.178718</td>\n",
       "      <td>0.086796</td>\n",
       "      <td>-0.004516</td>\n",
       "      <td>-0.096610</td>\n",
       "      <td>-0.189015</td>\n",
       "      <td>-0.280840</td>\n",
       "      <td>-0.369457</td>\n",
       "      <td>...</td>\n",
       "      <td>23.096828</td>\n",
       "      <td>60.722902</td>\n",
       "      <td>71.302267</td>\n",
       "      <td>48.175932</td>\n",
       "      <td>41.957419</td>\n",
       "      <td>57.514868</td>\n",
       "      <td>38.200027</td>\n",
       "      <td>51.286408</td>\n",
       "      <td>37.840401</td>\n",
       "      <td>73.517192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT34</th>\n",
       "      <td>0.429006</td>\n",
       "      <td>0.339865</td>\n",
       "      <td>0.252458</td>\n",
       "      <td>0.153270</td>\n",
       "      <td>0.063298</td>\n",
       "      <td>-0.025785</td>\n",
       "      <td>-0.116760</td>\n",
       "      <td>-0.207941</td>\n",
       "      <td>-0.298268</td>\n",
       "      <td>-0.385554</td>\n",
       "      <td>...</td>\n",
       "      <td>28.317897</td>\n",
       "      <td>75.382253</td>\n",
       "      <td>80.504027</td>\n",
       "      <td>55.899956</td>\n",
       "      <td>41.167696</td>\n",
       "      <td>56.112149</td>\n",
       "      <td>30.839067</td>\n",
       "      <td>42.011143</td>\n",
       "      <td>34.130929</td>\n",
       "      <td>70.382639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT22</th>\n",
       "      <td>0.362058</td>\n",
       "      <td>0.279639</td>\n",
       "      <td>0.201353</td>\n",
       "      <td>0.124048</td>\n",
       "      <td>0.039905</td>\n",
       "      <td>-0.038841</td>\n",
       "      <td>-0.122426</td>\n",
       "      <td>-0.201726</td>\n",
       "      <td>-0.284262</td>\n",
       "      <td>-0.368056</td>\n",
       "      <td>...</td>\n",
       "      <td>17.432355</td>\n",
       "      <td>21.846918</td>\n",
       "      <td>52.653036</td>\n",
       "      <td>45.285905</td>\n",
       "      <td>13.799507</td>\n",
       "      <td>83.794351</td>\n",
       "      <td>17.805066</td>\n",
       "      <td>59.125597</td>\n",
       "      <td>20.082834</td>\n",
       "      <td>34.459557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE12</th>\n",
       "      <td>0.369023</td>\n",
       "      <td>0.293792</td>\n",
       "      <td>0.216666</td>\n",
       "      <td>0.139246</td>\n",
       "      <td>0.061359</td>\n",
       "      <td>-0.020481</td>\n",
       "      <td>-0.102164</td>\n",
       "      <td>-0.181437</td>\n",
       "      <td>-0.267401</td>\n",
       "      <td>-0.345071</td>\n",
       "      <td>...</td>\n",
       "      <td>6.343060</td>\n",
       "      <td>32.012591</td>\n",
       "      <td>27.270656</td>\n",
       "      <td>14.774992</td>\n",
       "      <td>21.969098</td>\n",
       "      <td>8.994144</td>\n",
       "      <td>18.501852</td>\n",
       "      <td>15.697783</td>\n",
       "      <td>0.800087</td>\n",
       "      <td>22.877628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE31</th>\n",
       "      <td>0.393021</td>\n",
       "      <td>0.316925</td>\n",
       "      <td>0.237848</td>\n",
       "      <td>0.158581</td>\n",
       "      <td>0.081863</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.083704</td>\n",
       "      <td>-0.163548</td>\n",
       "      <td>-0.250621</td>\n",
       "      <td>-0.329829</td>\n",
       "      <td>...</td>\n",
       "      <td>8.032646</td>\n",
       "      <td>43.882330</td>\n",
       "      <td>26.354470</td>\n",
       "      <td>16.036839</td>\n",
       "      <td>53.360216</td>\n",
       "      <td>14.380335</td>\n",
       "      <td>25.319454</td>\n",
       "      <td>27.117401</td>\n",
       "      <td>0.571944</td>\n",
       "      <td>27.397082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE23</th>\n",
       "      <td>0.341699</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.193989</td>\n",
       "      <td>0.119873</td>\n",
       "      <td>0.045907</td>\n",
       "      <td>-0.032701</td>\n",
       "      <td>-0.111864</td>\n",
       "      <td>-0.188642</td>\n",
       "      <td>-0.271562</td>\n",
       "      <td>-0.347819</td>\n",
       "      <td>...</td>\n",
       "      <td>10.691078</td>\n",
       "      <td>37.226896</td>\n",
       "      <td>21.998423</td>\n",
       "      <td>28.406346</td>\n",
       "      <td>43.895362</td>\n",
       "      <td>19.456904</td>\n",
       "      <td>19.244330</td>\n",
       "      <td>21.156696</td>\n",
       "      <td>0.719964</td>\n",
       "      <td>33.797604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE22</th>\n",
       "      <td>0.340994</td>\n",
       "      <td>0.268944</td>\n",
       "      <td>0.194038</td>\n",
       "      <td>0.119387</td>\n",
       "      <td>0.045147</td>\n",
       "      <td>-0.033739</td>\n",
       "      <td>-0.111206</td>\n",
       "      <td>-0.188355</td>\n",
       "      <td>-0.269447</td>\n",
       "      <td>-0.346764</td>\n",
       "      <td>...</td>\n",
       "      <td>12.873478</td>\n",
       "      <td>35.929575</td>\n",
       "      <td>11.714310</td>\n",
       "      <td>12.151026</td>\n",
       "      <td>38.225015</td>\n",
       "      <td>3.871426</td>\n",
       "      <td>25.695477</td>\n",
       "      <td>17.154188</td>\n",
       "      <td>0.256013</td>\n",
       "      <td>33.830015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TR2</th>\n",
       "      <td>0.381070</td>\n",
       "      <td>0.305105</td>\n",
       "      <td>0.224779</td>\n",
       "      <td>0.147862</td>\n",
       "      <td>0.068841</td>\n",
       "      <td>-0.015275</td>\n",
       "      <td>-0.096520</td>\n",
       "      <td>-0.181826</td>\n",
       "      <td>-0.260832</td>\n",
       "      <td>-0.347499</td>\n",
       "      <td>...</td>\n",
       "      <td>7.815443</td>\n",
       "      <td>11.918062</td>\n",
       "      <td>9.546724</td>\n",
       "      <td>21.636890</td>\n",
       "      <td>21.098765</td>\n",
       "      <td>4.625079</td>\n",
       "      <td>5.295597</td>\n",
       "      <td>1.173964</td>\n",
       "      <td>28.684066</td>\n",
       "      <td>1.519959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Week19_SPEI  Week20_SPEI  Week21_SPEI  Week22_SPEI  Week23_SPEI  \\\n",
       "idJoin                                                                    \n",
       "AT32       0.408049     0.317654     0.230743     0.134998     0.044302   \n",
       "AT11       0.351903     0.272603     0.197916     0.125513     0.044137   \n",
       "AT33       0.466780     0.374567     0.284014     0.178718     0.086796   \n",
       "AT34       0.429006     0.339865     0.252458     0.153270     0.063298   \n",
       "AT22       0.362058     0.279639     0.201353     0.124048     0.039905   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "SE12       0.369023     0.293792     0.216666     0.139246     0.061359   \n",
       "SE31       0.393021     0.316925     0.237848     0.158581     0.081863   \n",
       "SE23       0.341699     0.268817     0.193989     0.119873     0.045907   \n",
       "SE22       0.340994     0.268944     0.194038     0.119387     0.045147   \n",
       "TR2        0.381070     0.305105     0.224779     0.147862     0.068841   \n",
       "\n",
       "        Week24_SPEI  Week25_SPEI  Week26_SPEI  Week27_SPEI  Week28_SPEI  ...  \\\n",
       "idJoin                                                                   ...   \n",
       "AT32      -0.046029    -0.137463    -0.226015    -0.316863    -0.404500  ...   \n",
       "AT11      -0.030291    -0.111182    -0.189610    -0.270201    -0.353404  ...   \n",
       "AT33      -0.004516    -0.096610    -0.189015    -0.280840    -0.369457  ...   \n",
       "AT34      -0.025785    -0.116760    -0.207941    -0.298268    -0.385554  ...   \n",
       "AT22      -0.038841    -0.122426    -0.201726    -0.284262    -0.368056  ...   \n",
       "...             ...          ...          ...          ...          ...  ...   \n",
       "SE12      -0.020481    -0.102164    -0.181437    -0.267401    -0.345071  ...   \n",
       "SE31      -0.000945    -0.083704    -0.163548    -0.250621    -0.329829  ...   \n",
       "SE23      -0.032701    -0.111864    -0.188642    -0.271562    -0.347819  ...   \n",
       "SE22      -0.033739    -0.111206    -0.188355    -0.269447    -0.346764  ...   \n",
       "TR2       -0.015275    -0.096520    -0.181826    -0.260832    -0.347499  ...   \n",
       "\n",
       "        Week19_Pre  Week20_Pre  Week21_Pre  Week22_Pre  Week23_Pre  \\\n",
       "idJoin                                                               \n",
       "AT32     25.071520   41.087120   71.494306   50.573064   35.502615   \n",
       "AT11      4.850283   17.734279   45.574093   46.512334    6.481053   \n",
       "AT33     23.096828   60.722902   71.302267   48.175932   41.957419   \n",
       "AT34     28.317897   75.382253   80.504027   55.899956   41.167696   \n",
       "AT22     17.432355   21.846918   52.653036   45.285905   13.799507   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "SE12      6.343060   32.012591   27.270656   14.774992   21.969098   \n",
       "SE31      8.032646   43.882330   26.354470   16.036839   53.360216   \n",
       "SE23     10.691078   37.226896   21.998423   28.406346   43.895362   \n",
       "SE22     12.873478   35.929575   11.714310   12.151026   38.225015   \n",
       "TR2       7.815443   11.918062    9.546724   21.636890   21.098765   \n",
       "\n",
       "        Week24_Pre  Week25_Pre  Week26_Pre  Week27_Pre  Week28_Pre  \n",
       "idJoin                                                              \n",
       "AT32     49.802822   28.829998   64.776065   31.749152   69.990794  \n",
       "AT11    107.623962   10.463139   36.278669    2.860229   13.403733  \n",
       "AT33     57.514868   38.200027   51.286408   37.840401   73.517192  \n",
       "AT34     56.112149   30.839067   42.011143   34.130929   70.382639  \n",
       "AT22     83.794351   17.805066   59.125597   20.082834   34.459557  \n",
       "...            ...         ...         ...         ...         ...  \n",
       "SE12      8.994144   18.501852   15.697783    0.800087   22.877628  \n",
       "SE31     14.380335   25.319454   27.117401    0.571944   27.397082  \n",
       "SE23     19.456904   19.244330   21.156696    0.719964   33.797604  \n",
       "SE22      3.871426   25.695477   17.154188    0.256013   33.830015  \n",
       "TR2       4.625079    5.295597    1.173964   28.684066    1.519959  \n",
       "\n",
       "[208 rows x 70 columns]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_his_new_update[weeklist_hiscom_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "12966a90-4b69-40b8-b82f-13e68ee9cf22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week19_SPEI</th>\n",
       "      <th>Week20_SPEI</th>\n",
       "      <th>Week21_SPEI</th>\n",
       "      <th>Week22_SPEI</th>\n",
       "      <th>Week23_SPEI</th>\n",
       "      <th>Week24_SPEI</th>\n",
       "      <th>Week25_SPEI</th>\n",
       "      <th>Week26_SPEI</th>\n",
       "      <th>Week27_SPEI</th>\n",
       "      <th>Week28_SPEI</th>\n",
       "      <th>...</th>\n",
       "      <th>Week19_Pre</th>\n",
       "      <th>Week20_Pre</th>\n",
       "      <th>Week21_Pre</th>\n",
       "      <th>Week22_Pre</th>\n",
       "      <th>Week23_Pre</th>\n",
       "      <th>Week24_Pre</th>\n",
       "      <th>Week25_Pre</th>\n",
       "      <th>Week26_Pre</th>\n",
       "      <th>Week27_Pre</th>\n",
       "      <th>Week28_Pre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idJoin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT11</th>\n",
       "      <td>0.378546</td>\n",
       "      <td>0.299539</td>\n",
       "      <td>0.214343</td>\n",
       "      <td>0.136893</td>\n",
       "      <td>0.051822</td>\n",
       "      <td>-0.028318</td>\n",
       "      <td>-0.104630</td>\n",
       "      <td>-0.189960</td>\n",
       "      <td>-0.268945</td>\n",
       "      <td>-0.339080</td>\n",
       "      <td>...</td>\n",
       "      <td>43.052968</td>\n",
       "      <td>5.913522</td>\n",
       "      <td>4.578223</td>\n",
       "      <td>56.943066</td>\n",
       "      <td>6.227609</td>\n",
       "      <td>5.700843</td>\n",
       "      <td>25.590212</td>\n",
       "      <td>20.853666</td>\n",
       "      <td>32.635587</td>\n",
       "      <td>36.962891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT12</th>\n",
       "      <td>0.372131</td>\n",
       "      <td>0.293921</td>\n",
       "      <td>0.210468</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>0.048054</td>\n",
       "      <td>-0.031043</td>\n",
       "      <td>-0.106907</td>\n",
       "      <td>-0.191283</td>\n",
       "      <td>-0.269091</td>\n",
       "      <td>-0.351541</td>\n",
       "      <td>...</td>\n",
       "      <td>30.556206</td>\n",
       "      <td>13.305013</td>\n",
       "      <td>12.525048</td>\n",
       "      <td>46.673459</td>\n",
       "      <td>5.005720</td>\n",
       "      <td>4.043101</td>\n",
       "      <td>24.757369</td>\n",
       "      <td>21.770578</td>\n",
       "      <td>36.404958</td>\n",
       "      <td>8.632812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT13</th>\n",
       "      <td>0.377216</td>\n",
       "      <td>0.299195</td>\n",
       "      <td>0.214661</td>\n",
       "      <td>0.137157</td>\n",
       "      <td>0.052376</td>\n",
       "      <td>-0.027251</td>\n",
       "      <td>-0.102619</td>\n",
       "      <td>-0.187918</td>\n",
       "      <td>-0.266642</td>\n",
       "      <td>-0.368250</td>\n",
       "      <td>...</td>\n",
       "      <td>27.988845</td>\n",
       "      <td>13.013152</td>\n",
       "      <td>10.023957</td>\n",
       "      <td>53.101030</td>\n",
       "      <td>6.708251</td>\n",
       "      <td>2.404604</td>\n",
       "      <td>31.635641</td>\n",
       "      <td>17.140076</td>\n",
       "      <td>30.350946</td>\n",
       "      <td>39.494141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT21</th>\n",
       "      <td>0.344479</td>\n",
       "      <td>0.265866</td>\n",
       "      <td>0.187785</td>\n",
       "      <td>0.111173</td>\n",
       "      <td>0.028251</td>\n",
       "      <td>-0.049602</td>\n",
       "      <td>-0.126448</td>\n",
       "      <td>-0.208941</td>\n",
       "      <td>-0.283995</td>\n",
       "      <td>-0.353736</td>\n",
       "      <td>...</td>\n",
       "      <td>55.339939</td>\n",
       "      <td>7.312349</td>\n",
       "      <td>34.588423</td>\n",
       "      <td>47.550913</td>\n",
       "      <td>24.407311</td>\n",
       "      <td>26.520981</td>\n",
       "      <td>27.039461</td>\n",
       "      <td>23.984824</td>\n",
       "      <td>62.519919</td>\n",
       "      <td>42.664063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AT22</th>\n",
       "      <td>0.359433</td>\n",
       "      <td>0.280675</td>\n",
       "      <td>0.198703</td>\n",
       "      <td>0.121564</td>\n",
       "      <td>0.038662</td>\n",
       "      <td>-0.038861</td>\n",
       "      <td>-0.114958</td>\n",
       "      <td>-0.199031</td>\n",
       "      <td>-0.274762</td>\n",
       "      <td>-0.355612</td>\n",
       "      <td>...</td>\n",
       "      <td>62.430359</td>\n",
       "      <td>9.096199</td>\n",
       "      <td>11.554838</td>\n",
       "      <td>43.601207</td>\n",
       "      <td>19.967512</td>\n",
       "      <td>21.879918</td>\n",
       "      <td>27.018040</td>\n",
       "      <td>17.007322</td>\n",
       "      <td>53.959546</td>\n",
       "      <td>17.478515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE31</th>\n",
       "      <td>0.348896</td>\n",
       "      <td>0.272956</td>\n",
       "      <td>0.197666</td>\n",
       "      <td>0.120931</td>\n",
       "      <td>0.043587</td>\n",
       "      <td>-0.031823</td>\n",
       "      <td>-0.108883</td>\n",
       "      <td>-0.191004</td>\n",
       "      <td>-0.270605</td>\n",
       "      <td>-0.333621</td>\n",
       "      <td>...</td>\n",
       "      <td>31.375346</td>\n",
       "      <td>26.210009</td>\n",
       "      <td>24.781536</td>\n",
       "      <td>19.667035</td>\n",
       "      <td>12.529726</td>\n",
       "      <td>19.681955</td>\n",
       "      <td>19.963743</td>\n",
       "      <td>14.792492</td>\n",
       "      <td>14.670331</td>\n",
       "      <td>26.217773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK01</th>\n",
       "      <td>0.385434</td>\n",
       "      <td>0.306823</td>\n",
       "      <td>0.222257</td>\n",
       "      <td>0.143939</td>\n",
       "      <td>0.058719</td>\n",
       "      <td>-0.021740</td>\n",
       "      <td>-0.098224</td>\n",
       "      <td>-0.182930</td>\n",
       "      <td>-0.263245</td>\n",
       "      <td>-0.342011</td>\n",
       "      <td>...</td>\n",
       "      <td>37.278366</td>\n",
       "      <td>11.012713</td>\n",
       "      <td>10.027587</td>\n",
       "      <td>50.345584</td>\n",
       "      <td>3.058030</td>\n",
       "      <td>2.959487</td>\n",
       "      <td>22.625322</td>\n",
       "      <td>22.536374</td>\n",
       "      <td>22.714922</td>\n",
       "      <td>27.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK02</th>\n",
       "      <td>0.378208</td>\n",
       "      <td>0.300011</td>\n",
       "      <td>0.215970</td>\n",
       "      <td>0.137516</td>\n",
       "      <td>0.052806</td>\n",
       "      <td>-0.027222</td>\n",
       "      <td>-0.103220</td>\n",
       "      <td>-0.185984</td>\n",
       "      <td>-0.265881</td>\n",
       "      <td>-0.344494</td>\n",
       "      <td>...</td>\n",
       "      <td>43.579727</td>\n",
       "      <td>11.488854</td>\n",
       "      <td>12.445776</td>\n",
       "      <td>45.730191</td>\n",
       "      <td>2.859425</td>\n",
       "      <td>1.495537</td>\n",
       "      <td>21.852090</td>\n",
       "      <td>27.156459</td>\n",
       "      <td>26.103275</td>\n",
       "      <td>18.910156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK03</th>\n",
       "      <td>0.375680</td>\n",
       "      <td>0.296848</td>\n",
       "      <td>0.212021</td>\n",
       "      <td>0.132818</td>\n",
       "      <td>0.048599</td>\n",
       "      <td>-0.030669</td>\n",
       "      <td>-0.106990</td>\n",
       "      <td>-0.187286</td>\n",
       "      <td>-0.265617</td>\n",
       "      <td>-0.348444</td>\n",
       "      <td>...</td>\n",
       "      <td>46.073651</td>\n",
       "      <td>8.267073</td>\n",
       "      <td>10.198250</td>\n",
       "      <td>34.823728</td>\n",
       "      <td>4.258134</td>\n",
       "      <td>4.730501</td>\n",
       "      <td>17.249563</td>\n",
       "      <td>24.844960</td>\n",
       "      <td>38.616156</td>\n",
       "      <td>22.107422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK04</th>\n",
       "      <td>0.371777</td>\n",
       "      <td>0.292809</td>\n",
       "      <td>0.207046</td>\n",
       "      <td>0.126882</td>\n",
       "      <td>0.043194</td>\n",
       "      <td>-0.034135</td>\n",
       "      <td>-0.111104</td>\n",
       "      <td>-0.189586</td>\n",
       "      <td>-0.268923</td>\n",
       "      <td>-0.339685</td>\n",
       "      <td>...</td>\n",
       "      <td>46.835763</td>\n",
       "      <td>9.916658</td>\n",
       "      <td>9.022992</td>\n",
       "      <td>33.769911</td>\n",
       "      <td>12.159632</td>\n",
       "      <td>16.070231</td>\n",
       "      <td>10.391138</td>\n",
       "      <td>34.285675</td>\n",
       "      <td>31.834879</td>\n",
       "      <td>30.914063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Week19_SPEI  Week20_SPEI  Week21_SPEI  Week22_SPEI  Week23_SPEI  \\\n",
       "idJoin                                                                    \n",
       "AT11       0.378546     0.299539     0.214343     0.136893     0.051822   \n",
       "AT12       0.372131     0.293921     0.210468     0.132705     0.048054   \n",
       "AT13       0.377216     0.299195     0.214661     0.137157     0.052376   \n",
       "AT21       0.344479     0.265866     0.187785     0.111173     0.028251   \n",
       "AT22       0.359433     0.280675     0.198703     0.121564     0.038662   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "SE31       0.348896     0.272956     0.197666     0.120931     0.043587   \n",
       "SK01       0.385434     0.306823     0.222257     0.143939     0.058719   \n",
       "SK02       0.378208     0.300011     0.215970     0.137516     0.052806   \n",
       "SK03       0.375680     0.296848     0.212021     0.132818     0.048599   \n",
       "SK04       0.371777     0.292809     0.207046     0.126882     0.043194   \n",
       "\n",
       "        Week24_SPEI  Week25_SPEI  Week26_SPEI  Week27_SPEI  Week28_SPEI  ...  \\\n",
       "idJoin                                                                   ...   \n",
       "AT11      -0.028318    -0.104630    -0.189960    -0.268945    -0.339080  ...   \n",
       "AT12      -0.031043    -0.106907    -0.191283    -0.269091    -0.351541  ...   \n",
       "AT13      -0.027251    -0.102619    -0.187918    -0.266642    -0.368250  ...   \n",
       "AT21      -0.049602    -0.126448    -0.208941    -0.283995    -0.353736  ...   \n",
       "AT22      -0.038861    -0.114958    -0.199031    -0.274762    -0.355612  ...   \n",
       "...             ...          ...          ...          ...          ...  ...   \n",
       "SE31      -0.031823    -0.108883    -0.191004    -0.270605    -0.333621  ...   \n",
       "SK01      -0.021740    -0.098224    -0.182930    -0.263245    -0.342011  ...   \n",
       "SK02      -0.027222    -0.103220    -0.185984    -0.265881    -0.344494  ...   \n",
       "SK03      -0.030669    -0.106990    -0.187286    -0.265617    -0.348444  ...   \n",
       "SK04      -0.034135    -0.111104    -0.189586    -0.268923    -0.339685  ...   \n",
       "\n",
       "        Week19_Pre  Week20_Pre  Week21_Pre  Week22_Pre  Week23_Pre  \\\n",
       "idJoin                                                               \n",
       "AT11     43.052968    5.913522    4.578223   56.943066    6.227609   \n",
       "AT12     30.556206   13.305013   12.525048   46.673459    5.005720   \n",
       "AT13     27.988845   13.013152   10.023957   53.101030    6.708251   \n",
       "AT21     55.339939    7.312349   34.588423   47.550913   24.407311   \n",
       "AT22     62.430359    9.096199   11.554838   43.601207   19.967512   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "SE31     31.375346   26.210009   24.781536   19.667035   12.529726   \n",
       "SK01     37.278366   11.012713   10.027587   50.345584    3.058030   \n",
       "SK02     43.579727   11.488854   12.445776   45.730191    2.859425   \n",
       "SK03     46.073651    8.267073   10.198250   34.823728    4.258134   \n",
       "SK04     46.835763    9.916658    9.022992   33.769911   12.159632   \n",
       "\n",
       "        Week24_Pre  Week25_Pre  Week26_Pre  Week27_Pre  Week28_Pre  \n",
       "idJoin                                                              \n",
       "AT11      5.700843   25.590212   20.853666   32.635587   36.962891  \n",
       "AT12      4.043101   24.757369   21.770578   36.404958    8.632812  \n",
       "AT13      2.404604   31.635641   17.140076   30.350946   39.494141  \n",
       "AT21     26.520981   27.039461   23.984824   62.519919   42.664063  \n",
       "AT22     21.879918   27.018040   17.007322   53.959546   17.478515  \n",
       "...            ...         ...         ...         ...         ...  \n",
       "SE31     19.681955   19.963743   14.792492   14.670331   26.217773  \n",
       "SK01      2.959487   22.625322   22.536374   22.714922   27.515625  \n",
       "SK02      1.495537   21.852090   27.156459   26.103275   18.910156  \n",
       "SK03      4.730501   17.249563   24.844960   38.616156   22.107422  \n",
       "SK04     16.070231   10.391138   34.285675   31.834879   30.914063  \n",
       "\n",
       "[170 rows x 70 columns]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_S2S_new_update.set_index(['idJoin'])[weeklist_hiscom_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa2ec6-a3b5-4018-ad39-3dae420adb9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
