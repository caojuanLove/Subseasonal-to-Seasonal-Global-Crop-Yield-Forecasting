{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb75241-ac8e-423c-bc45-fc636aeabe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Jun 25 00:51:49 2025\n",
    "\n",
    "@author: DELL\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler  # Keep only one import of StandardScaler\n",
    "import shutil\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#from IPython.display import display\n",
    "#from tqdm.notebook import tqdm\n",
    "import tensorflow.keras.backend as K\n",
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras API used:\", tf.keras.__version__)\n",
    "print(\"SHAP version:\", shap.__version__)\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to parse each line of data\n",
    "def parse_line(line):\n",
    "    parts = line.strip().split('\\t')\n",
    "    crop = parts[0]\n",
    "    filename = parts[1]\n",
    "    region = parts[2]\n",
    "    start_year = parts[3] if parts[3] else None  # Set to None if empty\n",
    "    end_year = parts[4] if parts[4] else None    # Set to None if empty\n",
    "    pre_name1 = parts[5] \n",
    "    pre_name2 = parts[6]\n",
    "    pre_name3 = parts[7] \n",
    "    shp_name = parts[8] \n",
    "    flag = parts[9]\n",
    "    number =  parts[10]\n",
    "    return crop, filename, region, start_year, end_year, pre_name1, pre_name2, pre_name3, shp_name, flag, number\n",
    "\n",
    "\n",
    "# Define a function to extract selected variables\n",
    "def extract_selected_variables(inputpath_base):\n",
    "    inpath_dates = os.path.join(inputpath_base, '01_data', '05_buildmodel', '04_selectFeatures', 'selectFeatures.txt')\n",
    "    # Construct file path\n",
    "    gs_infornamtion = pd.read_csv(inpath_dates, sep='\\t', header=None)\n",
    "    gs_infornamtion.columns = ['slected_dynamic_features', 'slected_static', 'regionID']\n",
    "    gs_infornamtion['slected_dynamic_features'] = gs_infornamtion['slected_dynamic_features'].apply(ast.literal_eval)\n",
    "    gs_infornamtion['slected_static'] = gs_infornamtion['slected_static'].apply(ast.literal_eval)\n",
    "    return gs_infornamtion\n",
    "    \n",
    "modelname = 'LSTM'; yield_type = 'actual_yield';\n",
    "\n",
    "data_lines = [\n",
    "    ### Maize 0-13\n",
    "    \n",
    "    \"01_Maize\\t01_China\\tII\\t2001\\t2017\\tMaize_ChinaII_\\tmaize_8aggChinaII_\\tMaize_ChinaII_\\tChinagee_up_load_regionII.shp\\tB\\t2\",\n",
    "    \"01_Maize\\t01_China\\tI\\t2001\\t2017\\tMaize_ChinaI_\\tmaize_8aggChinaI_\\tMaize_ChinaI_\\tChinagee_up_load_region_I.shp\\tB\\t1\",\n",
    "    \"01_Maize\\t08_Indonesia\\tI\\t2001\\t2015\\tMaize_dailyI_\\tMaize_8aggI_\\tMaize_I_\\tIndonesiagee_up_load.shp\\tB\\t3\",\n",
    "    \"01_Maize\\t05_India\\tI\\t2001\\t2018\\tMaize_dailyI_\\tMaize_8aggI_\\tMaize_I_\\tIndiagee_up_load.shp\\tB\\t4\",#\n",
    "    \"01_Maize\\t07_Ukraine\\tI\\t2004\\t2019\\tMaize_dailyI_\\tMaize_8aggI_\\tMaize_I_\\tUkrainegee_up_load.shp\\tB\\t5\", #\n",
    "    \"01_Maize\\t03_European\\tI\\t2001\\t2019\\tMaize_dailyI_\\tMaize_8aggI_\\tMaize_I_\\tEuropeangee_up_load.shp\\tB\\t6\",#\n",
    "    \"01_Maize\\t09_Africa\\tI\\t2001\\t2018\\tMaize_dailyI_\\tMaize_8aggI_\\tMaize_I_\\tAfrica5_I.shp\\tB\\t7\",\n",
    "    \"01_Maize\\t09_Africa\\tII\\t2001\\t2018\\tMaize_dailyII_\\tMaize_8aggII_\\tMaize_II_\\tAfrica5_II.shp\\tB\\t8\",\n",
    "    \"01_Maize\\t09_Africa\\tIII\\t2001\\t2018\\tMaize_dailyIII_\\tMaize_8aggIII_\\tMaize_III_\\tAfrica5_III.shp\\tB\\t9\",\n",
    "    \"01_Maize\\t04_Argentina\\tI\\t2001\\t2017\\tMaize_dailyI_\\tMaize_8aggI_\\tMaize_I_\\tArgentinagee_up_load.shp\\tB\\t10\",\n",
    "    \"01_Maize\\t03_Brazil\\tI\\t2001\\t2019\\tMaize_dailyBrazilI_\\tMaize_8aggBrazilI_\\thist_BrazilI_\\tBrazilgee_up_loadI.shp\\tB\\t11\",\n",
    "    \"01_Maize\\t03_Brazil\\tII\\t2001\\t2019\\tMaize_dailyBrazilII_\\tMaize_8aggBrazilII_\\thist_BrazilII_\\tBrazilgee_up_loadII.shp\\tB\\t12\",\n",
    "    \"01_Maize\\t06_Mexico\\tI\\t2003\\t2018\\tMaize_dailyI_\\tMaize_8aggSprsum_\\tMaize_I_\\tMexicogee_up_loadSprsum.shp\\tB\\t13\",\n",
    "    \"01_Maize\\t02_US\\tI\\t2001\\t2018\\tMaize_USI_\\tmaize_8aggUSI_\\tMaize_USI_\\tUSgee_up_load.shp\\tB\\t14\", \n",
    "    \n",
    "     ### Wheat 14-26\n",
    "    \"02_Wheat\\t08_Russia\\tII\\t2001\\t2019\\tSpringWheat_RussiaII_\\tSpringWheat_8aggRussiaII_\\tWheat_II_\\tRussiagee_up_load_spring.shp\\tB\\t1\",\n",
    "    \"02_Wheat\\t08_Russia\\tI\\t2001\\t2019\\tWinterWheat_RussiaI_\\tWinterWheat_8aggRussiaI_\\tWheat_I_\\tRussiagee_up_load_winter.shp\\tB\\t2\",\n",
    "    \"02_Wheat\\t10_Ukraine\\tI\\t2004\\t2019\\tWinterWheat_UkraineI_\\tWinterWheat_8aggUkraineI_\\tWheat_I_\\tUkrainegee_up_load_winter.shp\\tB\\t3\",\n",
    "    \"02_Wheat\\t09_European\\tI\\t2001\\t2019\\tWinterWheat_European_\\tWinterWheat_8aggEuropeanI_\\tWheat_I_\\tEuropeangee_up_load_winter.shp\\tB\\t4\",\n",
    "    \"02_Wheat\\t07_China\\tI\\t2001\\t2017\\tSpringWheat_ChinaI_\\tSpringWheat_8aggChinaI_\\tWheat_I_\\tchina_spring_wheat.shp\\tB\\t5\",\n",
    "    \"02_Wheat\\t07_China\\tII\\t2001\\t2017\\tWinterWheat_ChinaII_\\tSpringWheat_8aggChinaI_\\tWheat_I_\\tChinagee_up_load_winter.shp\\tB\\t6\",\n",
    "    \"02_Wheat\\t04_Australia\\tI\\t2005\\t2020\\tWinterWheat_AustraliaI_\\tWinterWheat_8aggAustaliaI_\\tWheat_I_1\\tAustraliagee_up_load_winter.shp\\tB\\t7\",\n",
    "    \"02_Wheat\\t06_India\\tI\\t2001\\t2018\\tWinterWheat_IndiaI_\\tWinterWheat_8aggIndiaI_\\tWheat_I_\\tIndiagee_up_load_winter.shp\\tB\\t8\",\n",
    "    \"02_Wheat\\t12_Pakistan\\tI\\t2001\\t2014\\tWinterWheat_Pakistan_\\tWinterWheat_8aggPakistanI_\\tWheat_I_\\t'Pakistangee_up_load_winter.shp\\tB\\t9\",\n",
    "    \"02_Wheat\\t05_Canada\\tI\\t2001\\t2019\\tSpringWheat_Canada_\\tSpringWheat_8aggCanada_\\tWheat_I_\\t'Canadagee_up_load_spring.shp\\tA\\t10\", # \n",
    "    \"02_Wheat\\t02_US\\tI\\t2001\\t2018\\tWinterWheat_USII_\\tWinterWheat_8aggUSII_\\tWheat_II_\\tUSgee_up_load_winter.shp\\tB\\t11\",    \n",
    "    \"02_Wheat\\t02_US\\tII\\t2001\\t2018\\tSpringWheat_USI_\\tSpringWheat_8aggUSI_\\tWheat_I_\\tUSgee_up_load_Spring.shp\\tB\\t12\",\n",
    "    \"02_Wheat\\t03_Argentina\\tI\\t2001\\t2017\\tWinterWheat_ArgentinaI_\\tWinterWheat_8aggArgentinaI_\\tWheat_I_\\tArgentinagee_up_load_winter.shp\\tB\\t13\",\n",
    "    \n",
    "    ### Rice 27\n",
    "    \"03_Rice\\t10_Japan\\tI\\t2001\\t2017\\tSingleRice_dailyJapan_SingleRice_\\tSingleRice_8aggJapan_SingleRice_\\tRice_I_1971\\tJapangee_up_load_single.shp\\tB\\t1\",\n",
    "    \"03_Rice\\t13_Southkorea\\tI\\t2001\\t2018\\tSingleRice_dailyKorea_SingleRice_\\tSingleRice_8aggKorea_SingleRice_\\tRice_I_\\tSouthkoreagee_up_load_single.shp\\tB\\t2\",\n",
    "    \"03_Rice\\t01_China\\tI\\t2001\\t2017\\tSingleRice_dailyChinaI_\\tSingleRice_8aggChinaI_\\tRice_I_\\tChinagee_up_load_rice_I.shp\\tB\\t3\",\n",
    "    \"03_Rice\\t01_China\\tII\\t2001\\t2017\\tEarlyRice_dailyChinaII_\\tEarlyRice_8aggChinaII_\\tRice_II_\\tChinagee_up_load_rice_II.shp\\tB\\t4\",\n",
    "    \"03_Rice\\t08_Philippines\\tI\\t2001\\t2016\\tDoubleRice_dailyPhilippines_DoubleRice_\\tDoubleRice_8aggPhilippines_DoubleRice_\\tRice_I_\\thilippinesgee_up_load_double1.shp\\tB\\t5\",\n",
    "    \"03_Rice\\t03_Indonesia\\tI\\t2001\\t2015\\tSingleRice_dailyIndonesia_SingleRice_\\tSingleRice_8aggIndonesia_SingleRice_\\tRice_I_\\tIndonesiagee_up_load_single.shp\\tB\\t6\", # \n",
    "    \"03_Rice\\t04_Vietnam\\tI\\t2001\\t2018\\tDoubleRice_dailyVietnam_DoubleRice_\\tDoubleRice_8aggVietnam_DoubleRice_\\tRice_I_\\tVietnamgee_up_load_double1.shp\\tB\\t7\",\n",
    "    \"03_Rice\\t04_Vietnam\\tII\\t2001\\t2018\\tTripleRice_dailyVietnam_TripleRice_\\tTripleRice_8aggVietnam_TripleRice_\\tRice_II_\\tVietnamgee_up_load_triple1.shp\\tB\\t8\",\n",
    "    \"03_Rice\\t14_Cambodia\\tI\\t2001\\t2013\\tSingleRice_dailyCambodia_SingleRice_\\tSingleRice_8aggCambodia_SingleRice_\\tRice_I_\\tCambodiagee_up_load_single.shp\\tB\\t9\",\n",
    "    \"03_Rice\\t07_Thailand\\tI\\t2001\\t2017\\tRice_dailyThailand_Rice_\\tRice_8aggThailand_Rice_\\tRice_I_\\tThailand_gee_up_load.shp\\tB\\t10\",\n",
    "    \"03_Rice\\t11_Myanmar\\tI\\t2001\\t2016\\tSingleRice_dailyMyanmar_SingleRice_\\tpre\\tRice_I_\\tMyanmargee_up_load_single.shp\\tB\\t11\", # Data downloading in progress\n",
    "    \"03_Rice\\t06_Bangladesh\\tI\\t2001\\t2017\\tTripleRice_dailyBangladesh_TripleRice_\\tTripleRice_8aggBangladesh_TripleRice\\tRice_I_\\tBangladeshgee_up_load_triple2.shp\\tB\\t12\",\n",
    "    \"03_Rice\\t02_India\\tI\\t2001\\t2018\\tSingleRRice_dailyIndiaI_\\tSingleRice_8aggIndiaI_\\tRice_I_\\tIndiagee_up_load_I.shp\\tB\\t13\",\n",
    "    \"03_Rice\\t02_India\\tII\\t2001\\t2018\\tEarlyRice_dailyIndiaII_\\tEarlyRice_8aggIndiaII_\\tRice_II_\\tIndiagee_up_load_II.shp\\tB\\t14\",\n",
    "    \"03_Rice\\t05_Pakistan\\tI\\t2001\\t2014\\tRiceI_\\tSingleRice_8aggPakistan_SingleRice_\\tRice_I_\\tPakistangee_up_load_single.shp\\tB\\t15\",\n",
    "    \"03_Rice\\t12_US\\tI\\t2001\\t2018\\tRice_dailyUS_I_\\tUS_8aggUS_Rice_\\tRice_I_\\tUSgee_up_load.shp\\tB\\t16\",\n",
    "    \"03_Rice\\t09_Brazil\\tI\\t2001\\t2019\\tRice_dailyBrazil_I_\\tRice_8aggBrazilI_\\tRice_I_\\tBrazilgee_up_load.shp\\tB\\t17\" ,\n",
    "    \n",
    "    ### Soybean 44\n",
    "    \"04_Soybean\\t07_China\\tI\\t2009\\t2017\\tSoybean_ChinaI\\tsoybean_8aggChinaI_\\tSoybean_I_1\\tChina_Final_shp.shp\\tB\\t1\",\n",
    "    \"04_Soybean\\t06_India\\tI\\t2001\\t2018\\tSoybean_India_\\tsoybean_8aggIndia_\\tSoybean_I_\\tIndiagee_up_load.shp\\tB\\t2\",# With errors\n",
    "    \"04_Soybean\\t05_Canada\\tI\\t2007\\t2019\\tSoybean_Canada_\\tsoybean_8aggCanada_\\tSoybean_I_\\tfiltered_data_ori_gee_up_load.shp\\tB\\t3\",\n",
    "    \"04_Soybean\\t02_US\\tI\\t2001\\t2018\\tsoybean_dailyUSI_\\tSoybean_dailyUSI_\\tSoybean_I_\\tUS_mergeall.shp\\tB\\t4\", # Data downloading in progress\n",
    "    \"04_Soybean\\t01_Brazil\\tI\\t2001\\t2019\\tSoybean_BrazilI_\\tSoybean_BrazilI_\\thist_\\tBrazilgee_up_load_reproject.shp\\tB\\t5\" ,\n",
    "    \"04_Soybean\\t03_Argentina_Uruguay\\tI\\t2001\\t2017\\tSoybean_Argentina_Uruguay_\\tsoybean_8aggArgentina_Uruguay_\\tSoybean_I_\\tgee_up_load.shp\\tB\\t6\",\n",
    "\n",
    "]\n",
    "\n",
    "# Get root directory (assuming root_directory is defined in the original environment; if not, uncomment the following line)\n",
    "# root_directory = os.getcwd()[0:3]  # Adjust based on actual path structure\n",
    "\n",
    "for line in data_lines:\n",
    "    crop, countryID, region, startyear, endyear, pre_name1, pre_name2, pre_name3, shp_name, flag, number = parse_line(line)\n",
    "    print(crop, countryID, region)\n",
    "    startyear = int(startyear); endyear = int(endyear)\n",
    "    inputpath_base = os.path.join(root_directory, 'SCI', 'SCI9_1', '02_data', crop, countryID)\n",
    "    startyear = startyear; Forecastyear = endyear  # Start year for the model\n",
    "    years = range(startyear, endyear + 1); country = countryID[3:]\n",
    "    years_str = [str(year) for year in years]\n",
    "    SelFeature_infornamtion = extract_selected_variables(inputpath_base)\n",
    "    ################### Read selected factor variables #########################################\n",
    "\n",
    "    data = pd.read_csv(os.path.join(inputpath_base, '01_data', '05_buildmodel', '03_modeldata', region + '_data_ori.csv'))\n",
    "   # data = data[data['idJoin'] == 'CAN_15']\n",
    "    TimeFeatures_sel, Static_sel, regionID = SelFeature_infornamtion[SelFeature_infornamtion['regionID'] == region].iloc[0]\n",
    "    feature_all = TimeFeatures_sel + Static_sel\n",
    "    filtered_columns = [col for col in data.columns if any(feature in col for feature in feature_all)]\n",
    "\n",
    "    # Additional filtering logic for flag == 'B'\n",
    "    feature_all = [col for col in feature_all if 'Yield' not in col]\n",
    "    filtered_columns = [col for col in filtered_columns if 'Yield' not in col]  # Remove yield correlation\n",
    "    filtered_columns = [col for col in filtered_columns if 'year.1' not in col]  # Remove redundant 'year.1' column\n",
    "    Static_sel = [col for col in Static_sel if 'Yield' not in col]  # Remove yield correlation\n",
    "\n",
    "        \n",
    "    MLdata_reduced = data[filtered_columns + [yield_type]]\n",
    "    MLdata_reduced['year'] = data['year']\n",
    "    # Read selected weeks\n",
    "    inpath_dates = os.path.join(inputpath_base, '01_data', '05_buildmodel', '02_extractdates', 'gs_three_periods.txt')\n",
    "    gs_infornamtion = pd.read_csv(inpath_dates, delim_whitespace=True, header=None)\n",
    "    gs_infornamtion.columns = ['start_point', 'peak', 'harvest_point', 'VI_select2', 'regionID']\n",
    "    start_point, peak, harvest_point, VI_select2, region = gs_infornamtion[gs_infornamtion['regionID'] == region].iloc[0]\n",
    "    ############################ Parameter tuning data preparation ###########################################################\n",
    "    data_all = MLdata_reduced; X_all = data_all.drop([yield_type], axis=1);\n",
    "    \n",
    "    y_all = data_all[yield_type];\n",
    "    # Data standardization\n",
    "    scaler_X = StandardScaler().fit(X_all)\n",
    "    X = scaler_X.transform(X_all)\n",
    "    scaler_y = StandardScaler().fit(y_all.values.reshape(-1, 1))\n",
    "    y = scaler_y.transform(y_all.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    X = pd.DataFrame(data=X, columns=X_all.columns.tolist())\n",
    "    if start_point > harvest_point:\n",
    "        weeks_select_list = list(range(start_point, 47)) + list(range(1, harvest_point + 1))\n",
    "    else:\n",
    "        weeks_select_list = list(range(start_point, harvest_point + 1))\n",
    "        \n",
    "    data_list = []\n",
    "    for i in weeks_select_list:\n",
    "        newfeatures = X[[f'Week{i}_{feature}' for feature in TimeFeatures_sel] + Static_sel]\n",
    "        data_i = X[[f'Week{i}_{feature}' for feature in TimeFeatures_sel] + Static_sel]\n",
    "        data_list.append(data_i.values)\n",
    "\n",
    "    \n",
    "    # Stack all arrays in data_list into a 3D array\n",
    "    data_list = np.array(data_list)\n",
    "    X_all_lstm = np.transpose(data_list, (1, 0, 2))  # Reshape the array into (samples, time steps, features)\n",
    "\n",
    "\n",
    "    ########################### Prepare background data ########################\n",
    "    data_background = data[filtered_columns + ['idJoin']].groupby('idJoin').mean()  # Calculate mean values\n",
    "    scaler_X_background = scaler_X.transform(data_background)\n",
    "    \n",
    "    X_background = pd.DataFrame(data=scaler_X_background, columns=X_all.columns.tolist())\n",
    "    \n",
    "    \n",
    "    data_list_background = []\n",
    "    for i in weeks_select_list:\n",
    "        newfeatures = X_background[[f'Week{i}_{feature}' for feature in TimeFeatures_sel] + Static_sel]\n",
    "        data_i = X_background[[f'Week{i}_{feature}' for feature in TimeFeatures_sel] + Static_sel]\n",
    "        data_list_background.append(data_i.values)\n",
    "    \n",
    "    \n",
    "    # Stack all arrays in data_list_background into a 3D array\n",
    "    data_list_background = np.array(data_list_background)\n",
    "    X_all_background = np.transpose(data_list_background, (1, 0, 2))  # Reshape into (samples, time steps, features)\n",
    "    ########################### Prepare background data ########################\n",
    "    \n",
    "    ########################### Model import ###########################\n",
    "    outpathmodel = os.path.join(inputpath_base, '04_model', region, 'LSTM')\n",
    "    model_path = os.path.join(outpathmodel, region + 'my_lstm_model.keras')\n",
    "    model = load_model(model_path)\n",
    "    ########################### SHAP interpretation for LSTM model ###########################\n",
    "    # ✅ Manually use GradientExplainer (key point)\n",
    "    explainer = shap.GradientExplainer(model, X_all_background)\n",
    "    # ✅ Correctly call shap_values\n",
    "    shap_values = explainer.shap_values(X_all_lstm)\n",
    "    shap_values = shap_values.squeeze(-1)  # Compress to a 3D array (last dimension is 1)\n",
    "    shap_valuesnew = shap_values.reshape(shap_values.shape[0], -1)  # Reshape to a 2D array\n",
    "    \n",
    "    \n",
    "    # Generate column names according to 3D structure; static variables are included for each week\n",
    "    newfeatures = []\n",
    "    for i in weeks_select_list:\n",
    "        features_this_week = [f'Week{i}_{feature}' for feature in TimeFeatures_sel] + Static_sel\n",
    "        newfeatures.extend(features_this_week)\n",
    "    \n",
    "    # Generate only dynamic variable names\n",
    "    newfeaturesday = []\n",
    "    for i in weeks_select_list:\n",
    "        features_this_week = [f'Week{i}_{feature}' for feature in TimeFeatures_sel]\n",
    "        newfeaturesday.extend(features_this_week)\n",
    "    \n",
    "    # Store as a 2D DataFrame\n",
    "    Shape_AllYear = pd.DataFrame(shap_valuesnew, columns=newfeatures)\n",
    "    X_all_scale1 = pd.DataFrame(X_all_lstm.reshape(X_all_lstm.shape[0], -1), columns=newfeatures)\n",
    "    Shape_AllYear['idJoin'] = data['idJoin']\n",
    "    X_all_scale1['idJoin'] = data['idJoin']\n",
    "    \n",
    "    # Create output directory if it does not exist\n",
    "    output_shapefile = os.path.join(inputpath_base, '03_results', 'SHAP', region)\n",
    "    os.makedirs(output_shapefile, exist_ok=True)\n",
    "    Shape_AllYear.to_csv(os.path.join(output_shapefile, f'{country}_{region}_shap_values.csv'), index=False)\n",
    "    X_all_scale1.to_csv(os.path.join(output_shapefile, f'{country}_{region}_X_all_scale1.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
